
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>6. Large Scale Data Management &#8212; Online-Skript Datenbanksysteme II</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="5. Optimierung" href="../05/optimierung.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/DBIS_Kurzlogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Online-Skript Datenbanksysteme II</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Datenbanksysteme II
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01/speicherung.html">
   1. Speicherung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02/repraesentation.html">
   2. Repräsentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03/indizes.html">
   3. Indizes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/anfrageausfuehrung.html">
   4. Anfrageausführung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05/optimierung.html">
   5. Optimierung
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Large Scale Data Management
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/LUH-DBS/GDBS_Script/main/?urlpath=tree/06/large-scale-data-management.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/LUH-DBS/GDBS_Script"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/LUH-DBS/GDBS_Script/issues/new?title=Issue%20on%20page%20%2F06/large-scale-data-management.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/06/large-scale-data-management.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-enabler-virtulization">
   6.1. Key enabler: Virtulization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parallel-data-processing">
   6.2. Parallel Data Processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grundlagen-der-parallelen-datenverarbeitung-parellel-processing">
     6.2.1. Grundlagen der Parallelen Datenverarbeitung (Parellel Processing)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-speedup-amdahls-law">
     6.2.2. Parallel Speedup – Amdahl‘s law
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallelisierungsstufen-auf-der-hardware">
     6.2.3. Parallelisierungsstufen auf der Hardware
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#varianten-der-anfrage-parallelisierung">
     6.2.4. Varianten der Anfrage-Parallelisierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-parallelism">
     6.2.5. Pipeline Parallelism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-parallelism">
     6.2.6. Data Parallelism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grundlagen-der-parallelen-anfragebearbeitung-parallel-query-processing">
     6.2.7. Grundlagen der Parallelen Anfragebearbeitung (Parallel Query Processing)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-architekturen-shared-memory">
     6.2.8. Parallele Architekturen – Shared Memory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-architectures-shared-disk">
     6.2.9. Parallel Architectures – Shared Disk
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-architectures-shared-nothing">
     6.2.10. Parallel Architectures – Shared Nothing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partitionierung">
     6.2.11. Partitionierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partitionierungsstrategien">
     6.2.12. Partitionierungsstrategien
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-parallelism-beispiel">
     6.2.13. Data Parallelism: Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     6.2.14. Data Parallelism: Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-operators">
     6.2.15. Parallel Operators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notationen-und-annahmen">
     6.2.16. Notationen und Annahmen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-selection-projection">
     6.2.17. Parallel Selection / Projection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-gruppierung-aggregation">
     6.2.18. Parallele Gruppierung &amp; Aggregation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-sortierung">
     6.2.19. Parallele Sortierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symmetric-fragment-and-replicate-join">
     6.2.20. Symmetric Fragment-and-Replicate Join
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asymmetric-fragment-and-replicate-join">
     6.2.21. Asymmetric Fragment-and-Replicate Join
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-equi-joins">
     6.2.22. Parallele Equi-Joins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grenzen-der-parallelen-datenbanken">
     6.2.23. Grenzen der Parallelen Datenbanken
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wann-passen-traditionelle-datenbanken-nicht-gut">
     6.2.24. Wann passen traditionelle Datenbanken nicht gut?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#webindex-fur-eine-suchmaschine-beispiel">
     6.2.25. Webindex für eine Suchmaschine - Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fortlaufende-neuentwicklung">
     6.2.26. Fortlaufende Neuentwicklung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anforderungen-an-die-speicherung">
     6.2.27. Anforderungen an die Speicherung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-storage-model-distributed-file-system">
     6.2.28. The Storage Model – Distributed File System
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#retrieving-and-analyzing-data">
     6.2.29. Retrieving and Analyzing Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#skalierungsmuster">
     6.2.30. Skalierungsmuster
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-reduce-hadoop">
   6.3. Map Reduce &amp; Hadoop
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#was-ist-map-reduce">
     6.3.1. Was ist Map/Reduce?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grundbausteine">
     6.3.2. Grundbausteine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mapreduce-workflow">
     6.3.3. MapReduce Workflow
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beispiel-1">
       6.3.3.1. Beispiel 1
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beispiel-2">
     6.3.4. Beispiel 2
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beispiel-3">
       6.3.4.1. Beispiel 3
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-dbms-vs-map-reduce">
     6.3.5. Parallel DBMS vs. Map/Reduce
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relational-operators-as-map-reduce-jobs">
     6.3.6. Relational Operators as Map/Reduce jobs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-a-map-reduce-framework">
     6.3.7. Hadoop – A map/reduce Framework
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-distributed-file-system-hdfs">
     6.3.8. Hadoop Distributed File System (HDFS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-map-reduce-engine">
     6.3.9. Hadoop Map/Reduce Engine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fehlertoleranz">
     6.3.10. Fehlertoleranz
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wann-sollte-man-hadoop-nutzen">
     6.3.11. Wann sollte man Hadoop nutzen?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-der-praxis-komplexe-optimierte-mapreduce-workflows">
     6.3.12. In der Praxis: Komplexe (optimierte) MapReduce Workflows
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-vs-parallel-dbms">
     6.3.13. Hadoop vs. Parallel DBMS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-der-praxis-viele-bibliotheken">
     6.3.14. In der Praxis: Viele Bibliotheken
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Large Scale Data Management</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-enabler-virtulization">
   6.1. Key enabler: Virtulization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parallel-data-processing">
   6.2. Parallel Data Processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grundlagen-der-parallelen-datenverarbeitung-parellel-processing">
     6.2.1. Grundlagen der Parallelen Datenverarbeitung (Parellel Processing)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-speedup-amdahls-law">
     6.2.2. Parallel Speedup – Amdahl‘s law
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallelisierungsstufen-auf-der-hardware">
     6.2.3. Parallelisierungsstufen auf der Hardware
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#varianten-der-anfrage-parallelisierung">
     6.2.4. Varianten der Anfrage-Parallelisierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pipeline-parallelism">
     6.2.5. Pipeline Parallelism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-parallelism">
     6.2.6. Data Parallelism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grundlagen-der-parallelen-anfragebearbeitung-parallel-query-processing">
     6.2.7. Grundlagen der Parallelen Anfragebearbeitung (Parallel Query Processing)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-architekturen-shared-memory">
     6.2.8. Parallele Architekturen – Shared Memory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-architectures-shared-disk">
     6.2.9. Parallel Architectures – Shared Disk
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-architectures-shared-nothing">
     6.2.10. Parallel Architectures – Shared Nothing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partitionierung">
     6.2.11. Partitionierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partitionierungsstrategien">
     6.2.12. Partitionierungsstrategien
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-parallelism-beispiel">
     6.2.13. Data Parallelism: Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     6.2.14. Data Parallelism: Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-operators">
     6.2.15. Parallel Operators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notationen-und-annahmen">
     6.2.16. Notationen und Annahmen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-selection-projection">
     6.2.17. Parallel Selection / Projection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-gruppierung-aggregation">
     6.2.18. Parallele Gruppierung &amp; Aggregation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-sortierung">
     6.2.19. Parallele Sortierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symmetric-fragment-and-replicate-join">
     6.2.20. Symmetric Fragment-and-Replicate Join
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asymmetric-fragment-and-replicate-join">
     6.2.21. Asymmetric Fragment-and-Replicate Join
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallele-equi-joins">
     6.2.22. Parallele Equi-Joins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grenzen-der-parallelen-datenbanken">
     6.2.23. Grenzen der Parallelen Datenbanken
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wann-passen-traditionelle-datenbanken-nicht-gut">
     6.2.24. Wann passen traditionelle Datenbanken nicht gut?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#webindex-fur-eine-suchmaschine-beispiel">
     6.2.25. Webindex für eine Suchmaschine - Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fortlaufende-neuentwicklung">
     6.2.26. Fortlaufende Neuentwicklung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anforderungen-an-die-speicherung">
     6.2.27. Anforderungen an die Speicherung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-storage-model-distributed-file-system">
     6.2.28. The Storage Model – Distributed File System
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#retrieving-and-analyzing-data">
     6.2.29. Retrieving and Analyzing Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#skalierungsmuster">
     6.2.30. Skalierungsmuster
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-reduce-hadoop">
   6.3. Map Reduce &amp; Hadoop
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#was-ist-map-reduce">
     6.3.1. Was ist Map/Reduce?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grundbausteine">
     6.3.2. Grundbausteine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mapreduce-workflow">
     6.3.3. MapReduce Workflow
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beispiel-1">
       6.3.3.1. Beispiel 1
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beispiel-2">
     6.3.4. Beispiel 2
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beispiel-3">
       6.3.4.1. Beispiel 3
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-dbms-vs-map-reduce">
     6.3.5. Parallel DBMS vs. Map/Reduce
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relational-operators-as-map-reduce-jobs">
     6.3.6. Relational Operators as Map/Reduce jobs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-a-map-reduce-framework">
     6.3.7. Hadoop – A map/reduce Framework
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-distributed-file-system-hdfs">
     6.3.8. Hadoop Distributed File System (HDFS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-map-reduce-engine">
     6.3.9. Hadoop Map/Reduce Engine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fehlertoleranz">
     6.3.10. Fehlertoleranz
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#wann-sollte-man-hadoop-nutzen">
     6.3.11. Wann sollte man Hadoop nutzen?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-der-praxis-komplexe-optimierte-mapreduce-workflows">
     6.3.12. In der Praxis: Komplexe (optimierte) MapReduce Workflows
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hadoop-vs-parallel-dbms">
     6.3.13. Hadoop vs. Parallel DBMS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-der-praxis-viele-bibliotheken">
     6.3.14. In der Praxis: Viele Bibliotheken
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="large-scale-data-management">
<h1><span class="section-number">6. </span>Large Scale Data Management<a class="headerlink" href="#large-scale-data-management" title="Permalink to this headline">#</a></h1>
<p>In diesem Kapitel geht es insbesondere darum, die Verfahren und Datenbankoperationen, die wir bisher kennengelernt haben, hinsichtlich paralleler Verarbeitung zu betrachten und auch die Kostenelemente, die dann eine Rolle spielen.</p>
<p>Beim Large Scale Data Management geht es um sehr große Datenmengen. Da reicht es dann nicht mehr, nur eine Datenbank zu haben, man muss nun auch über die Verteilung, Server und Nebenläufigkeiten nachdenken.</p>
<figure class="align-default" id="id1">
<img alt="../_images/Large-scale-Data-Management.png" src="../_images/Large-scale-Data-Management.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Large Scale Data Management</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Zur Wiederholung einmal die Frage: Was ist Big Data? Big Data wird anhand von Dimensionen spezifiziert- die sogenannten V’s: <strong>Volume</strong> (Menge von Daten), <strong>Velocity</strong> (Schnelligkeit der Datenverarbeitung), <strong>Variety</strong> (Heterogenität der Daten), <strong>Verocity</strong> (Daten, bei denen die Korrektheit ungewiss ist) und <strong>Value</strong> (die Wertigkeit der Daten).</p>
<p>Nun gibt es Big Data in zwei Varianten - <strong>Operational</strong> und <strong>Analytic</strong>. In der ersten Variante geht es um operationelle Sachen, also dem Transaktionsmanagement. In der zweiten Variante geht es darum, Daten zu analysieren, Insights aus Daten herzustellen und neue Erkenntnisse zu gewinnen.</p>
<p>Zur Verdeutlichung, über was für Datenmengen wir bei Big Data reden:</p>
<p>Google ist ein klassisches Beispiel für ein Datenproduzierendes und -verwaltendes Unternehmen. Dort werden jeden Tag 20 PB an Daten verarbeitet. Das sind Billionen von Zeilen, Tausende/Millionen Spalten und Tabellen, aber auch strukturierte Daten wie Text, Bilder und Videos. Würde man versuchen, diese 20 PB mit 50 MB/s zu lesen, würde das 12 Jahre dauern. Aus diesem Grund werden die Daten partioniert und verteilt verarbeitet.</p>
<section id="key-enabler-virtulization">
<h2><span class="section-number">6.1. </span>Key enabler: Virtulization<a class="headerlink" href="#key-enabler-virtulization" title="Permalink to this headline">#</a></h2>
<p>Die beiden Varianten Operational und Analytic lassen sich mit der Virtualization managen. Hierbei versucht man entweder ein logisches System auf viele physische Systeme (Load Balancing) oder andersherum mehrere logische Systeme auf ein physisches System abzubilden (Multy-Tenancy).</p>
<figure class="align-default" id="virtualization">
<img alt="../_images/Virtualization.png" src="../_images/Virtualization.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Virtualization</span><a class="headerlink" href="#virtualization" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="parallel-data-processing">
<h2><span class="section-number">6.2. </span>Parallel Data Processing<a class="headerlink" href="#parallel-data-processing" title="Permalink to this headline">#</a></h2>
<p><strong>Was bisher geschah: Serielle Verarbeitung/Single Threaded</strong>
</br></p>
<p>Bisher haben wir immer von einem Computer mit mehreren Festplatten gesprochen und damit auch ein wenig über parallele Plattenzugriffe. Diese hatten insbesondere auch immer nur einen Kern. Das heißt, bei jeder Operation wurden die Blöcke nacheinander durch nur einen Kern abgearbeitet. Außerdem spielten auch Synchronisation und Kommunikation keine Rolle, da Anfragen in nur einem Thread bearbeitet wurden. Dies wollen wir nun erweitern.</p>
<figure class="align-default" id="serial-single-threaded">
<img alt="../_images/serial-single-threaded.png" src="../_images/serial-single-threaded.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Serial Single Threaded</span><a class="headerlink" href="#serial-single-threaded" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Was wir verschwiegen haben…</strong>
</br></p>
<p>Das Datenvolumen wächst stetig. Data Warehouses mit 1 EB sind nicht untypisch. Manche Organisationen produzieren täglich mehr als 1 PB an neuen Daten. Das entspricht 1.000.000.000.000.000 Byte (1 quadrillion).
Manche Systeme, wie beispielsweise Finanzinstitute, Onlineshops und soziale Netzwerke, haben einen sehr hohen Durchsatz (throughput) von Transaktionen.
Deshalb ist es wichtig zu überlegen, wie die Zugriffe über die Netzwerke verteilt werden.
Auch Analyseanfragen werden immer komplexer. Eine statistische Mustererkennung ist teuer und über die Daten muss mehrfach iteriert werden. Da reicht eine Single-CPU- oder Single-Node-Architektur nicht mehr aus und auch Moore’s Law ist hier nicht mehr anwendbar. Die Lösung: <strong>Parallele Datenverarbeitung</strong>.</p>
<section id="grundlagen-der-parallelen-datenverarbeitung-parellel-processing">
<h3><span class="section-number">6.2.1. </span>Grundlagen der Parallelen Datenverarbeitung (Parellel Processing)<a class="headerlink" href="#grundlagen-der-parallelen-datenverarbeitung-parellel-processing" title="Permalink to this headline">#</a></h3>
<p>Bei der parallelen Datenverarbeitung kommt <strong>Amdahl’s Law</strong> zum Einsatz, welches die Grenzen bei der parallelen Beschleunigung definiert. Es gibt außerdem verschiedenen Stufen der Parallelisierung, mit denen auf unterschiedlichen Ebenen parallelisiert werden kann. Des Weiteren existieren noch verschiedene <strong>Varianten der Anfrage-Parallelisierung</strong>. Dadurch können mehrere Anfragen parallel verarbeitet werden (<strong>Inter-Query</strong>) oder nur eine Anfrage (<strong>Intra-Query</strong>).</p>
</section>
<section id="parallel-speedup-amdahls-law">
<h3><span class="section-number">6.2.2. </span>Parallel Speedup – Amdahl‘s law<a class="headerlink" href="#parallel-speedup-amdahls-law" title="Permalink to this headline">#</a></h3>
<p>Die Frage die sich bei Amdahl’s law stellt ist, wie viel wir an Geschwindigkeit überhaupt dazugewinnen können. Berechnen lässt sich das zum einen mit der sequentiellen Laufzeit <span class="math notranslate nohighlight">\(T_1\)</span> (1 Prozessor) und zum anderen mit der parallelen Laufzeit <span class="math notranslate nohighlight">\(T_p\)</span> (<em>p</em> Prozessoren): <span class="math notranslate nohighlight">\(S_p\)</span> = <span class="math notranslate nohighlight">\(\frac{T_1}{T_p}\)</span> . Die maximale Beschleunigung ist durch den nicht-parallelisierbaren Anteil des Programms begrenzt. Wie hoch diese ist, lässt sich folgendermaßen berechnen: <span class="math notranslate nohighlight">\(S_p\)</span> = <span class="math notranslate nohighlight">\(\frac{1}{(1 - f) + \frac{f}{p}}\)</span> . <br>
Die Variable <em>f</em> entspricht prozentual dem parallelisierbaren Anteil. Die ideale Beschleunigung wäre <em>S = p</em> für <em>f = 1</em>. Oft ist <em>f</em> &lt; 1 während <em>S</em> durch eine Konstante begrenzt wird. Beispiel: <em>f = 0,9</em> und 10/20 Server. <span class="math notranslate nohighlight">\(S_p\)</span> = <span class="math notranslate nohighlight">\(\frac{1}{(1 - f) + \frac{f}{p}}\)</span> = <span class="math notranslate nohighlight">\(\frac{1}{(1 - 0,9) + \frac{0,9}{10}} \approx \)</span> 5,3 und <span class="math notranslate nohighlight">\(S_p\)</span> = <span class="math notranslate nohighlight">\(\frac{1}{(1 - 0,9) + \frac{0,9}{20}} \approx \)</span> 6,9 . Lassen wir hier unsere Prozessoren gegen unendlich laufen, ist unser <span class="math notranslate nohighlight">\(S_p\)</span> = 10. Das bedeutet, wir können weitere Server hinzufügen, aber es bleibt bei der 10-fachen Geschwindigkeit.</p>
<p>Hier sehen wir, wie sich die parallele Beschleunigung nach Amdahl’s law je nach Prozessorzahl verhält.</p>
<figure class="align-default" id="parallel-speedup">
<img alt="../_images/Parallel-Speedup.png" src="../_images/Parallel-Speedup.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.4 </span><span class="caption-text">Parallel Speedup</span><a class="headerlink" href="#parallel-speedup" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="parallelisierungsstufen-auf-der-hardware">
<h3><span class="section-number">6.2.3. </span>Parallelisierungsstufen auf der Hardware<a class="headerlink" href="#parallelisierungsstufen-auf-der-hardware" title="Permalink to this headline">#</a></h3>
<p>Es gibt unterschiedliche Stufen der Parallelisierung auf der Hardware. Zum einen gibt es <strong>Instruction-level parallelism</strong> (Prozessoranweisungen). Dabei werden Prozessorbefehle durch die CPU-Architektur automatisch parallelisiert. Zum Anderen gibt es <strong>Data parallelism</strong> (Daten). Jeder Prozessor verarbeitet die gleichen Befehle auf seiner eigenen Partition der Daten. Dadurch können unterschiedliche Daten parallel verarbeitet werden, beispielsweise durch verteilte Schleifeniterationen auf mehreren Prozessoren oder GPU processing. Auf der letzten Stufe der Parallelisierung haben wir <strong>Task parallelism</strong> (Aufgaben). Hierbei erhält jeder Prozessor/Knoten eine andere Aufgabe.</p>
</section>
<section id="varianten-der-anfrage-parallelisierung">
<h3><span class="section-number">6.2.4. </span>Varianten der Anfrage-Parallelisierung<a class="headerlink" href="#varianten-der-anfrage-parallelisierung" title="Permalink to this headline">#</a></h3>
<p>Es existieren unterschiedliche Varianten der Anfrage-Parallelisierung. Die erste Variante ist <strong>Inter-Query Parallelism</strong> (mehrere nebenläufige Anfragen). Dies ist wichtig für eine effiziente Ressourcennutzung. Wartet eine Anfrage z.B. auf I/O, kann in der Zeit eine andere Anfrage ausgeführt werden. Dies erfordert <strong>Concurrency Control</strong>, also das Sperren, um Transaktionseigenschaften zu garantieren. Das ist auch wichtig für OLTP. Die zweite Variante ist <strong>Intra-Query Parallelism</strong> (parallele Verarbeitung einer einzelnen Anfrage). Dieser unterteilt sich nochmal in <strong>I/O Parallelism</strong>, <strong>Intra-Operator Parallelism</strong> und <strong>Inter-Operator Parallelism</strong>. Beim <strong>I/O parallelism</strong> werden nebenläufig mehrere Platten gelesen. Dabei wird mit ‘Spanned Tablespaces’, Partitionierung und eventuell auch mit Hardware RAIDs (versteckt) gearbeitet. Beim <strong>Intra-Operator Parallelism</strong> arbeiten mehrere Threads für den selben Operator, wie beispielsweise beim Parallel Sort, während beim <strong>Inter-Operator Parallelism</strong> mehrere Teile eines Anfrageplans parallel laufen (Pipeline). Letzteres ist wichtig für komplexe analytische Aufgaben (OLAP).</p>
</section>
<section id="pipeline-parallelism">
<h3><span class="section-number">6.2.5. </span>Pipeline Parallelism<a class="headerlink" href="#pipeline-parallelism" title="Permalink to this headline">#</a></h3>
<p>Pipeline Parallelism besteht aus drei verschiedenen Schritten und erlaubt, dass mehrere Teile eines Anfrageplans gleichzeitig laufen können.</p>
<figure class="align-default" id="id2">
<img alt="../_images/Pipeline-Parallelism.png" src="../_images/Pipeline-Parallelism.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.5 </span><span class="caption-text">Pipeline Parallelism</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Im <strong>ersten Schritt</strong> können zwei Threads (T2 und T3) je einen Base Table scannen und für die Joins einen Hash Table bauen. </br>
Dann scannt im <strong>zweitem Schritt</strong> ein Thread den ersten Table. Der Thread prüft die Hashtabellen auf Kollisionen und versucht eine alternative Stelle zu finden (Probing). Bei Hash Tabellen kann es zu Kollisionen kommen, wenn zwei verschiedene Schlüssel auf denselben Hashwert abgebildet werden. Der zweite Thread fängt an die Sublisten zu sortieren (Sort) und fügt die ersten Listen zusammen. </br>
Am Ende beim <strong>dritten Schritt</strong> ist nur noch ein Thread vorhanden. Dieser gibt das Ergebnis zurück (Return) und arbeitet wieder wie zuvor weiter.</p>
<p><strong>Pipeline Parallelism</strong> ist auch bekannt als <strong>Inter-operator Parallelism</strong>: Eine Parallelisierung der Operatoren.
Inter Operator bedeutet dabei: Während man an etwas arbeitet, gibt man die Ergebnisse weiter an andere Threads. Dadurch können diese schon früher andere Operationen auf den Daten durchführen.
Es können somit mehrere Pipelines gleichzeitig ausgeführt werden, sofern mehrere vorhanden sind und auch nicht voneinander abhängen.</p>
<p>Zudem hat Pipeline Parallelism einige Probleme.
Der <strong>Synchronisationsaufwand</strong> ist sehr hoch, wenn Fehler gemacht werden oder auf Threads gewartet wird, die noch nicht fertiggestellt wurden. </br>
Häufig kann auch nur wenig parallelisiert werden, sodass der <strong>Parallelisierungsgrad</strong> gering ist (degree of parallelism). Wenn man beispielsweise eine Anfrage mit fünf Operationen hat, kann man sie maximal mit Faktor 5 parallelisieren. Dazu kommen noch die Kosten der einzelnen Operationen: Nicht jede Operation kostet gleich viel. Wenn eine Operation zwar sehr schnell ausgeführt werden kann, aber eine andere Operation sehr lange braucht, muss trotzdem auf die längere gewartet werden. </br>
Das Verfahren ist eher nur für Shared-Memory-Architekturen geeignet. Dabei spielen die I/O-Kosten eine untergeordnete Rolle.</p>
</section>
<section id="data-parallelism">
<h3><span class="section-number">6.2.6. </span>Data Parallelism<a class="headerlink" href="#data-parallelism" title="Permalink to this headline">#</a></h3>
<p>Pipeline Parallelism ist nicht immer anwendbar. Die Alternative bietet <strong>Data Parallelism</strong>. Die Daten werden in Teilmengen partitioniert. Sie werden also auf verschiedenen Rechnern oder Disks gespeichert. Die Idee hierbei ist, dass es Operationen gibt, die im selben Kontext nicht alles sehen müssen. Mit anderen Worten: Die Operationen werden geteilt auf Rechnern oder auch auf Prozessoren ausgeführt. Die entstehenden Ergebnisse auf den verschiedenen Rechnern/Prozessoren müssen dann nur noch zusammengefügt werden. Dadurch können Teilmengen unanhängig und parallel verarbeitet werden.</p>
<p>Ein kleines <strong>Beispiel bei einer Selektion</strong>: </br>
Man teilt einen Stapel Klausuren auf 5 Stapel auf. Gesucht werden alle 1er Kandidaten. Nun stellt man an jeden Stapel eine Person, die diesen Stapel Klausur für Klausur durchsucht und die Klausuren mit einer 1 herausnimmt. Das Ergebnis ist trotz mehrerer Teilstapel am Ende korrekt. Die Klausuren müssen nur noch zusammengelegt werden.
An diesem Beispiel kann man nun sehen, dass die Selektion jedes Tupels unabhängig ist.</p>
<p>Der <strong>maximale Parallelisierungsgrad</strong> hängt von der maximalen Anzahl von Teilmengen ab. Bei einer Selektion wäre es somit die Anzahl der Tupel.</p>
<p>Andere Operationen brauchen eine umfassendere Sicht auf die Daten. Dazu zählen z.B. die Gruppierung oder die Aggregation. In dem Beispiel bräuchten die Personen einen Blick in die anderen Stapel, um die jeweilige Operation auszuführen. Es reicht nicht mehr nur seinen eigenen Stapel zu betrachten. Sie benötigen also unterschiedliche Mengen, um korrekt zu funktionieren.</p>
</section>
<section id="grundlagen-der-parallelen-anfragebearbeitung-parallel-query-processing">
<h3><span class="section-number">6.2.7. </span>Grundlagen der Parallelen Anfragebearbeitung (Parallel Query Processing)<a class="headerlink" href="#grundlagen-der-parallelen-anfragebearbeitung-parallel-query-processing" title="Permalink to this headline">#</a></h3>
<p>Nun werden die Grundlagen der Parallelen Anfragebearbeitung genauer thematisiert. Hier sei zunächst ein kleiner Überblick über die Parallelen Architekturen, die Datenpartitionierungsstrategien und über die Kosten gegeben.</p>
<p>Bei der parallelen Anfragebearbeitung kommt es darauf an, welche gemeinsamen Ressourcen man zur Verfügung hat. Es gibt drei Stufen von gemeinsam genutzten Ressourcen (<strong>Ressource Sharing</strong>). Diese werden auch <strong>Parallele Architekturen</strong> genannt.</p>
<ol class="arabic simple">
<li><p><strong>Shared-Memory (Hauptspeicher)</strong></p></li>
<li><p><strong>Shared-Disk</strong></p></li>
<li><p><strong>Shared-Nothing</strong></p></li>
</ol>
<p>Parallele Architekturen haben den Vorteil, dass man keine teuren Großrechner benötigt auf denen man die gesamten Daten speichert. Bei Vergrößerung des teuren Großrechners würden unter Anderem die Kosten exponentiell steigen. Zudem ist die Reparatur sehr komplex. Aus diesem Grund verwendet man Parallelen Architekturen.</p>
<p>Zudem gibt es noch unterschiedliche Partitionierungsvarianten bei der parallelen Anfragebearbeitung. Die Daten können zufällig mit Round-Robin verteilt werden, mit einer Hash-Funktion oder mit einer Bereichsfunktion.</p>
<p>Je nach parallelem Operator variieren die Kosten.
Eine Selektion verarbeitet beispielsweise nur ein Tupel auf einmal. Andere Operatoren benötigen zusätzlich eine Sortierung, Projektion, Gruppierung, Aggregation oder auch einen Join. </br></p>
</section>
<section id="parallele-architekturen-shared-memory">
<h3><span class="section-number">6.2.8. </span>Parallele Architekturen – Shared Memory<a class="headerlink" href="#parallele-architekturen-shared-memory" title="Permalink to this headline">#</a></h3>
<p>Unter den Parallelen Architekturen ist <strong>Shared Memory</strong> die einfachste Architektur. Mehrere CPUs teilen sich einen einzigen Speicher und eigene Disks (Array). Die Kommunikation wird über einen einzigen gemeinsamen Bus betrieben. In der Realität hat jeder Prozessor nocht zusätzlich einen eigenen privaten Speicher (NUMA: non-uniform memory access).</p>
<p>Man hat ein Interface auf den Speicher (M = Memory). Alle Prozessoren § können darüber auf den ganzen Speicher zugreifen. Entweder greifen sie auf diesselbe oder eben auch auf unterschiedliche Disks zu.</p>
<figure class="align-default" id="shared-memory">
<img alt="../_images/shared-memory.png" src="../_images/shared-memory.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.6 </span><span class="caption-text">Shared Memory</span><a class="headerlink" href="#shared-memory" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="parallel-architectures-shared-disk">
<h3><span class="section-number">6.2.9. </span>Parallel Architectures – Shared Disk<a class="headerlink" href="#parallel-architectures-shared-disk" title="Permalink to this headline">#</a></h3>
<p>Bei der parallelen Architektur <strong>Shared Disks</strong> existieren mehrere Knoten mit mehreren CPUs. Jeder Knoten besitzt eigene private Hauptspeicheradressen oder Hauptspeicheradressbereiche. Die Knoten greifen alle auf die gleichen Disks zu und nutzen die gleiche Datenbank. Diese Architektur wird oft bei NAS, SAN oder anderen Systemen verwendet.</p>
<figure class="align-default" id="shared-disk">
<img alt="../_images/shared-disk.png" src="../_images/shared-disk.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.7 </span><span class="caption-text">Shared Disk</span><a class="headerlink" href="#shared-disk" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="parallel-architectures-shared-nothing">
<h3><span class="section-number">6.2.10. </span>Parallel Architectures – Shared Nothing<a class="headerlink" href="#parallel-architectures-shared-nothing" title="Permalink to this headline">#</a></h3>
<p>Shared Nothing ist die meist genutzte Architektur für skalierbare Datenverarbeitung (<strong>Large-Scale Data Management</strong>). Bei der <strong>Shared Nothing</strong> Architektur hat jeder Knoten seinen eigenen Satz an CPUs, Speicher und Disks. Im Prinzip ist jeder Knoten somit ein eigener Server.
Um die Vorteile der Architektur im vollem Umfang nutzen zu können, müssen die Daten über die Knoten partitioniert werden. Hierbei gibt es unterschiedliche Partitionierungsvarianten, die im Anschluss noch weiter thematisiert werden. Für die Partitionierung werden die Daten direkt über eine Knoten-zu-Knoten Kommunikation ausgetauscht. Die Nachrichten haben dabei einen signifikanten Overhead.
Es werden somit doch Daten zwischen den Knoten ausgetauscht. Die Knoten teilen sich nämlich immernoch das Netzwerk. Der Name Shared Nothing ist also etwas irreführend.</p>
<figure class="align-default" id="shared-nothing">
<img alt="../_images/shared-nothing.png" src="../_images/shared-nothing.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.8 </span><span class="caption-text">Shared Nothing</span><a class="headerlink" href="#shared-nothing" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="partitionierung">
<h3><span class="section-number">6.2.11. </span>Partitionierung<a class="headerlink" href="#partitionierung" title="Permalink to this headline">#</a></h3>
<p><strong>Partitionierung</strong> bedeutet möglichst disjunkte Teilmenge zu generieren. Je nach Fall können die Daten unterschiedlich partitioniert werden. Bei Verkaufsdaten kann zum Beispiel jedes Jahr seine eigene Partition erhalten.</p>
<p>Für eine Shared Nothing Architektur müssen die Daten auf mehreren Knoten verteilt werden. Würde man die Daten einfach replizieren, kann man aus der Sicht der Anfragebearbeitung auch von einer <strong>Shared-Disk</strong> sprechen. Es ist so als würde man alle Daten auf einer Disk speichern. Die lokalen Disks verhalten sich dann wie Caches. Außerdem muss bei der Replikation die Konsistenz sichergestellt werden.</p>
<p>Manche Datenbankanfragen können auf bestimmte Bereiche eingegrenzt werden, sofern man sicherstellen kann, dass alle relevanten Daten in der entsprechenden Partition zu finden sind. Die Partitionierung gewinnt durch solche Eigenschaften an Vorteil.</p>
<p>In der Datenbankadministration entsteht durch Partitionierung ein weiterer Vorteil. Nicht mehr benötigte Partitionen wie alte Verkäufe usw. können einfach verworfen werden.</p>
</section>
<section id="partitionierungsstrategien">
<h3><span class="section-number">6.2.12. </span>Partitionierungsstrategien<a class="headerlink" href="#partitionierungsstrategien" title="Permalink to this headline">#</a></h3>
<p>Bei der Durchführung der Partitionierung können verschiedene Strategien gewählt werden. Jede bietet andere Vorteile.</p>
<p>Bei <strong>Round Robin</strong> erhält jede Partition ein Tupel pro Runde. Dadurch haben alle Teilmengen garantiert eine möglichst gleiche Anzahl von Tupeln. Zwischen den einzelnen Tupeln in einer Partition herrscht widerrum keine explizite Beziehung.</p>
<p>Bei <strong>Hash Partitioning</strong> wird eine Menge von Partitionsspalten definiert. Für jede Spalte wird ein Hashwert generiert. Dieser Hashwert wird verwendet, um zu entscheiden, welche Partition als Ziel für die Tupel gewählt wird. Der Hashwert wird für jede Zeile berechnet, basierend auf den Werten der Partitionsspalten. Das Tupel wird dann der Partition zugeordnet, die dem berechneten Hashwert entspricht.</p>
<p><strong>Range Partitioned</strong> bedeutet, dass eine Menge von Partitionsspalten definiert wird. Die Domäne der Spalten wird in Bereiche aufgeteilt. Die Tupel werden den Partitionen basierend auf den Bereichswerten zugeordnet. Alle Tupel aus einer Partition stammen somit aus dem gleichen Bereich.</p>
</section>
<section id="data-parallelism-beispiel">
<h3><span class="section-number">6.2.13. </span>Data Parallelism: Beispiel<a class="headerlink" href="#data-parallelism-beispiel" title="Permalink to this headline">#</a></h3>
<p>Ein Client schickt eine SQL-Anfrage an einen Cluster-Knoten. Dieser Cluster-Knoten wird dann der Coordinator. Der Coordinater kompiliert die Anfrage. Er parst, überprüft und optimiert also die Anfrage. Zudem überlegt er sich wie die Anfrage parallelisiert werden kann. Die Teilpläne werden an andere Knoten geschickt und der Coordinator führt den eignen Teilplan auch aus. Am Ende sammelt der Coordinator alle Teilergebnisse und finalisiert diese für die Ausgabe.</p>
<figure class="align-default" id="data-parallelism-example">
<img alt="../_images/Data-Parallelism-example.png" src="../_images/Data-Parallelism-example.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.9 </span><span class="caption-text">Data Parallelism Example</span><a class="headerlink" href="#data-parallelism-example" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="id3">
<h3><span class="section-number">6.2.14. </span>Data Parallelism: Beispiel<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<p><strong>Data Parallelism</strong> ist eine Strategie, die in Shared-Nothing und Shared-Disk Architekturen genutzt wird. Dabei führen multiple Instanzen eines Verarbeitungsplans gleichzeitig auf verschiedenen Knoten des Systems Operationen auf unterschiedlichen Datenpartitionen durch. Die Ergebnisse werden anschließend zusammengeführt. Bei komplexen Anfragen können die Ergebnisse auch für weitere parallele Verarbeitungsschritte neu verteilt werden. Dies ermöglicht eine effiziente und beschleunigte Verarbeitung großer Datensätze in verteilten Systemen.</p>
<a class="reference internal image-reference" href="../_images/Data-Parallelism-example_2.png"><img alt="Data-Parallelism-example_2" src="../_images/Data-Parallelism-example_2.png" style="width: 400px;" /></a>
</section>
<section id="parallel-operators">
<h3><span class="section-number">6.2.15. </span>Parallel Operators<a class="headerlink" href="#parallel-operators" title="Permalink to this headline">#</a></h3>
<p>Bei der Verarbeitung von Daten in parallelen Systemen strebt man idealerweise an, dass parallele Operatoren auf unterschiedlichen Partitionen der Daten laufen. Dies bedeutet, dass Operationen direkt zu den Daten geschickt werden können. Dies ist besonders einfach für grundlegende “tuple-at-a-time” Operatoren wie Scans, Index-Scans und Selektionen.</p>
<p>Allerdings gibt es Herausforderungen bei sogenannten <strong>“Blocking Operatoren”</strong>, die alle Daten sehen müssen. Beispielsweise können Sortier- und Aggregationsoperatoren nur parallel vorverarbeitet werden. Der finale Schritt einer Sortierung und Aggregation wird auf einem einzelnen Knoten durchgeführt, es sei denn, es handelt sich um Teilpläne, die dies nicht erfordern.</p>
<p>Ein Beispiel hierfür sind Joins, die passende Tupel benötigen. Dies kann durch die Organisation der Eingabedaten oder durch die Ausführung des Joins am Koordinator nach Zusammenführung der Teilergebnisse erreicht werden. Letzteres führt jedoch zu einer nicht mehr parallelen Verarbeitung.</p>
</section>
<section id="notationen-und-annahmen">
<h3><span class="section-number">6.2.16. </span>Notationen und Annahmen<a class="headerlink" href="#notationen-und-annahmen" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>S	Relation S</p></li>
<li><p>S[i,h] Partition i der Relation S basierend auf Partitionierungsschema h</p></li>
<li><p>B(S) Anzahl der Blöcke von S</p></li>
<li><p>p	Anzahl der Knoten</p></li>
</ul>
<p>In der Annahme einer Shared-Nothing-Architektur, bei der jeder Knoten über eigene lokale Ressourcen verfügt und Datenpartitionen unabhängig voneinander verarbeitet, sind <strong>Netzwerktransfers und Diskzugriffe</strong> als <strong>kostenequivalent</strong> angenommen. Obwohl die Netzwerkkosten gelegentlich höher sein können, wird davon ausgegangen, dass die Netzwerk- und Diskbandbreite heutzutage etwa gleichwertig sind.</p>
<p>Es ist jedoch zu berücksichtigen, dass das Netzwerk geteilt wird und durch Durchsatzlimits an Switches und Routern begrenzt ist. Diese Einschränkung kann Auswirkungen auf die Gesamtleistung haben, insbesondere wenn viele Knoten gleichzeitig auf das Netzwerk zugreifen.</p>
<p>Eine weitere Annahme betrifft <strong>Partitionierungsansätze wie Hashing oder Range-Partitionierung</strong>. Sie erzeugen ungefähr gleichgroße Partitionen auf den Servern. Dies ist wichtig, um eine gleichmäßige Lastverteilung und effiziente parallele Verarbeitung zu gewährleisten. Bei den folgenden Berechnungen wird von diesem Fall ausgegangen und nicht davon, dass die Daten ungleichmßig verteilt liegen.</p>
<p>Die letzte Annahme, dass S[i,h] &gt; M, bedeutet, dass die Größe der Daten auf einem bestimmten Knoten i größer ist als der verfügbare Hauptspeicher (M).</p>
</section>
<section id="parallel-selection-projection">
<h3><span class="section-number">6.2.17. </span>Parallel Selection / Projection<a class="headerlink" href="#parallel-selection-projection" title="Permalink to this headline">#</a></h3>
<p>Die parallele Durchführung von Selektion und Projektion erweist sich als äußerst effizient und einfach. Daher werden diese Operationen auch als <strong>“Embarrassingly Parallel”</strong> bezeichnet. Jeder Knoten kann die Operation unabhängig auf seiner eigenen existierenden Datenpartition ausführen. Besonders vorteilhaft ist, dass die Selektion keinen Kontext benötigt, und die Daten willkürlich partitioniert vorliegen können.</p>
<p>In diesem Szenario werden die Teilergebnisse am Ende einfach zusammengeführt. Die Kosten für diese Parallelität werden durch die Formel <span class="math notranslate nohighlight">\(B(S)/p + Transfer\)</span> bestimmt, wobei B(S) die Datenmenge für die Operation ist und p die Anzahl der beteiligten Knoten darstellt. Die Daten liegen verteilt auf den Blöcken vor. Diese müssen nur parallel gelesen werden, also durch <span class="math notranslate nohighlight">\(p\)</span> geteilt werden. Hinzu kommen die Transferkosten. Diese basieren auf der Selektivität, also wie viele Tupel weitergegeben werden müssen.
Insgesamt ermöglicht die parallele Ausführung von Selektion und Projektion eine effiziente Verarbeitung großer Datenmengen in verteilten Umgebungen.</p>
</section>
<section id="parallele-gruppierung-aggregation">
<h3><span class="section-number">6.2.18. </span>Parallele Gruppierung &amp; Aggregation<a class="headerlink" href="#parallele-gruppierung-aggregation" title="Permalink to this headline">#</a></h3>
<p>Die <strong>parallele Gruppierung und Aggregation</strong> in verteilten Systemen erfolgt in zwei Phasen, um eine effiziente Verarbeitung zu gewährleisten. In der ersten Phase wird eine lokale Gruppierung und Aggregation auf jeder Partition durchgeführt. Anschließend erfolgt in der zweiten Phase ein Zusammenführen der Ergebnisse.</p>
<p>Die <strong>Kosten</strong> dieses Prozesses setzen sich zusammen aus den lokalen Algorithmuskosten (3 B(S)/p), dem Transfer der (kleinen) Ergebnisse zwischen den Partitionen und der schnellen Zusammenführung, die schon fast vernachlässigbar ist. Also insgesamt ist die Formel: <span class="math notranslate nohighlight">\(3 B(S)/p lokaler Algorithmus + Transfer (kleiner) Ergebnisse + (schnelle) Zusammenfuehrung\)</span></p>
<p>Dieser Ansatz funktioniert besonders gut für assoziative Aggregationsoperationen wie MIN, MAX, SUM und COUNT, sowie für AVG, das sich als SUM / COUNT berechnen lässt.</p>
<p>Um eine kostspielige zweite Phase zu vermeiden, kann Hashing verwendet werden, um die Gruppierungsspalten zu partitionieren. Alternativ ist auch eine Parallelisierung der Merge-Phase möglich. Beide Ansätze tragen dazu bei, die Gesamtkosten der parallelen Gruppierung und Aggregation zu minimieren und eine effiziente Verarbeitung großer Datenmengen in verteilten Umgebungen zu ermöglichen.</p>
</section>
<section id="parallele-sortierung">
<h3><span class="section-number">6.2.19. </span>Parallele Sortierung<a class="headerlink" href="#parallele-sortierung" title="Permalink to this headline">#</a></h3>
<p>Die parallele Sortierung in verteilten Systemen kann durch verschiedene Ansätze realisiert werden. Beispielsweise wird bei der “<strong>Range partitioned sort</strong>” Methode die Relation anhand der Sortierattribute in unterschiedliche Bereiche partitioniert. Diese Partitionen werden dann lokal sortiert, beispielsweise mit dem Two-Phase Multiway Merge Sort (TPMMS). Die Gesamtkosten dieses Ansatzes setzen sich aus den Kosten für die Partitionierung, den Transfer der Partitionen und der lokalen Sortierung (3 B(S)/p) zusammen. Eine Herausforderung besteht jedoch darin, eine Partitionierung mit gleichgroßen Bereichen zu finden. Die komplette Formel sieht dann wie folgt aus: <span class="math notranslate nohighlight">\(B(S) Partitionierung + B(S) Transfer + 3 B(S)/p lokale Sortierung\)</span></p>
<p>Eine alternative Methode ist der “<strong>Parallel external sort-merge</strong>”. Hier wird die existierende Partitionierung genutzt, und jede Partition wird lokal sortiert. Für die Sortierung kann auch wieder der Two-Phase Multiway Merge Sort (TPMMS) genutzt werden. Die sortierten Partitionen müssen dann gemergt werden, wobei die Kosten für den Pair-wise Merge aus den lokalen Sortierungen, dem Transfer der Daten und dem lokalen Merge resultieren.
Dies kann durch einen log2§-stufigen Merge-Prozess erfolgen, wobei p die Anzahl der beteiligten Knoten ist. Die Gesamtkosten für den Merge können durch Multi-way Merge-Verfahren weiter optimiert werden. Zusammen ergibt sich für die Kosten des Pair-wise Merge die folgende Formel: <span class="math notranslate nohighlight">\(3 B(S)/p lokale Sortierung + log2(p)*B(S)/2 Transfer + log2(p)*B(S) lokaler Merge\)</span></p>
</section>
<section id="symmetric-fragment-and-replicate-join">
<h3><span class="section-number">6.2.20. </span>Symmetric Fragment-and-Replicate Join<a class="headerlink" href="#symmetric-fragment-and-replicate-join" title="Permalink to this headline">#</a></h3>
<p>Der <strong>Symmetric Fragment-and-Replicate Join</strong> ist eine spezielle Methode, um das Joinen von Relationen R und S effizienter zu gestalten. Im herkömmlichen Fall müsste jedes Tupel des kartesischen Produkts betrachtet werden, was in parallelen DBMS dazu führen würde, dass jede Partition von R mit jeder Partition von S kombiniert werden müsste.</p>
<p>Bei der Symmetric Fragment-and-Replicate Methode wird R in m Partitionen und S in n Partitionen fragmentiert. Diese Partitionen werden dann repliziert, wobei jede Partition von R n-mal und jede Partition von S m-mal repliziert wird. Auf einem System mit m * n = p Knoten kann dann jeder Knoten lokal genau ein Partitionspaar von R und S verbinden.</p>
<a class="reference internal image-reference" href="../_images/Symmetric-Fragment-and-Replicate-Join.png"><img alt="Symmetric-Fragment-and-Replicate-Join" src="../_images/Symmetric-Fragment-and-Replicate-Join.png" style="width: 400px;" /></a>
<p>Die <strong>Gesamtkosten</strong> dieses Algorithmus setzen sich aus den Fragmentierungskosten, den Transferkosten und den lokalen Join-Kosten zusammen. Die Fragmentierungskosten beinhalten die Größe von R und S (<span class="math notranslate nohighlight">\(B(R) + B(S)\)</span>), während die Transferkosten die Kosten für das Übertragen der replizierten Partitionen darstellen mit <span class="math notranslate nohighlight">\(\frac{B(R)}{m} * n + \frac{B(S)}{n} * m\)</span>. Der lokale Join wird auf jedem Knoten durchgeführt. Die Kosten variieren je nach Wahl des Joins.
Hier seien die Kosten nochmal zusammengefasst dargestellt:</p>
<ul class="simple">
<li><p>Fragementierungskosten: <span class="math notranslate nohighlight">\(B(R) + B(S)\)</span></p></li>
<li><p>Transferkosten: <span class="math notranslate nohighlight">\(\frac{B(R)}{m} * n + \frac{B(S)}{n} * m\)</span></p></li>
<li><p>Kosten des lokalen Joins: <span class="math notranslate nohighlight">\(???\)</span></p></li>
</ul>
<p>Der Parallel-Join-Algorithmus bietet den Vorteil, dass er für alle Join-Varianten, einschließlich Theta-Joins, funktioniert.</p>
</section>
<section id="asymmetric-fragment-and-replicate-join">
<h3><span class="section-number">6.2.21. </span>Asymmetric Fragment-and-Replicate Join<a class="headerlink" href="#asymmetric-fragment-and-replicate-join" title="Permalink to this headline">#</a></h3>
<p>Der <strong>Asymmetric Fragment-and-Replicate Join</strong> ist eine Optimierungsmöglichkeit, insbesondere wenn die Relation S deutlich kleiner als die Relation R ist. Die grundlegende Idee besteht darin, die existierende Partitionierung von R zu nutzen und die Relation S auf jedem Knoten zu replizieren.</p>
<a class="reference internal image-reference" href="../_images/Asymmetric-Fragment-and-Replicate-Join.png"><img alt="Asymmetric-Fragment-and-Replicate-Join" src="../_images/Asymmetric-Fragment-and-Replicate-Join.png" style="width: 200px;" /></a>
<p>Die Kosten dieses Ansatzes setzen sich hauptsächlich aus den Transferkosten zusammen, da S auf jedem Knoten repliziert wird. Die Formel <span class="math notranslate nohighlight">\(p * B(S)\)</span> beschreibt die Transferkosten, wobei p die Anzahl der beteiligten Knoten und <span class="math notranslate nohighlight">\(B(S)\)</span> die Größe von S ist. Die Kosten sind somit insgesamt:</p>
<ul class="simple">
<li><p>Transferkosten: <span class="math notranslate nohighlight">\(p*B(S)\)</span></p></li>
<li><p>Lokaler Join: <span class="math notranslate nohighlight">\(???\)</span></p></li>
</ul>
<p>Es ist wichtig zu beachten, dass der Asymmetric Fragment-and-Replicate Join als ein <strong>Spezialfall</strong> des Symmetric Fragment-and-Replicate Algorithmus betrachtet werden kann, bei dem m (Anzahl der Partitionen von R) gleich p ist und n (Anzahl der Partitionen von S) gleich 1 ist. Dieser Ansatz ist besonders effizient, wenn die Größe von S im Vergleich zu R vernachlässigbar ist und ermöglicht eine optimierte Verarbeitung von Join-Operationen.</p>
</section>
<section id="parallele-equi-joins">
<h3><span class="section-number">6.2.22. </span>Parallele Equi-Joins<a class="headerlink" href="#parallele-equi-joins" title="Permalink to this headline">#</a></h3>
<p><strong>Parallele Equi-Joins</strong>, insbesondere Natural Joins und Equi-Joins, lassen sich effizient parallelisieren. Die grundlegende Idee besteht darin, die Relationen R und S mit derselben Partitionierungsstrategie, anhand des Join-Schlüssels, zu partitionieren.</p>
<p>Durch diese Vorgehensweise landen alle Tupel aus R und S mit dem gleichen Joinattribut auf den gleichen Knoten. Dadurch sind keine weiteren Broadcasts oder Replikationen notwendig sind. Die Joins können somit lokal auf den jeweiligen Knoten ausgeführt werden.</p>
<p>Es gibt drei Varianten von parallelen Equi-Joins, die auf der existierenden Partitionierung aufbauen. Die Strategien bieten verschiedene Möglichkeiten, um Join-Operationen effizient in verteilten Systemen durchzuführen, wobei die Auswahl zwischen den Varianten von der vorhandenen Partitionierung und den spezifischen Anforderungen abhängt.</p>
<ol class="arabic simple">
<li><p><strong>Co-Located Join:</strong> Wenn sowohl die Relationen R als auch S bereits anhand des Joinattributs partitioniert sind, ermöglicht der Co-Located Join eine effiziente lokale Durchführung der Join-Operation. Es ist keine Neupartitionierungn notwendig. Die Kosten für diese Variante sind in erster Linie durch die lokalen Join-Kosten geprägt:</p></li>
</ol>
<ul class="simple">
<li><p>Lokale Joinkosten: <span class="math notranslate nohighlight">\(???\)</span></p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Directed Join:</strong> Falls nur eine der Relationen, beispielsweise R, anhand des Joinattributes partitioniert ist, kann der Directed Join angewendet werden. Hier wird die andere Relation, in diesem Fall S, neu partitioniert, um die gleiche Partitionierung wie R zu erhalten. Die Kosten setzen sich aus den Partitionierungskosten von S und den Transferkosten der neu partitionierten Daten zusammen, gefolgt von den lokalen Joinkosten.</p></li>
</ol>
<ul class="simple">
<li><p>Partitionierungskosten: <span class="math notranslate nohighlight">\(B(S)\)</span></p></li>
<li><p>Transferkosten: <span class="math notranslate nohighlight">\(B(S)\)</span></p></li>
<li><p>Lokale Joinkosten: <span class="math notranslate nohighlight">\(???\)</span></p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Re-Partitioning Join:</strong> Wenn keine der Relationen passend partitioniert ist, erfolgt ein Repartition Join. Beide Relationen R und S werden anhand des Joinattributes neu partitioniert, um eine gemeinsame Partitionierung zu erreichen. Die Kosten umfassen die Partitionierungskosten beider Relationen, die Transferkosten für die neu partitionierten Daten und schließlich die lokalen Joinkosten.</p></li>
</ol>
<ul class="simple">
<li><p>Partitionierungskosten: <span class="math notranslate nohighlight">\(B(S) + B(R)\)</span></p></li>
<li><p>Transferkosten: <span class="math notranslate nohighlight">\(B(S) + B(R)\)</span></p></li>
<li><p>Lokale Joinkosten: <span class="math notranslate nohighlight">\(???\)</span></p></li>
</ul>
</section>
<section id="grenzen-der-parallelen-datenbanken">
<h3><span class="section-number">6.2.23. </span>Grenzen der Parallelen Datenbanken<a class="headerlink" href="#grenzen-der-parallelen-datenbanken" title="Permalink to this headline">#</a></h3>
<p>Parallele Datenbanken weisen gewisse Grenzen auf, die ihre Skalierbarkeit beeinträchtigen:</p>
<ol class="arabic simple">
<li><p><strong>Begrenzte Skalierung von Datenbank-Clustern:</strong> Die Skalierbarkeit von Datenbank-Clustern zeigt oft eine abflachende Beschleunigungskurve jenseits von etwa 128 Knoten. Dies liegt daran, dass der Kommunikationsmehraufwand mit zunehmender Knotenzahl die Beschleunigung reduziert. Ein Beispiel hierfür ist das harte Limit von 1000 Knoten für DB2 im Jahr 2010.</p></li>
<li><p><strong>Shared Disk-Architektur:</strong> Shared Disk-Systeme haben Skalierungsgrenzen, da der Bus- und Synchronisationsaufwand mit zunehmender Größe Overhead verursacht. Für Updates entstehen Cache-Coherency-Probleme, während für Lesevorgänge die I/O-Bandbreite begrenzt ist.</p></li>
<li><p><strong>Shared Nothing-Architektur:</strong> Obwohl Shared Nothing-Systeme eine bessere Skalierbarkeit bieten, können sie den Verlust von Knoten nicht leicht kompensieren. In großen Clustern sind Ausfälle häufig, was bedeutet, dass der Verlust von Knoten auch einen Verlust von Daten mit sich bringen kann, es sei denn, die Daten sind repliziert. Die Replikation von Daten bringt jedoch einen Mehraufwand mit sich, da die Konsistenz der replizierten Daten aufrechterhalten werden muss.</p></li>
</ol>
</section>
<section id="wann-passen-traditionelle-datenbanken-nicht-gut">
<h3><span class="section-number">6.2.24. </span>Wann passen traditionelle Datenbanken nicht gut?<a class="headerlink" href="#wann-passen-traditionelle-datenbanken-nicht-gut" title="Permalink to this headline">#</a></h3>
<p>Traditionelle Datenbanken stoßen an ihre Grenzen in verschiedenen Szenarien. Zum Einen sind sie weniger geeignet für die <strong>Analyse unstrukturierter Daten</strong>, insbesondere im Fall von Textdaten oder bei Dokumenten. Wenn kein relationales Schema vorhanden ist, erweisen sich traditionelle Datenbanken ebenfalls als ungeeignet.</p>
<p>Ein weiterer Aspekt ist die <strong>Kosteneffektivität</strong>. Wenn lediglich einfache Hardware vorhanden ist oder veränderbare Cluster mit horizontaler Skalierung (horizontal scaling) benötigt werden, können traditionelle Datenbanken an Effizienz verlieren. Wenn beispielsweise ab und zu ein Server hinzugefügt oder entfernt wird, bedeutet das jedes Mal für die Datenbank eine Umorganisation. Das inkrementelle Wachstum, also das Hinzufügen von Knoten ohne zusätzlichen Aufwand, ist oft eine Herausforderung für herkömmliche Datenbankmodelle.</p>
</section>
<section id="webindex-fur-eine-suchmaschine-beispiel">
<h3><span class="section-number">6.2.25. </span>Webindex für eine Suchmaschine - Beispiel<a class="headerlink" href="#webindex-fur-eine-suchmaschine-beispiel" title="Permalink to this headline">#</a></h3>
<p>Bei der Entwicklung eines Webindex für eine Suchmaschine treten spezifische Anforderungen auf. Die Suchmaschine durchsucht das Internet, speichert Dokumente mit Worten und Links ab. Das Speichern der Daten kann man sich wie folgt vorstellen:</p>
<ul class="simple">
<li><p>Dokumente beinhalten Wörter: <code class="docutils literal notranslate"><span class="pre">(Doc-URL,</span> <span class="pre">[list</span> <span class="pre">of</span> <span class="pre">words])</span></code></p></li>
<li><p>Dokumente beinhalten Links: <code class="docutils literal notranslate"><span class="pre">(Doc-URL,</span> <span class="pre">[Target-URLs])</span></code></p></li>
</ul>
<p>Um einen effizienten Index zu erstellen, werden die Dateien invertiert, was eine Zuordnung von Wörtern zu URLs ermöglicht.</p>
<ul class="simple">
<li><p>Invertiere die Dateien: <code class="docutils literal notranslate"><span class="pre">(word,</span> <span class="pre">[list</span> <span class="pre">of</span> <span class="pre">URLs])</span></code></p></li>
</ul>
<p>Angenommen man möchte mit einer Anfrage nach der ‘Universität Hannover’ suchen. Eine Suchmaschine soll alle möglichen Websiten finden in denen ‘Universität Hannover’ vorkommt und es noch nach Relevanz sortieren. Um es nach der Relevanz sortieren zu können, muss ein Ranking erstellt werden. Dieses wird durch Berechnung eines invertierten Graphen erstellt. Es sind im Prinzip verschiedene Websiten, die auf andere Seiten mit der URL verweisen. Dadurch kann man z.B. die Relevanz von Dokumenten etc. einschätzen.
Ein bekanntes Beispiel ist das Page Rank von Google, das nach Larry Page, dem Co-Founder von Google, benannt worden ist.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">(Doc-URL,</span> <span class="pre">[URLs-pointing-to-it])</span></code></p></li>
</ul>
<p>Traditionelle relationale Datenbankmanagementsysteme (RDBMS) sind jedoch für diese Aufgabe weniger geeignet. Das relationale Schema passt nicht optimal, und der Import sowie die Konvertierung der Dokumente erweisen sich als kostspielig.</p>
<p>RDBMS sind primär für Transaktionsverarbeitung konzipiert und bieten Garantien hinsichtlich absoluter Konsistenz, was für die Analyse von Webdokumenten, die eher read-only sind, nicht unbedingt erforderlich ist. Daher sind alternative Ansätze besser geeignet, um den spezifischen Anforderungen eines Webindex gerecht zu werden.</p>
</section>
<section id="fortlaufende-neuentwicklung">
<h3><span class="section-number">6.2.26. </span>Fortlaufende Neuentwicklung<a class="headerlink" href="#fortlaufende-neuentwicklung" title="Permalink to this headline">#</a></h3>
<p>Die kontinuierliche Weiterentwicklung von Technologien hat dazu geführt, dass führende FAANG-Unternehmen zunehmend auf stark <strong>verteilte Systeme</strong> setzen. Ein bemerkenswertes Beispiel ist Google, das bereits im Jahr 2006 auf 450.000 kostengünstige Commodity-Server setzte, die in Clustern mit 1000 bis 5000 Knoten organisiert waren. Der Fokus bei der Neugestaltung solcher Systeme liegt auf Hochskalierbarkeit und Ausfalltoleranz. Auch hier werden die Daten wieder häufig repliziert und vielen verschiedenen Servern gespeichert, um die <strong>Ausfalltoleranz</strong> zu gewährleisten.</p>
<p>Ein entscheidendes Merkmal ist ein generisches und schemafreies Datenmodell, das die Flexibilität erhöht. Der Einstieg erfolgt oft mit einem Datenablagesystem als Ausgangspunkt. Der nächste Schritt in dieser Entwicklung ist eine verteilte Analyse.
Dieser Ansatz ermöglicht es den Unternehmen, sich an die sich ständig verändernden Anforderungen anzupassen.</p>
</section>
<section id="anforderungen-an-die-speicherung">
<h3><span class="section-number">6.2.27. </span>Anforderungen an die Speicherung<a class="headerlink" href="#anforderungen-an-die-speicherung" title="Permalink to this headline">#</a></h3>
<p>Die Anforderungen an die Speicherung von Daten in modernen Umgebungen sind vielfältig und anspruchsvoll. Insbesondere bei sehr großen Dateien im Bereich von Terabytes bis Petabytes ist eine robuste Speicherlösung entscheidend. Neben der <strong>Skalierbarkeit</strong> sind hohe <strong>Verfügbarkeit</strong> und <strong>Replikation</strong> entscheidend, um Ausfälle zu vermeiden.</p>
<p>Ein weiterer kritischer Punkt ist der hohe <strong>Durchsatz</strong>, wobei Lese- und Schreiboperationen nicht durch andere Server gehen sollen. Idealerweise wird versucht alles lokal zu berechnen.
Um einzelne Ausfallpunkte (<strong>Single Point of Failure</strong>) zu vermeiden, müssen Koordinatoren redundant sein. In diesem Kontext hat das Google Filesystem (GFS) als Referenzpunkt gedient, indem es eine Architektur mit hohem Durchsatz und hoher Verfügbarkeit bereitstellt.</p>
</section>
<section id="the-storage-model-distributed-file-system">
<h3><span class="section-number">6.2.28. </span>The Storage Model – Distributed File System<a class="headerlink" href="#the-storage-model-distributed-file-system" title="Permalink to this headline">#</a></h3>
<p>Das <strong>Storage-Modell</strong> eines verteilten Dateisystems basiert auf mehreren Schlüsselkomponenten. Das Dateisystem selbst ist auf mehrere Knoten, auch DataNodes genannt, verteilt und verfügt über einen gemeinsamen Namensraum für den gesamten Cluster. Die Verwaltung der Metadaten erfolgt auf einem speziellen Knoten, dem NameNode. Das Zugriffsmodell ist als <strong>“write-once-read-many”</strong> konzipiert.</p>
<a class="reference internal image-reference" href="../_images/The-Storage-Model.png"><img alt="The-Storage-Model" src="../_images/The-Storage-Model.png" style="width: 500px;" /></a>
<p>Die Dateien werden in Blöcke von 128 MB aufgeteilt, wobei jeder Block auf mehreren DataNodes repliziert wird. Der Client kann den Standort eines Blocks identifizieren und direkt über das Netzwerk die Daten von einem DataNode anfordern.</p>
<p>Eine Herausforderung besteht jedoch in der begrenzten Bandbreite zum Zugriff auf die Daten. Das Abrufen von Daten aus einem entfernten Speicher ist teurer als lokale Zugriffe (50 MB/s remote access vs. 150-200 MB/s local access).
Um dieses Problem zu umgehen, versucht das Map/Reduce-Framework, Berechnungen möglichst nah an den Daten auszuführen. Die Berechnung zu bewegen ist billiger als die Daten zu bewegen. In diesem Konzept sind die Knoten sowohl für die Speicherung als auch für die Berechnungen verantwortlich.</p>
</section>
<section id="retrieving-and-analyzing-data">
<h3><span class="section-number">6.2.29. </span>Retrieving and Analyzing Data<a class="headerlink" href="#retrieving-and-analyzing-data" title="Permalink to this headline">#</a></h3>
<p>Die Suche und Analyse von Daten in modernen Systemen erfordert eine spezielle Herangehensweise. Die Daten liegen oft in maßgeschneiderten Einheiten (Records) innerhalb von Dateien vor, was ein sehr generisches Datenmodell darstellt. Ein häufig verwendetes Modell ist das <strong>“Key/Value-Modell”</strong>.</p>
<p>Um Analyse- und Transformationsaufgaben durchzuführen, wurden zuvor einfache SQL-Anfragen genutzt. Nun werden direkt ganze Programme geschrieben. Diese Programme müssen jedoch parallel, fehlertolerant und hochskalierbar sein, was die Programmierung erschwert. Hier kommt das <strong>Map/Reduce-Programmiermodell</strong> ins Spiel, das speziell für diese Anforderungen entwickelt wurde. Es ermöglicht die effiziente Verarbeitung von großen Datenmengen, indem es Programme in kleine, parallel ausführbare Aufgaben aufteilt.</p>
</section>
<section id="skalierungsmuster">
<h3><span class="section-number">6.2.30. </span>Skalierungsmuster<a class="headerlink" href="#skalierungsmuster" title="Permalink to this headline">#</a></h3>
<p><strong>Skalierungsmuster</strong> spielen eine entscheidende Rolle bei der Verarbeitung großer Datenmengen. Dieser Prozess kann in verschiedene Phasen unterteilt werden.</p>
<a class="reference internal image-reference" href="../_images/Skalierungsmuster.png"><img alt="Skalierungsmuster" src="../_images/Skalierungsmuster.png" style="width: 300px;" /></a>
<p>In der <strong>Phase 0</strong> erfolgt die Verteilung der Daten, auch als <strong>Split</strong> bezeichnet. Hier werden die Daten in Teilmengen aufgeteilt, um die Verarbeitung zu erleichtern.
In der darauffolgenden <strong>Phase 1</strong> werden Berechnungen auf diesen Teilmengen durchgeführt, was als <strong>Map</strong> bezeichnet wird. Jede Teilmenge wird individuell analysiert oder bearbeitet.
In der <strong>Phase 2</strong> erfolgt die Zusammenführung der Teilmengen, bekannt als <strong>Reduce</strong>. Hier werden die Ergebnisse der vorherigen Phase in einer gemeinsamen Betrachtung zusammengeführt.</p>
<p>Ein <strong>Beispiel</strong> für dieses Muster ist der Two-Phase-Multiway-Mergesort (TPMMS), bei dem in Phase 1 Teile der Daten sortiert werden und in Phase 2 die sortierten Teillisten zusammengeführt werden.</p>
<p>Dieses Skalierungsmuster findet auch Anwendung in verschiedenen Anwendungsfällen wie Datenanalyse oder dem Bau von Indizes. Bei der Datenanalyse werden in Phase 1 Gruppierungen durchgeführt, gefolgt von der Aggregation in Phase 2. Beim Bau von Indizes werden in Phase 1 Teilmengen indiziert und in Phase 2 die Indizes zusammengeführt.</p>
</section>
</section>
<section id="map-reduce-hadoop">
<h2><span class="section-number">6.3. </span>Map Reduce &amp; Hadoop<a class="headerlink" href="#map-reduce-hadoop" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><p>“MapReduce is a programming model and an associated implementation for processing and generating large data sets.”</p>
</div></blockquote>
<p>Das zugrunde liegende <a class="reference external" href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf">Paper</a> zu Map Reduce ist von Jeffrey Dean und Sanjay Ghemawat im Jahr 2006 verfasst worden.</p>
<a class="reference internal image-reference" href="../_images/MapReduce-Simplified-data-processing-on-large-clusters.png"><img alt="MapReduce-Simplified-data-processing-on-large-clusters" src="../_images/MapReduce-Simplified-data-processing-on-large-clusters.png" style="width: 500px;" /></a>
<section id="was-ist-map-reduce">
<h3><span class="section-number">6.3.1. </span>Was ist Map/Reduce?<a class="headerlink" href="#was-ist-map-reduce" title="Permalink to this headline">#</a></h3>
<p><strong>Map/Reduce</strong> ist ein Programmiermodell, das auf Konzepten der funktionalen Programmierung basiert und sich besonders gut für parallele Verarbeitung eignet. Es ermöglicht automatische Parallelisierung und Verteilung von Daten und Berechnungslogik. Diese saubere Abstraktion erleichtert die Programmierung für Entwickler und Entwicklerinnen.</p>
<p>In der <strong>funktionalen Programmierung</strong> erfolgt die Berechnung durch die Evaluierung mathematischer Funktionen. Ein zentrales Prinzip dabei ist die Vermeidung von Zustandsänderungen, also keine Seiteneffekte. Das Ergebnis einer Funktion hängt ausschließlich von den Eingangsparametern ab.</p>
<p><strong>Map</strong> und <strong>Reduce</strong> sind sogenannte <strong>Higher-Order-Funktionen</strong> zweiter Ordnung, die benutzerdefinierte Funktionen als Parameter nutzen und selbst eine Funktion als Ergebnis liefern. Programmierende müssen lediglich zwei Funktionen implementieren: die Map-Funktion, die auf jeden Datensatz angewendet wird, und die Reduce-Funktion, die die aggregierten Ergebnisse zusammenführt.</p>
</section>
<section id="grundbausteine">
<h3><span class="section-number">6.3.2. </span>Grundbausteine<a class="headerlink" href="#grundbausteine" title="Permalink to this headline">#</a></h3>
<p>Die Grundbausteine von Map/Reduce umfassen ein einfaches Datenmodell in Form von Schlüssel/Wert-Paaren <span class="math notranslate nohighlight">\((K \times V)\)</span>, auch als <strong>“key/value pairs”</strong> bekannt. Diese Paare können verschiedene Formen annehmen, wie zum Beispiel (int, string) oder (string, [string]).</p>
<p>Nutzer definieren zwei wesentliche Funktionen im Map/Reduce-Programm. Die <strong>Map-Funktion</strong> <span class="math notranslate nohighlight">\((k', v') \rightarrow list(k_{1}, v_{1})\)</span> wird auf jedes Schlüssel/Wert-Paar angewendet und gibt oft nur ein neues Paar <span class="math notranslate nohighlight">\((k_{1}, v_{1})\)</span> aus.</p>
<p>Die <strong>Reduce-Funktion</strong> <span class="math notranslate nohighlight">\((k_{1}, list(v1)) \rightarrow list(v_{2})\)</span> agiert meist auf ein einzelnes Wertpaar <span class="math notranslate nohighlight">\((v_{2}\)</span>, oft auch mit Rückgabe des ursprünglichen Schlüssels <span class="math notranslate nohighlight">\(k_{1}\)</span>. Dadurch ist eine Verkettung von Map/Reduce-Schritten möglich.</p>
<p>Ein MapReduce-Programm nimmt eine Liste von Schlüssel/Wert-Paaren als Eingabe und gibt eine Liste von Werten als Ausgabe aus. Die Ausgabe erfolgt erst am Ende nachdem sowohl die Map- als auch die Reduce-Funktion ausgeführt wurde.
Dabei stehen Nutzern und Nutzerinnen zwei Herausforderungen bevor: das Entwerfen geeigneter Map- und Reduce-Funktionen sowie die Gewährleistung einer verteilten, fehlertoleranten und effizienten Ausführung des Programms.</p>
</section>
<section id="mapreduce-workflow">
<h3><span class="section-number">6.3.3. </span>MapReduce Workflow<a class="headerlink" href="#mapreduce-workflow" title="Permalink to this headline">#</a></h3>
<p>Der <strong>MapReduce Workflow</strong> beinhaltet einige Schritte. Zunächst werden aus dem Input Datafile ein Key und Value gelesen und der Mapper-Funktion übergeben. Das Ergebnis daraus in Form von einem Key und Value wird an eine Key-Sorter-Funktion übergeben. Diese Funktion gibt einen Key mit einer Liste an Werten an eine Reducer-Funktion weiter. Das Resultat aus der Reducer-Funktion in Form von einem key und value wird in eine Output Datafile geschrieben. Den ganzen Workflow kann man mehrmals wiederholen und aneinander ketten, wenn dies je nach Situation verlangt wird.</p>
<a class="reference internal image-reference" href="../_images/MapReduce-workflow.png"><img alt="MapReduce-workflow" src="../_images/MapReduce-workflow.png" style="width: 500px;" /></a>
<section id="beispiel-1">
<h4><span class="section-number">6.3.3.1. </span>Beispiel 1<a class="headerlink" href="#beispiel-1" title="Permalink to this headline">#</a></h4>
<p><strong>Bestimme für jedes Wort dessen Häufigkeit im Korpus.</strong></p>
<a class="reference internal image-reference" href="../_images/Aufgabe-Häufigkeit-bestimmen.png"><img alt="Aufgabe-Häufigkeit-bestimmen" src="../_images/Aufgabe-Häufigkeit-bestimmen.png" style="width: 500px;" /></a>
<p>Um die Aufgabe zu lösen und für jedes Wort die Häufigkeit im Korpus zu bestimmen, müssen zwei Funktionen geschrieben werden: Eine Map- und eine Reduce-Funktion.</p>
<p>Zunächst wird eine <strong>Map-Funktion</strong> geschrieben, die jedes Wort zusammen mit einer 1 zurückgibt.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">map</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">line</span><span class="p">){</span>
	  <span class="k">for</span> <span class="n">each</span> <span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span>
	     <span class="n">emit</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>	 	 
<span class="p">}</span>
</pre></div>
</div>
<p>Als nächstes wird eine <strong>Reduce-Funktion</strong> programmiert. Diese Funktion bekommt einen Key, in dem Fall word, und eine Liste an Values, hier ist es numbers, übergeben. Zu jedem Wort zählt die Reduce-Funktion die Summe an Einsen in der numbers-Liste. Am Schluss gibt die Funktion das Wort zusammen mit der Häufigkeit des Wortes aus.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reduce</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">numbers</span><span class="p">){</span>
	  <span class="nb">int</span> <span class="nb">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	  <span class="k">for</span> <span class="n">each</span> <span class="p">(</span><span class="n">value</span> <span class="ow">in</span> <span class="n">numbers</span><span class="p">){</span>
	    <span class="nb">sum</span> <span class="o">+=</span> <span class="n">value</span><span class="p">;</span>
	  <span class="p">}</span>
	  <span class="n">emit</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="nb">sum</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="beispiel-2">
<h3><span class="section-number">6.3.4. </span>Beispiel 2<a class="headerlink" href="#beispiel-2" title="Permalink to this headline">#</a></h3>
<a class="reference internal image-reference" href="../_images/MapReduce-Illustrated-2.png"><img alt="MapReduce-Illustrated-2" src="../_images/MapReduce-Illustrated-2.png" style="width: 500px;" /></a>
<p>Von den oberen zwei Sätzen sollen die Häufigkeiten der einzelnen Wörter gezählt werden. Die <strong>Mapper-Funktionen</strong> geben jedes Wort zusammen mit einer 1 zurück. Jede <strong>Reduce-Funktion</strong> bekommt nun die gleichen Wörter, sodass diese die Wörter zusammenzählen kann. Als Ergebnis wird das Wort und dessen Häufigkeit zurückgegeben.</p>
<section id="beispiel-3">
<h4><span class="section-number">6.3.4.1. </span>Beispiel 3<a class="headerlink" href="#beispiel-3" title="Permalink to this headline">#</a></h4>
<p><strong>Bestimme die Liste gemeinsamer Bekannte für jedes Personenpaar.</strong></p>
<p>Im Jahr 2016 hatte Facebook insgesamt 1,4 Milliarden Nutzer und Nutzerinnen. Durchschnittlich hat jeder Nutzer bzw. jede Nutzerin 155 Freunde und Freundinnen. Daraus ergeben sich insgesamt 979.999.999.300.000.000 Paare. Um nun eine Liste gemeinsamer Bekannte für jedes Personenpaar zu erstellen, muss eine Map- und eine Reduce-Funktion geschrieben werden.</p>
<p>Die <strong>Map-Funktion</strong> bekommt eine Person und deren Freundesliste übergeben. Für jeden Freund oder Freundin in der Freundesliste wird ein sortiertes Paar zurückgegeben. Dadurch ist die selbe Person mit dem gleichem Freund oder der gleichen Freundin immer in der selben Anordnung. Zudem wird mit dem Paar noch die Freundesliste mit zurückgegeben.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">map</span><span class="p">(</span><span class="n">person</span><span class="p">,</span> <span class="n">friendlist</span><span class="p">){</span>
    <span class="k">for</span> <span class="n">each</span> <span class="p">(</span><span class="n">friend</span> <span class="ow">in</span> <span class="n">friendlist</span><span class="p">)</span>
        <span class="k">if</span><span class="p">(</span><span class="n">friend</span> <span class="o">&lt;</span> <span class="n">person</span><span class="p">)</span>
	        <span class="n">emit</span><span class="p">(</span><span class="o">&lt;</span><span class="n">friend</span><span class="p">,</span> <span class="n">person</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">friendlist</span><span class="p">);</span>
        <span class="k">else</span> 
            <span class="n">emit</span><span class="p">(</span><span class="o">&lt;</span><span class="n">person</span><span class="p">,</span> <span class="n">friend</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">friendlist</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Die <strong>Reduce-Funktion</strong> bekommt dann als Schlüssel das Freundespaar und als Value die Freundesliste übergeben. Die Reduce-Funktion bildet dann die Schnittmenge der beiden Freundeslisten und gibt diese zurück. So erhält man am Ende eine Liste gemeinsamer Bekannte für jedes Personenpaar.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>reduce(&lt;person1, person2&gt;, friendlists){
    emit(&lt;person1, person2&gt;, friendlist[1] ∩ friendlist[2]);
}
</pre></div>
</div>
<p>Dazu sei noch ein graphisches Beispiel gegeben. Es gibt 5 Personen: A, B, C, D, E und jede Person hat eine Freundesliste.</p>
<p>A -&gt; B C D <br>
B -&gt; A C D E <br>
C -&gt; A B D E <br>
D -&gt; A B C E <br>
E -&gt; B C D <br></p>
<p>Nach dem <strong>Mapping</strong> bekommt man ein Personenpaar als Schlüsselwert. Der Wert ist die gleiche Freundesliste der einzelnen Personen. Jeder Schlüssel taucht zweimal auf.</p>
<p>(A B) -&gt; B C D <br>
(A C) -&gt; B C D <br>
(A D) -&gt; B C D <br>
<br>
(A B) -&gt; A C D E <br>
(B C) -&gt; A C D E <br>
(B D) -&gt; A C D E <br>
(B E) -&gt; A C D E <br>
<br>
(A C) -&gt; A B D E <br>
(B C) -&gt; A B D E <br>
(C D) -&gt; A B D E <br>
(C E) -&gt; A B D E <br>
<br>
(A D) -&gt; A B C E <br>
(B D) -&gt; A B C E <br>
(C D) -&gt; A B C E <br>
(D E) -&gt; A B C E <br>
<br>
(B E) -&gt; B C D <br>
(C E) -&gt; B C D <br>
(D E) -&gt; B C D <br></p>
<p>Nach der <strong>Shuffle</strong> gehören zu jedem Key zwei Freundeslisten. Die Daten werden also gruppiert. Je nach Implementierung wird diese Funktion teilweise auch direkt von der Reduce-Funktion übernommen.</p>
<p>(A B) -&gt; (A C D E) (B C D) <br>
(A C) -&gt; (A B D E) (B C D) <br>
(A D) -&gt; (A B C E) (B C D) <br>
(B C) -&gt; (A B D E) (A C D E) <br>
(B D) -&gt; (A B C E) (A C D E) <br>
(B E) -&gt; (A C D E) (B C D) <br>
(C D) -&gt; (A B C E) (A B D E) <br>
(C E) -&gt; (A B D E) (B C D) <br>
(D E) -&gt; (A B C E) (B C D) <br></p>
<p>Durch die <strong>Reduce</strong>-Funktion wird eine Schnittmenge der beiden Freundeslisten gebildet und zurückgegeben.</p>
<p>(A B) -&gt; (C D) <br>
(A C) -&gt; (B D) <br>
(A D) -&gt; (B C) <br>
(B C) -&gt; (A D E) <br>
(B D) -&gt; (A C E) <br>
(B E) -&gt; (C D) <br>
(C D) -&gt; (A B E) <br>
(C E) -&gt; (B D) <br>
(D E) -&gt; (B C) <br></p>
</section>
</section>
<section id="parallel-dbms-vs-map-reduce">
<h3><span class="section-number">6.3.5. </span>Parallel DBMS vs. Map/Reduce<a class="headerlink" href="#parallel-dbms-vs-map-reduce" title="Permalink to this headline">#</a></h3>
<p>Prinzipiell ist bei den aufgelisteten Punkten alles möglich zu programmieren. Zum Beispiel kann auch für Map/Reduce ein Schema Support programmiert werden. Es entstehen dann Mischformen zwischen Parallelen DBMS und Map/Reduce, die die Vor- und Nachteile gegenseitig aushebeln.</p>
<a class="reference internal image-reference" href="../_images/Parallel_DBMS_vs_Map_Reduce.png"><img alt="Parallel_DBMS_vs_Map_Reduce" src="../_images/Parallel_DBMS_vs_Map_Reduce.png" style="width: 500px;" /></a>
</section>
<section id="relational-operators-as-map-reduce-jobs">
<h3><span class="section-number">6.3.6. </span>Relational Operators as Map/Reduce jobs<a class="headerlink" href="#relational-operators-as-map-reduce-jobs" title="Permalink to this headline">#</a></h3>
<p><strong>Gruppierung</strong></p>
<p>Das Beispiel zeigt wie man eine SQL-Anfrage auf Map/Reduce abbilden könnte.
Die <strong>SQL-Anfrage</strong> möchte für jedes Jahr die Summe der Preise in den United States (US) erfahren.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span>SELECT year, SUM(price)
FROM sales
WHERE area_code = “US”
GROUP BY year
</pre></div>
</div>
<p><strong>Map</strong> findet zunächst das Jahr des Tupels heraus. Dann fügt er jedes Mal bei einem ‘US’-Tupel das Jahr mit dem Preis aus dem Tupel zusammen.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>map(key, tuple) {
    int year = YEAR(tuple.date);
	if (tuple.area_code = “US”)
        emit(year, {‘price’ =&gt; tuple.price });
}
</pre></div>
</div>
<p><strong>Reduce</strong> rechnet dann die Preise für jedes Jahr zusammen.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reduce</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">tuples</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">double</span> <span class="n">sum_price</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">foreach</span> <span class="p">(</span><span class="nb">tuple</span> <span class="ow">in</span> <span class="n">tuples</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">sum_price</span> <span class="o">+=</span> <span class="nb">tuple</span><span class="o">.</span><span class="n">price</span><span class="p">;</span>
    <span class="p">}</span>
	<span class="n">emit</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">sum_price</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Sortierung</strong></p>
<p>Die <strong>SQL-Anfrage</strong> sortiert die Sales nach dem jeweiligen Jahr.</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span>
<span class="k">FROM</span><span class="w"> </span><span class="n">sales</span><span class="w"> </span>
<span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="k">year</span><span class="w"></span>
</pre></div>
</div>
<p>Die <strong>Map</strong>-Funktion gibt das Jahr geteilt durch 10 zurück. Mit dem Teilen durch 10 kann man die Datenmenge kontrollieren, die an die Reduce-Funktion geht.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">map</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">emit</span><span class="p">(</span><span class="n">YEAR</span><span class="p">(</span><span class="nb">tuple</span><span class="o">.</span><span class="n">date</span><span class="p">)</span> <span class="n">div</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Die <strong>Reduce</strong>-Funktion sortiert dann die Teillisten, die sie erhält.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reduce</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">tuples</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">emit</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">sort</span><span class="p">(</span><span class="n">tuples</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="hadoop-a-map-reduce-framework">
<h3><span class="section-number">6.3.7. </span>Hadoop – A map/reduce Framework<a class="headerlink" href="#hadoop-a-map-reduce-framework" title="Permalink to this headline">#</a></h3>
<a class="reference internal image-reference" href="../_images/hadoop.png"><img alt="hadoop" src="../_images/hadoop.png" style="width: 500px;" /></a>
<p><strong>Hadoop</strong> ist ein Map/Reduce-Framework und zählt zu den Apache Top-Level-Projekten. Die Open-Source-Lösung ist in Java geschrieben. Der Hadoop-Stack bietet verschiedene Komponenten, darunter ein verteiltes Dateisystem (HDFS). Vergleichbar ist es mit dem Google File System. Außerdem bietet es eine Map/Reduce-Engine, Datenverarbeitungssprachen wie Pig Latin und Hive SQL sowie weitere Pakete.</p>
<p>Hadoop kann auf Betriebssystemen wie Linux, Mac OS/X, Windows und Solaris ausgeführt werden. Es ist darauf ausgelegt, auf kostengünstiger Hardware, sogenannter Commodity-Hardware, zu laufen.</p>
</section>
<section id="hadoop-distributed-file-system-hdfs">
<h3><span class="section-number">6.3.8. </span>Hadoop Distributed File System (HDFS)<a class="headerlink" href="#hadoop-distributed-file-system-hdfs" title="Permalink to this headline">#</a></h3>
<p>Das <strong>Hadoop Distributed File System</strong> (HDFS) folgt einer Koordinator-Arbeiter-Architektur und wurde auf Grundlagen des Google File Systems (GFS) entwickelt.</p>
<p>Der <strong>HDFS-Koordinator</strong>, auch als <strong>“NameNode”</strong> bekannt, ist verantwortlich für die Verwaltung aller Metadaten des Dateisystems. Er loggt und merged Transaktionen beim Systemstart, kontrolliert den Lese- und Schreibzugriff auf Dateien, verwaltet die Replikation von Datenblöcken und kann selbst repliziert werden.</p>
<p>Die <strong>HDFS-Arbeiter</strong>, auch <strong>“DataNode”</strong> genannt, kommunizieren periodisch mit dem NameNode über Herzschläge (heartbeats). Der DataNode fragt periodisch nach, ob dieser eine Aufgabe hat und wenn ja, erledigt er sie. DataNodes bedienen auch Lese- und Schreibanfragen von Clients, führen Replikationsaufgaben aus, die vom NameNode kommen, und haben einen Standard-Replikationsfaktor von 3.</p>
<a class="reference internal image-reference" href="../_images/HDFS.png"><img alt="HDFS" src="../_images/HDFS.png" style="width: 350px;" /></a>
</section>
<section id="hadoop-map-reduce-engine">
<h3><span class="section-number">6.3.9. </span>Hadoop Map/Reduce Engine<a class="headerlink" href="#hadoop-map-reduce-engine" title="Permalink to this headline">#</a></h3>
<p>Die Hadoop Map/Reduce Engine ermöglicht die Ausführung von Jobs wie in Unix-Pipelines. Ähnlich dem Befehl <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">*</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">|</span> <span class="pre">sort</span> <span class="pre">|</span> <span class="pre">uniq</span> <span class="pre">-c</span> <span class="pre">|</span> <span class="pre">cat</span> <span class="pre">&gt;</span> <span class="pre">output</span></code> werden in Map/Reduce Jobs in verschiedene Phasen unterteilt: <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">|</span> <span class="pre">Map</span> <span class="pre">|</span> <span class="pre">Sort</span> <span class="pre">&amp;</span> <span class="pre">Shuffle</span> <span class="pre">|</span> <span class="pre">Reduce</span> <span class="pre">und</span> <span class="pre">Output</span></code></p>
<p><strong>Ablauf</strong></p>
<ol class="arabic simple">
<li><p><strong>Input Phase</strong>: Sie generiert eine Anzahl von FileSplits aus den Inputfiles (eine pro Map-Task).</p></li>
<li><p><strong>Map Phase</strong>: Die Phase führt eine benutzerdefinierte Funktion (User Defined Function: UDF) aus, um Input-Schlüssel/Wert-Paare in neue Paare abzubilden.</p></li>
<li><p><strong>Sort &amp; Shuffle Phase</strong>: Sie sortiert und verteilt diese Paare auf Output-Knoten.</p></li>
<li><p><strong>Reduce Phase</strong>: Die Phase kombiniert alle Paare mit dem selben Schlüssel zu neuen Paaren.</p></li>
<li><p><strong>Output Phase</strong>: Sie schreibt am Ende die Ergebnispaare in das HDFS.</p></li>
</ol>
<p>Alle Phasen sind verteilt und bestehen aus vielen Tasks. Das Map/Reduce-Framework übernimmt das Scheduling der Tasks auf dem Cluster und kümmert sich um die Wiederherstellung im Falle eines Knotenausfalls. Man kann den Knoten einfach neustarten ohne einen Datenverlust zu risikieren.</p>
<a class="reference internal image-reference" href="../_images/Hadoop-Map_Reduce-engine.png"><img alt="Hadoop-Map_Reduce-engine" src="../_images/Hadoop-Map_Reduce-engine.png" style="width: 500px;" /></a>
<p>Die Coordinator/Worker-Architektur des Map/Reduce-Frameworks in Hadoop besteht aus einem Map/Reduce Coordinator, bekannt als JobTracker, und den Map/Reduce Workern, auch als TaskTracker bezeichnet.</p>
<p>Der JobTracker übernimmt die Koordination von Jobs, die von Clients eingereicht werden. Er weist sowohl Map- als auch Reduce-Tasks den TaskTrackers zu. Zudem überwacht er den Ausführungsstatus und führt fehlgeschlagene Tasks erneut aus.</p>
<p>Die TaskTracker, als Worker-Komponente, ist für die Ausführung von Map- und Reduce-Tasks verantwortlich. Zusätzlich verwaltet sie die Ablage, Sortierung und Ausgabe von Zwischenergebnissen.</p>
</section>
<section id="fehlertoleranz">
<h3><span class="section-number">6.3.10. </span>Fehlertoleranz<a class="headerlink" href="#fehlertoleranz" title="Permalink to this headline">#</a></h3>
<p>In Umgebungen mit vielen Daten, die in langen Prozessen auf zahlreichen Maschinen verarbeitet werden, ist Verteilung unerlässlich. Ausfälle sind in einem solchen Szenario unvermeidlich und eher die Regel als die Ausnahme.</p>
<p>Um damit umzugehen, setzt Hadoop auf ein <strong>verteiltes Dateisystem (DFS / HDFS)</strong>, das Daten verteilt und durch Replikation fehlertolerant speichert. Dies ermöglicht eine redundante Verfügbarkeit der Eingabedaten.</p>
<p>Die Speicherung von Zwischenergebnissen im DFS mag zwar aufwändig sein, jedoch erleichtert dies die Fehlererholung während des laufenden Prozesses. Die Daten können einfach und schnell wiederhergestellt werden.</p>
<p>Im Falle von Abstürzen werden diese erkannt, indem das periodische Signal <strong>(Heartbeat)</strong> ausbleibt. Mapper oder Reducer können dann auf einer anderen Maschine neu gestartet werden, wobei auf Replikate des ursprünglichen Inputs zurückgegriffen wird.</p>
</section>
<section id="wann-sollte-man-hadoop-nutzen">
<h3><span class="section-number">6.3.11. </span>Wann sollte man Hadoop nutzen?<a class="headerlink" href="#wann-sollte-man-hadoop-nutzen" title="Permalink to this headline">#</a></h3>
<p>Hadoop eignet sich besonders gut für Anwendungen, die <strong>große Mengen von Daten</strong> verarbeiten müssen. Dazu zählen zum Beispiel Data Mining, Model Tuning und Textverarbeitung.</p>
<p>Allerdings ist Hadoop weniger geeignet für Punktanfragen, da dies mit einem hohen Overhead und einer hohen Latenz verbunden ist.
Die Architektur von Hadoop basiert auf der Verarbeitung unabhängig ausführbarer Aufgaben (Tasks), was die Kommunikation zwischen Prozessen erschwert. Daher sind Anwendungen, die eine starke Kommunikation zwischen Prozessen erfordern, besser mit anderen Ansätzen zu lösen.</p>
</section>
<section id="in-der-praxis-komplexe-optimierte-mapreduce-workflows">
<h3><span class="section-number">6.3.12. </span>In der Praxis: Komplexe (optimierte) MapReduce Workflows<a class="headerlink" href="#in-der-praxis-komplexe-optimierte-mapreduce-workflows" title="Permalink to this headline">#</a></h3>
<p>In der praktischen Anwendung von MapReduce treten komplexe und optimierte Workflows auf, die über einfache Map- und Reduce-Schritte hinausgehen. Map/Reduce-Funktionen werden aneinander gekettet. Neue relationale Operatoren wie Join, Cross, Union etc. werden nun auch genutzt.</p>
<p>Die Effizienz solcher Workflows wird durch <strong>Planoptimierung</strong> und <strong>Re-optimierung</strong> gesteigert.
<strong>Scheduling</strong> und <strong>Lastbalancierung</strong> sind auch entscheidend, um die Ressourcen im Cluster effektiv zu nutzen und sicherzustellen, dass die Aufgaben gleichmäßig verteilt werden.
Außerdem ist die <strong>Cross-Plattform-Ausführung</strong> noch von hoher Wichtigkeit, insbesondere wenn verschiedene Technologien oder Plattformen innerhalb desselben Workflows integriert werden müssen.</p>
<a class="reference internal image-reference" href="../_images/Komplexe-optimierte-MapReduce-workflows.png"><img alt="Komplexe-optimierte-MapReduce-workflows" src="../_images/Komplexe-optimierte-MapReduce-workflows.png" style="width: 500px;" /></a>
</section>
<section id="hadoop-vs-parallel-dbms">
<h3><span class="section-number">6.3.13. </span>Hadoop vs. Parallel DBMS<a class="headerlink" href="#hadoop-vs-parallel-dbms" title="Permalink to this headline">#</a></h3>
<p>Turing Award Gewinner <strong>Michael Stonebreaker</strong> war in den Anfangsjahren ein großer Kritiker von Hadoop. Sein berühmtes <a class="reference external" href="https://cacm.acm.org/blogs/blog-cacm/149074-possible-hadoop-trajectories/fulltext">Paper aus dem Jahr 2012</a> kritisiert die Nutzung von Hadoop für Pilotprojekte, die dann in der Praxis Leistungsprobleme feststellen werden und am Ende wieder zu echten parallelen DBMS wechseln müssen.</p>
<a class="reference internal image-reference" href="../_images/Hadoop-vs-Parallel-DBMS.png"><img alt="Hadoop-vs-Parallel-DBMS" src="../_images/Hadoop-vs-Parallel-DBMS.png" style="width: 300px;" /></a>
<a class="reference internal image-reference" href="../_images/Hadoop_vs_Parallel_DBMS.png"><img alt="Hadoop_vs_Parallel_DBMS" src="../_images/Hadoop_vs_Parallel_DBMS.png" style="width: 500px;" /></a>
<p>Im Jahr 2014 hat Michael Stonebreaker mit einem weiterem <a class="reference external" href="https://cacm.acm.org/blogs/blog-cacm/177467-hadoop-at-a-crossroads/fulltext">Paper</a> Hadoop noch stärker kritisiert mit der Aussage, dass Google Hadoop erst veröffentlicht hat, nachdem sie ein neues System verwendet haben.</p>
<a class="reference internal image-reference" href="../_images/Hadoop_vs_Parallel_DBMS_2.png"><img alt="Hadoop_vs_Parallel_DBMS_2" src="../_images/Hadoop_vs_Parallel_DBMS_2.png" style="width: 500px;" /></a>
<p>In der Tat haben sich viele Hadoop System im Laufe der Zeit sich Datenbanksystemen angenähert und Funktionen übernommen. 2014 ist vom Map/Reduce System wenig übrig geblieben mit Ausnahme des Filesystems.</p>
</section>
<section id="in-der-praxis-viele-bibliotheken">
<h3><span class="section-number">6.3.14. </span>In der Praxis: Viele Bibliotheken<a class="headerlink" href="#in-der-praxis-viele-bibliotheken" title="Permalink to this headline">#</a></h3>
<p>In der praktischen Anwendung von MapReduce gibt es viele Bibliotheken, die von Hadoop ausgehen und den Funktionsumfang erweitern.</p>
<p>Hadoop bildet den Ausgangspunkt und ist in Java geschrieben. Es stellt Basis-Bibliotheken für ein verteiltes Dateisystem sowie Scheduling und Monitoring bereit.</p>
<p>Zu den Erweiterungen gehören Bibliotheken für Service- und Cluster-Verwaltung, Datenspeicher, Datenbanken und Anfragesprachen. Darüber hinaus existieren spezialisierte Bibliotheken für komplexe Verfahren, Datenstromverarbeitung und vieles mehr.</p>
<p>Einige prominente <strong>Beispiele</strong> für solche Bibliotheken sind:</p>
<ul class="simple">
<li><p>Common und MapReduce (Teil von Hadoop)</p></li>
<li><p>HDFS (Hadoop Distributed File System)</p></li>
<li><p>Yarn (Yet Another Resource Negotiator)</p></li>
<li><p>ZooKeeper (Service für verteilte Koordination)</p></li>
<li><p>HBase (NoSQL-Datenbank)</p></li>
<li><p>Pig, Hive, Phoenix (Datenverarbeitungssprachen)</p></li>
<li><p>Mahout, Giraph, Solr (Bibliotheken für maschinelles Lernen, Graphenverarbeitung, Suche)</p></li>
<li><p>Kafka, Flink, Spark (Bibliotheken für Datenstromverarbeitung und parallele Datenverarbeitung)</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/Viele-Bibliotheken.png"><img alt="Viele-Bibliotheken" src="../_images/Viele-Bibliotheken.png" style="width: 400px;" /></a>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "LUH-DBS/GDBS_Script",
            ref: "main/",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./06"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../05/optimierung.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>Optimierung</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Prof. Dr. Ziawasch Abedjan<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>