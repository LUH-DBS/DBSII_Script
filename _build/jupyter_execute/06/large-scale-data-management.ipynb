{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aba103-11e1-43f4-98a3-fa756243c505",
   "metadata": {},
   "source": [
    "# Large Scale Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981228f1-614d-415e-bb54-112dd93352e0",
   "metadata": {},
   "source": [
    "In diesem Kapitel geht es insbesondere darum, die Verfahren und Datenbankoperationen, die wir bisher kennengelernt haben, hinsichtlich paralleler Verarbeitung zu betrachten und auch die Kostenelemente, die dann eine Rolle spielen. \n",
    "\n",
    "Beim Large Scale Data Management geht es um sehr große Datenmengen. Da reicht es dann nicht mehr, nur eine Datenbank zu haben, man muss nun auch über die Verteilung, Server und Nebenläufigkeiten nachdenken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac2534-e148-4a3d-9ea9-a5d8768021d2",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"pictures/Large-scale-Data-Management.png\" alt=\"Large-scale-Data-Management\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Zur Wiederholung einmal die Frage: Was ist Big Data? Big Data wird anhand von Dimensionen spezifiziert- die sogenannten V's: **Volume** (Menge von Daten), **Velocity** (Schnelligkeit der Datenverarbeitung), **Variety** (Heterogenität der Daten), **Verocity** (Daten, bei denen die Korrektheit ungewiss ist) und **Value** (die Wertigkeit der Daten).\n",
    "\n",
    "Nun gibt es Big Data in zwei Varianten - **Operational** und **Analytic**. In der ersten Variante geht es um operationelle Sachen, also dem Transaktionsmanagement. In der zweiten Variante geht es darum, Daten zu analysieren, Insights aus Daten herzustellen und neue Erkenntnisse zu gewinnen.\n",
    "\n",
    "Zur Verdeutlichung, über was für Datenmengen wir bei Big Data reden:\n",
    "\n",
    "Google ist ein klassisches Beispiel für ein Datenproduzierendes und -verwaltendes Unternehmen. Dort werden jeden Tag 20 PB an Daten verarbeitet. Das sind Billionen von Zeilen, Tausende/Millionen Spalten und Tabellen, aber auch strukturierte Daten wie Text, Bilder und Videos. Würde man versuchen, diese 20 PB mit 50 MB/s zu lesen, würde das 12 Jahre dauern. Aus diesem Grund werden die Daten partioniert und verteilt verarbeitet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b4e51",
   "metadata": {},
   "source": [
    "## Key enabler: Virtulization\n",
    "\n",
    "Die beiden Varianten Operational und Analytic lassen sich mit der Virtualization managen. Hierbei versucht man entweder ein logisches System auf viele physische Systeme (Load Balancing) oder andersherum mehrere logische Systeme auf ein physisches System abzubilden (Multy-Tenancy).\n",
    "\n",
    "\n",
    "<img src=\"pictures/Virtualization.png\" alt=\"Virtualization\" width=\"500\" style=\"background-color: white;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b2968-007a-405a-b4d5-3e4d36a6405e",
   "metadata": {},
   "source": [
    "## Parallel Data Processing\n",
    "\n",
    "<img src=\"pictures/Overview_3.png\" alt=\"Overview_3\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3562a51-ec56-4ad4-8501-03942de6c2de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Was bisher geschah: Serielle Verarbeitung/Single Threaded\n",
    "\n",
    "Bisher haben wir immer von einem Computer mit mehreren Festplatten gesprochen und damit auch ein wenig über parallele Plattenzugriffe. Diese hatten insbesondere auch immer nur einen Kern. Das heißt, bei jeder Operation wurden die Blöcke nacheinander durch nur einen Kern abgearbeitet. Außerdem spielten auch Synchronisation und Kommunikation keine Rolle, da Anfragen in nur einem Thread bearbeitet wurden. Dies wollen wir nun erweitern.\n",
    "\n",
    "<img src=\"pictures/serial-single-threaded.png\" alt=\"serial-single-threaded\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Was wir verschwiegen haben...\n",
    "\n",
    "Das Datenvolumen wächst stetig. Data Warehouses mit 1 EB sind nicht untypisch. Manche Organisationen produzieren täglich mehr als 1 PB an neuen Daten. Das entspricht 1.000.000.000.000.000 Byte (1 quadrillion).\n",
    "Manche Systeme, wie beispielsweise Finanzinstitute, Onlineshops und soziale Netzwerke, haben einen sehr hohen Durchsatz (throughput) von Transaktionen. \n",
    "Deshalb ist es wichtig zu überlegen, wie die Zugriffe über die Netzwerke verteilt werden. \n",
    "Auch Analyseanfragen werden immer komplexer. Eine statistische Mustererkennung ist teuer und über die Daten muss mehrfach iteriert werden. Da reicht eine Single-CPU- oder Single-Node-Architektur nicht mehr aus und auch Moore's Law ist hier nicht mehr anwendbar. Die Lösung: **Parallele Datenverarbeitung**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae1665",
   "metadata": {},
   "source": [
    "### Grundlagen der Parallelen Datenverarbeitung (Parellel Processing)\n",
    "\n",
    "Bei der parallelen Datenverarbeitung kommt Amdahl's law zum Einsatz, welches die Grenzen bei der parallelen Beschleunigung definiert. Es gibt außerdem verschiedenen Stufen der Parallelisierung, mit denen auf unterschiedlichen Ebenen parallelisiert werden kann. Des Weiteren existieren noch verschiedenen Varianten der Anfrage-Parallelisierung. Dadurch können mehrere Anfragen parallel verarbeitet werden (Inter-Query) oder nur eine Anfrage (Intra-Query).  \n",
    "\n",
    "### Parallel Speedup – Amdahl‘s law\n",
    "\n",
    "Die Frage die sich bei Amdahl's law stellt ist, wie viel wir an Geschwindigkeit überhaupt dazugewinnen können. Berechnen lässt sich das zum einen mit der sequentiellen Laufzeit $T_1$ (1 Prozessor) und zum anderen mit der parallelen Laufzeit $T_p$ (*p* Prozessoren): $S_p$ = $\\frac{T_1}{T_p}$ . Die maximale Beschleunigung ist durch den nicht-parallelisierbaren Anteil des Programms begrenzt. Wie hoch diese ist, lässt sich folgendermaßen berechnen: $S_p$ = $\\frac{1}{(1 - f) + \\frac{f}{p}}$ . <br>\n",
    "*f* entspricht prozentual dem parallelisierbaren Anteil. Die ideale Beschleunigung wäre *S = p* für *f = 1*. Oft ist *f* < 1 während *S* durch eine Konstante begrenzt wird. Beispiel: *f = 0,9* und 10/20 Server. $S_p$ = $\\frac{1}{(1 - f) + \\frac{f}{p}}$ = $\\frac{1}{(1 - 0,9) + \\frac{0,9}{10}} \\approx $ 5,3 und $S_p$ = $\\frac{1}{(1 - 0,9) + \\frac{0,9}{20}} \\approx $ 6,9 . Lassen wir hier unsere Prozessoren gegen unendlich laufen, ist unser $S_p$ = 10. Das bedeutet, wir können weitere Server hinzufügen, aber es bleibt bei der 10-fachen Geschwindigkeit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb1bca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Parallel Speedup\n",
    "\n",
    "Hier sehen wir, wie sich die parallele Beschleunigung nach Amdahl's law je nach Prozessorzahl verhält. \n",
    "\n",
    "<img src=\"pictures/Parallel-Speedup.png\" alt=\"Parallel-Speedup\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Parallelisierungsstufen auf der Hardware\n",
    "\n",
    "Es gibt unterschiedliche Stufen der Parallelisierung auf der Hardware. Zum einen gibt es *Instruction-level parallelism* (Prozessoranweisungen). Dabei werden Prozessorbefehle durch die CPU-Architektur automatisch parallelisiert. Zum anderen gibt es *Data parallelism* (Daten). Jeder Prozessor verarbeitet die gleichen Befehle auf seiner eigenen Partition der Daten. Dadurch können unterschiedliche Daten parallel verarbeitet werden, beispielsweise durch verteilte Schleifeniterationen auf mehreren Prozessoren oder GPU processing. Auf der letzten Stufe der Parallelisierung haben wir *Task parallelism* (Aufgaben). Hierbei erhält jeder Prozessor/Knoten eine andere Aufgabe.\n",
    "\n",
    "### Varianten der Anfrage-Parallelisierung\n",
    "\n",
    "Es existieren unterschiedliche Varianten der Anfrage-Parallelisierung. Die erste Variante ist *Inter-Query parallelism* (mehrere nebenläufige Anfragen). Dies ist wichtig für eine effiziente Ressourcennutzung. Wartet eine Anfrage z.B. auf I/O, kann in der Zeit eine andere Anfrage ausgeführt werden. Dies erfordert *concurrency control*, also das Sperren, um Transaktionseigenschaften zu garantieren. Das ist auch wichtig für OLTP. Die zweite Variante ist *Intra-Query parallelism* (parallele Verarbeitung einer einzelnen Anfrage). Dieser unterteilt sich nochmal in *I/O parallelism, Intra-Operator parallelism* und *Inter-Operator parallelism*. Beim *I/O parallelism* werden nebenläufig mehrere Platten gelesen. Dabei wird mit spanned tablespaces und Partitionierung gearbeitet und eventuell auch mit Hardware RAIDs (versteckt). Beim *Intra-Operator parallelism* arbeiten mehrere Threads für den selben Operator, wie beispielsweise beim parallel sort, während beim *Inter-Operator parallelism* mehrere Teile eines Anfrageplans parallel laufen (pipeline). Letzteres ist wichtig für komplexe analytische Aufgaben (OLAP). \n",
    "\n",
    "Schauen wir uns als nächstes an, wie Inter-Operator parallelism genauer funktioniert. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6afae2-8b04-4930-80cf-44e3b959ef2e",
   "metadata": {},
   "source": [
    "### Pipeline Parallelism\n",
    "\n",
    "<img src=\"pictures/Pipeline-Parallelism.png\" alt=\"Pipeline-Parallelism\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "Im ersten Schritt können zwei Threads (T2 und T3) je einen Base Table scannen und für die Joins einen Hash Table bauen. </br>\n",
    "Dann scannt ein Thread den ersten Table. Der Thread prüft die Hashtabellen auf Kollisionen und versucht eine alternative Stelle zu finden (Probing). Bei Hash Tabellen kann es zu Kollisionen kommen, wenn zwei verschiedene Schlüssel auf denselben Hashwert abgebildet werden. Der zweite Thread fängt an die Sublisten zu sortieren (Sort) und fügt die ersten Listen zusammen. </br>\n",
    "Am Ende ist nur noch ein Thread vorhanden. Dieser gibt das Ergebnis zurück (Return) und arbeitet wieder wie zuvor weiter. </br>\n",
    "</br>\n",
    "\n",
    "Pipeline Parallelism ist auch bekannt als inter-operator parallelism: Eine Parallelisierung der Operatoren. \n",
    "Inter Operator bedeutet dabei: Während man an etwas arbeitet, gibt man die Ergebnisse weiter an andere Threads. Dadurch können diese schon früher andere Operationen auf den Daten durchführen.\n",
    "Es können somit mehrere Pipelines gleichzeitig ausgeführt werden, sofern mehrere vorhanden sind und auch nicht voneinander abhängen. </br>\n",
    "</br>\n",
    "\n",
    "Zudem hat Pipeline Parallelism einige Probleme. \n",
    "Der Synchronisationsaufwand ist sehr hoch, wenn Fehler gemacht werden oder auf Threads gewartet wird, die noch nicht fertiggestellt wurden. </br>\n",
    "Häufig kann auch nur wenig parallelisiert werden, sodass der Parallelisierungsgrad gering ist (degree of parallelism). Wenn man beispielsweise eine Anfrage mit fünf Operationen hat, kann man sie maximal mit Faktor 5 parallelisieren. Dazu kommen noch die Kosten der einzelnen Operationen: Nicht jede Operation kostet gleich viel. Wenn eine Operation zwar sehr schnell ausgeführt werden kann, aber eine andere Operation sehr lange braucht, muss trotzdem auf die längere gewartet werden. </br>\n",
    "Das Verfahren ist eher nur für Shared-Memory-Architekturen geeignet. Dabei spielen die I/O-Kosten eine untergeordnete Rolle. </br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975d690-6d98-4774-92bb-03bb21eec988",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Parallelism\n",
    "\n",
    "Pipeline Parallelism ist nicht immer anwendbar. Die Alternative bietet Data Parallelism. Die Daten werden in Teilmengen partitioniert. Sie werden also auf verschiedenen Rechnern oder Disks gespeichert. Die Idee hierbei ist, dass es Operationen gibt, die im selben Kontext nicht alles sehen müssen. Mit anderen Worten: Die Operationen werden geteilt auf Rechnern oder auch auf Prozessoren ausgeführt. Die entstehenden Ergebnisse auf den verschiedenen Rechnern/Prozessoren müssen dann nur noch zusammengefügt werden. Dadurch können Teilmengen unanhängig und parallel verarbeitet werden. </br>\n",
    "</br>\n",
    "Ein kleines Beispiel bei einer Selektion: </br>\n",
    "Man teilt einen Stapel Klausuren auf 5 Stapel auf. Gesucht werden alle 1er Kandidaten. Nun stellt man an jeden Stapel eine Person, die diesen Stapel Klausur für Klausur durchsucht und die Klausuren mit einer 1 herausnimmt. Das Ergebnis ist trotz mehrerer Teilstapel am Ende korrekt. Die Klausuren müssen nur noch zusammengelegt werden. \n",
    "An diesem Beispiel kann man nun sehen, dass die Selektion jedes Tupels unabhängig ist. </br>\n",
    "</br>\n",
    "Der maximale Parallelisierungsgrad hängt von der maximalen Anzahl von Teilmengen ab. Bei einer Selektion wäre es somit die Anzahl der Tupel.\n",
    "</br>\n",
    "Andere Operationen brauchen eine umfassendere Sicht auf die Daten. Dazu zählen z.B. die Gruppierung oder die Aggregation. In dem Beispiel bräuchten die Personen einen Blick in die anderen Stapel, um die jeweilige Operation auszuführen. Es reicht nicht mehr nur seinen eigenen Stapel zu betrachten. Sie benötigen also unterschiedliche Mengen, um korrekt zu funktionieren.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86925bb3-16b0-4747-8590-e2328358cf7b",
   "metadata": {},
   "source": [
    "### Grundlagen der Parallelen Anfragebearbeitung (Parallel Query Processing) \n",
    "\n",
    "Nun werden die Grundlagen der Parallelen Anfragebearbeitung genauer thematisiert. Hier sei zunächst ein kleiner Überblick über die Parallelen Architekturen, die Datenpartitionierungsstrategien und über die Kosten gegeben. </br>\n",
    "</br>\n",
    "\n",
    "Bei der parallelen Anfragebearbeitung kommt es darauf an, welche gemeinsamen Ressourcen man zur Verfügung hat. Es gibt drei Stufen von gemeinsam genutzten Ressourcen (Ressource Sharing). Diese werden auch Parallele Architekturen genannt. \n",
    "1. Shared-Memory (Hauptspeicher)\n",
    "2. Shared-Disk\n",
    "3. Shared-Nothing\n",
    "Parallele Architekturen haben den Vorteil, dass man keine teuren Großrechner benötigt auf denen man die gesamten Daten speichert. Bei Vergrößerung des teuren Großrechners würden unter Anderem die Kosten exponentiell steigen. Zudem ist die Reparatur sehr komplex. Aus diesem Grund verwendet man Parallelen Architekturen.</br>\n",
    "</br>\n",
    "\n",
    "Zudem gibt es noch unterschiedliche Partitionierungsvarianten bei der parallelen Anfragebearbeitung. Die Daten können zufällig mit Round-Robin verteilt werden, mit einer Hash-Funktion oder mit einer Bereichsfunktion.</br>\n",
    "</br>\n",
    "\n",
    "Je nach parallelem Operator variieren die Kosten. \n",
    "Eine Selektion verarbeitet beispielsweise nur ein Tupel auf einmal. Andere Operatoren benötigen zusätzlich eine Sortierung, Projektion, Gruppierung, Aggregation oder auch einen Join. </br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e578380",
   "metadata": {},
   "source": [
    "### Parallele Architekturen – Shared Memory\n",
    "\n",
    "Unter den Parallelen Architekturen ist Shared Memory die einfachste Architektur. Mehrere CPUs teilen sich einen einzigen Speicher und eigene Disks (Array). Die Kommunikation wird über einen einzigen gemeinsamen Bus betrieben. In der Realität hat jeder Prozessor nocht zusätzlich einen eigenen privaten Speicher (NUMA: non-uniform memory access). \n",
    "\n",
    "\n",
    "<img src=\"pictures/shared-memory.png\" alt=\"shared-memory\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "Man hat ein Interface auf den Speicher (M = Memory). Alle Prozessoren (P) können darüber auf den ganzen Speicher zugreifen. Entweder greifen sie auf diesselbe oder eben auch auf unterschiedliche Disks zu. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f7b36",
   "metadata": {},
   "source": [
    "### Parallel Architectures – Shared Disk\n",
    "\n",
    "Bei der parallelen Architektur Shared Disks existieren mehrere Knoten mit mehreren CPUs. Jeder Knoten besitzt eigene private Hauptspeicheradressen oder Hauptspeicheradressbereiche. Die Knoten greifen alle auf die gleichen Disks zu und nutzen die gleiche Datenbank. Diese Architektur wird oft bei NAS, SAN oder anderen Systemen verwendet. \n",
    "\n",
    "<img src=\"pictures/shared-disk.png\" alt=\"shared-disk\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d013f6e-74f1-4c80-902a-dbe88e9bce1a",
   "metadata": {},
   "source": [
    "### Parallel Architectures – Shared Nothing\n",
    "\n",
    "Shared Nothing ist die meist genutzte Architektur für skalierbare Datenverarbeitung (Large-Scale Data Management). Bei der Shared Nothing Architektur hat jeder Knoten seinen eigenen Satz an CPUs, Speicher und Disks. Im Prinzip ist jeder Knoten somit ein eigener Server. \n",
    "Um die Vorteile der Architektur im vollem Umfang nutzen zu können, müssen die Daten über die Knoten partitioniert werden. Hierbei gibt es unterschiedliche Partitionierungsvarianten, die im Anschluss noch weiter thematisiert werden. Für die Partitionierung werden die Daten direkt über eine Knoten-zu-Knoten Kommunikation ausgetauscht. Die Nachrichten haben dabei einen signifikanten Overhead. \n",
    "Es werden somit doch Daten zwischen den Knoten ausgetauscht. Die Knoten teilen sich nämlich immernoch das Netzwerk. Der Name Shared Nothing ist also etwas irreführend.\n",
    "\n",
    "<img src=\"pictures/shared-nothing.png\" alt=\"shared-nothing\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14157ef-42e5-4f0d-824e-1931f4db6130",
   "metadata": {},
   "source": [
    "### Partitionierung\n",
    "\n",
    "Partitionierung bedeutet möglichst disjunkte Teilmenge zu generieren. Je nach Fall können die Daten unterschiedlich partitioniert werden. Bei Verkaufsdaten kann zum Beispiel jedes Jahr seine eigene Partition erhalten. \n",
    "\n",
    "Für eine Shared Nothing Architektur müssen die Daten auf mehreren Knoten verteilt werden. Würde man die Daten einfach replizieren, kann man aus der Sicht der Anfragebearbeitung auch von einer Shared-Disk sprechen. Es ist so als würde man alle Daten auf einer Disk speichern. Die lokalen Disks verhalten sich dann wie Caches. Außerdem muss bei der Replikation die Konsistenz sichergestellt werden.  \n",
    "\n",
    "Manche Datenbankanfragen können auf bestimmte Bereiche eingegrenzt werden, sofern man sicherstellen kann, dass alle relevanten Daten in der entsprechenden Partition zu finden sind. Die Partitionierung gewinnt durch solche Eigenschaften an Vorteil. \n",
    "\n",
    "In der Datenbankadministration entsteht durch Partitionierung ein weiterer Vorteil. Nicht mehr benötigte Partitionen wie alte Verkäufe usw. können einfach verworfen werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88416b-e137-4355-8149-8a862b0bda9e",
   "metadata": {},
   "source": [
    "### Partitionierungsstrategien\n",
    "\n",
    "Bei der Durchführung der Partitionierung können verschiedene Strategien gewählt werden. Jede bietet andere Vorteile.\n",
    "\n",
    "Bei **Round Robin** erhält jede Partition ein Tupel pro Runde. Dadurch haben alle Teilmengen garantiert eine möglichst gleiche Anzahl von Tupeln. Zwischen den einzelnen Tupeln in einer Partition herrscht widerrum keine explizite Beziehung.  \n",
    "\n",
    "Bei **Hash Partitioning** wird eine Menge von Partitionsspalten definiert. Für jede Spalte wird ein Hashwert generiert. Dieser Hashwert wird verwendet, um zu entscheiden, welche Partition als Ziel für die Tupel gewählt wird. Der Hashwert wird für jede Zeile berechnet, basierend auf den Werten der Partitionsspalten. Das Tupel wird dann der Partition zugeordnet, die dem berechneten Hashwert entspricht.\n",
    "    \n",
    "**Range Partitioned** bedeutet, dass eine Menge von Partitionsspalten definiert wird. Die Domäne der Spalten wird in Bereiche aufgeteilt. Die Tupel werden den Partitionen basierend auf den Bereichswerten zugeordnet. Alle Tupel aus einer Partition stammen somit aus dem gleichen Bereich. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225daf3a",
   "metadata": {},
   "source": [
    "### Data Parallelism: Example\n",
    "\n",
    "49min\n",
    "\n",
    "* Client sends a SQL query to one of the cluster nodes\n",
    "    * Node becomes the\n",
    "\"coordinator\"\n",
    "\n",
    "* Coordinator compiles query\n",
    "    * Parsing, checking, optimization\n",
    "    * Parallelization\n",
    "\n",
    "* Sends partial plans to the other cluster nodes\n",
    "    * Coordinator also executes the partial plan on his part of the data\n",
    "* Coordinator collects partial results and finalizes them \n",
    "\n",
    "\n",
    "<img src=\"pictures/Data-Parallelism-example.png\" alt=\"Data-Parallelism-example\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3936e",
   "metadata": {},
   "source": [
    "### Data Parallelism – Example\n",
    "\n",
    "* For shared-nothing & shared-disk\n",
    "    * Multiple instances of a sub-plan are executed on different computers\n",
    "    * The instances operate on different splits or partitions of the data\n",
    "    * At some point, results from the sub-plans are collected\n",
    "    * For more complex queries, results are not collected but re-distributed, for further parallel processing\n",
    "\n",
    "\n",
    "<img src=\"pictures/Data-Parallelism-example_2.png\" alt=\"Data-Parallelism-example_2\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Parallel Operators\n",
    "\n",
    "* Ideally: Operate as much as possible on individual partitions of the data\n",
    "    * Ship operation to data\n",
    "* Easy for simple “per-tuple” operators\n",
    "    * Scan, index-scan, selection\n",
    "* Problem: Some operators need the whole picture (blocking operators)\n",
    "    * Sort and aggregations can only be preprocessed in parallel and need a final step on a single node.\n",
    "        * Unless they occur in a correlated subplan known to contain only tuples from one partition.\n",
    "    * E.g., joins need matching tuples. \n",
    "        * Either organize the inputs accordingly,\n",
    "        * or join at the coordinator after collection of partial results (not parallel any more!)\n",
    "\n",
    "### Notations and Assumptions\n",
    "\n",
    "* S\tRelation S\n",
    "* S[i,h]\tPartition i of relation S according to partitioning scheme h\n",
    "* B(S)\tNumber of blocks of relation S\n",
    "* p\tNumber of nodes\n",
    "\n",
    "* Assume a shared-nothing architecture\n",
    "    * Most commercial database vendors use shared-nothing approaches.\n",
    "* Network transfer is at least as expensive as disk access\n",
    "    * In some cost models network is still far more expensive.\n",
    "    * Today network bandwidth ≈ disk bandwidth\n",
    "    * But: Network is shared\n",
    "        * Switches and routers have a throughput limit\n",
    "* Assume partitioning schemes (hash/range) produce partitions of roughly equal size.\n",
    "* Assume S[i,h] > M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94549d-f60a-4119-9fc2-8bf22cb0d946",
   "metadata": {},
   "source": [
    "### Parallel Selection / Projection\n",
    "\n",
    "* Selection and projection can be parallelized very efficiently\n",
    "    * “Embarrassingly parallel” problem\n",
    "\n",
    "* Each node performs the selection on its existing local partition.\n",
    "    * Selection needs no context\n",
    "    * Data can be partitioned in an arbitrary way\n",
    "\n",
    "* Partial results are unioned afterwards.\n",
    "\n",
    "* Cost:\tB(S)/p\t+ transfer (depends on selectivity)\n",
    "\n",
    "### Parallel Grouping & Aggregation\n",
    "\n",
    "* Two phases\n",
    "    1. Local grouping & aggregation to each partition\n",
    "    2. Merge results\n",
    "\n",
    "* Cost: 3 B(S)/p local algorithm + transfer of (small) results + (fast) merge\n",
    "\n",
    "* Works only for associative aggregation functions\n",
    "    * MIN, MAX, SUM, COUNT\n",
    "    * AVG: Use SUM / COUNT\n",
    "\n",
    "* To avoid possibly expensive second phase:\n",
    "    * Use hashing function on group-columns to re-partition relation onto nodes\n",
    "    * Or: Parallelization of merge phase\n",
    " \n",
    "### Parallel Sorting\n",
    "\n",
    "* Range partitioned sort: partition by range, then sort\n",
    "    * Range-partition the relation according to the sort column(s)\n",
    "    * Sort the single partitions locally (e.g., by TPMMS)\n",
    "    * Cost: B(S) partitioning + B(S) transfer + 3 B(S)/p local sorting\n",
    "    * Problem: Find a uniform range partitioning scheme\n",
    "        * Partitions of same/similar size\n",
    "\n",
    "* Parallel external sort-merge: sort locally, then merge\n",
    "    * Reuse an existing data partitioning\n",
    "    * Partitions are sorted locally (e.g. by TPMMS)\n",
    "    * Sorted partitions need to be merged\n",
    "    * Pair-wise with cost: 3 B(S)/p local sorting + log2(p)*B(S)/2 transfer + log2(p)*B(S) local merge\n",
    "    * Or multi-way merge\n",
    " \n",
    "### Symmetric Fragment-and-Replicate Join\n",
    "\n",
    "* Joining two relations R and S requires looking at every tuple of the Cartesian product. \n",
    "* Parallel databases need to combine every partition of R with every partition of S.\n",
    "* Symmetric Fragment-and-Replicate (or Broadcast) Join:\n",
    "* Given  nodes\n",
    "* Fragment R into m and S into n partitions\n",
    "* Replicate the fragments onto the nodes\n",
    "* Each fragment of R is replicated n times\n",
    "* Each fragment of S is replicated m times\n",
    "* Each node locally joins exactly one fragment pair of R and S.\n",
    "* Cost: \t\tfragmentation cost <br>\n",
    "        \t\ttransfer cost <br>\n",
    "        \t???\t\tlocal join cost\n",
    "* Only parallel join type that works for all join predicates (Theta-Join)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d950b",
   "metadata": {},
   "source": [
    "### Symmetric Fragment-and-Replicate Join\n",
    "\n",
    "<img src=\"pictures/Symmetric-Fragment-and-Replicate-Join.png\" alt=\"Symmetric-Fragment-and-Replicate-Join\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f361c36",
   "metadata": {},
   "source": [
    "### Asymmetric Fragment-and-Replicate Join\n",
    "\n",
    "* We can do better, if relation S is much smaller than R.\n",
    "\n",
    "* Idea: Reuse the existing partitioning of R and replicate the whole relation S to each node.\n",
    "\n",
    "* Cost:\tp * B(S)\ttransport <br>\n",
    "\t???\t\tlocal join\n",
    "\n",
    "* Asymmetric Fragment-and-replicate join is a special case of the Symmetric Algorithm with m=p and n=1.\n",
    "\n",
    "<img src=\"pictures/Asymmetric-Fragment-and-Replicate-Join.png\" alt=\"Asymmetric-Fragment-and-Replicate-Join\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4210fa-9e8b-4733-bb8c-449c120ec03b",
   "metadata": {},
   "source": [
    "### Parallel Equi-Joins (I)\n",
    "\n",
    "* A special class of joins that are more suited for parallelization are natural- and equi-joins.\n",
    "\n",
    "* Idea: Partition relations R and S using the same partition scheme over the join key.\n",
    "    * All tuples of R and S with the same join key end up at the same node.\n",
    "    * No further broadcast is needed, all joins can be performed locally.\n",
    "\n",
    "* Actual implementation depends on how the relations are partitioned:\n",
    "    * Co-Located Join\n",
    "    * Directed Join\n",
    "    * Re-Partitioning Join\n",
    " \n",
    "### Parallel Equi-Joins: Three cases\n",
    "\n",
    "1. Both R and S are already partitioned over the join key with the same partitioning scheme\n",
    "    - „Co-Located Join“\n",
    "    - No re-partitioning is needed!\n",
    "    - Cost: \t???\t\tLocal join cost\n",
    "2. Only one relation is partitioned over the join key:\n",
    "    - „Directed Join“\n",
    "    - Re-Partition the other relation with same partitioning scheme.\n",
    "    - Cost (assuming R is already partitioned):\t\n",
    "    \t- B(S) \t\tpartitioning\n",
    "        - B(S)\t\ttransfer \n",
    "    \t- ???\t\tLocal join cost\n",
    "3. No relation is partitioned over the join key: \t\n",
    "    - „Repartition Join“\n",
    "    - Re-Partition both relations over the join key\n",
    "    - Cost:\tB(S)+B(R) \tpartitioning\n",
    "        - B(S)+B(R)\ttransfer\n",
    "        - ???\t\tLocal join cost\n",
    "     \n",
    "### Limits in Parallel Databases\n",
    "\n",
    "* Database clusters tend to scale until 64 or 128 nodes\n",
    "    * Afterwards the speedup curve flattens\n",
    "    * Communication overhead eats speedup\n",
    "    * Hard limit example: 1000 nodes for DB2 (2010)\n",
    "\n",
    "* Shared Disk: Does not scale infinitely; bus and synchronization become overhead\n",
    "    * For updates: Cache Coherency Problem\n",
    "    * For reads: I/O Bandwidth Limits\n",
    "\n",
    "* Shared Nothing: Cannot compensate loss of a node easily\n",
    "    * In large clusters, failures and outages are most common.\n",
    "    * Loss of a node means loss of data!\n",
    "    * Unless: Data is replicated. But: Replicated data must be kept consistent! Has a high overhead…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0081069-b230-4718-a87c-41442d215f42",
   "metadata": {},
   "source": [
    "### Where traditional databases are unsuitable\n",
    "\n",
    "* Analysis over raw (unstructured) data\n",
    "    * Text processing\n",
    "    * In general: If relation schema does not fit\n",
    "* Where cost-effective scalability is required\n",
    "    * Use commodity hardware\n",
    "    * Adaptive cluster size (horizontal scaling)\n",
    "    * Incremental growth: add computers without expensive reorganization that halts the system\n",
    "* In unreliable (= large) infrastructures\n",
    "    * Must be able to deal with failures – hardware, software, network\n",
    "        * Failure is expected rather than the exception\n",
    "    * Transparent to applications\n",
    "        * Too expensive to build reliability into each application\n",
    "     \n",
    "### Example Use Case: Web Index Creation\n",
    "\n",
    "* A Search Engine scenario:\n",
    "    * Have crawled the internet and stored the relevant documents\n",
    "    * Documents contain words \t(Doc-URL, [list of words])\n",
    "    * Documents contain links   \t(Doc-URL, [Target-URLs])\n",
    "* Need to build a search index\n",
    "    * Invert the files \t\t(word, [list of URLs])\n",
    "    * Compute a ranking that requires an inverted graph: \n",
    "\t\t\t\t(Doc-URL, [URLs-pointing-to-it])\n",
    "* Obvious reasons against relational databases\n",
    "    * Relational schema is unsuitable/”unnatural”\n",
    "    * Importing the documents, converting them to the storage format is expensive\n",
    "* A mismatch between what databases were designed for and what is really needed:\n",
    "    * Databases come originally from transactional processing. They give hard guarantees about absolute consistencies in the case of concurrent updates.\n",
    "    * Analytics are added on top of that\n",
    "    * The documents are never updated, they are read only.\n",
    "    * Perfect transactional consistency is not always necessary\n",
    " \n",
    "### An ongoing Re-Design…\n",
    "\n",
    "* Driven by companies like Google, Facebook, Yahoo, Apple, Microsoft\n",
    "* Use heavily distributed system\n",
    "    * Google used 450,000 low-cost commodity servers in 2006\n",
    "in cluster of 1000 – 5000 nodes\n",
    "* Redesign infrastructure and architectures completely with the key goal to be\n",
    "    * Highly scalable\n",
    "    * Tolerant of failures\n",
    "* Stay generic and schema free in the data model\n",
    "* Start with: Data Storage\n",
    "* Next Step: Distributed Analysis\n",
    "\n",
    "### Storage Requirements\n",
    "\n",
    "* Extremely large files: Terabytes to Petabytes\n",
    "* High availability: Data must be kept replicated\n",
    "* High throughput\n",
    "    * Read/write operations must not go through other servers\n",
    "* No single point of failure\n",
    "    * Any master must be kept redundantly\n",
    "* Many different distributed file systems exist.\n",
    "    * Different goals: transparency, updateability, archiving, etc…\n",
    "* Google Filesystem (GFS)\n",
    "    * Widely used reference architecture for high-throughput and high-availability DFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4035b",
   "metadata": {},
   "source": [
    "### The Storage Model – Distributed File System\n",
    "\n",
    "* The file system\n",
    "    * Distributed across many nodes (DataNodes)\n",
    "    * Provides a single namespace for the entire cluster\n",
    "    * Metadata is managed on a dedicated node (NameNode)\n",
    "    * Write-once-read-many access model\n",
    "* Files are split into blocks\n",
    "    * Typically 128 MB block size\n",
    "    * Each block is replicated on multiple data nodes\n",
    "* The client\n",
    "    * can determine the location of blocks\n",
    "    * can access data directly from the DataNode\n",
    "    * over the network\n",
    "* Problem: bandwidth to data\n",
    "    * Scanning the data from remote storage is expensive (50MB/s remote access vs. 150-200MB/s local access)\n",
    "    * Moving computation is more efficient than moving data\n",
    "    * Map/Reduce framework tries to perform computations close to the data\n",
    "    * Nodes have two purposes: data storage and computation\n",
    "\n",
    "<img src=\"pictures/The-Storage-Model.png\" alt=\"The-Storage-Model\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Retrieving and Analyzing Data\n",
    "\n",
    "* Data is stored as custom records in files\n",
    "    * Most generic data model that is possible\n",
    "    * Key/value model\n",
    "* Records are read and written with data model specific (de)serializers\n",
    "* Analysis or transformation tasks must be written directly as a program\n",
    "    * Not possible to generate it from a higher level statement\n",
    "    * Like a query-plan that is automatically generated from SQL\n",
    "* Programs must be parallel, highly scalable, fault tolerant\n",
    "    * Extremely hard to program\n",
    "    * Need a programming model and framework that takes care of that\n",
    "    * The map/reduce model has been suggested and successfully adapted on a broad scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62756e90",
   "metadata": {},
   "source": [
    "### Skalierungsmuster\n",
    "\n",
    "* Phase 0: Daten verteilen (split)\n",
    "* Phase 1: Berechnungen auf Teilmengen der Daten (map)\n",
    "* Phase 2: Zusammenführung der Teilmengen (reduce)\n",
    "    * Gemeinsame Betrachtung zusammengehöriger Daten\n",
    "\n",
    "* Beispiel: **Two-Phase-Multiway-Mergesort** (TPMMS)\n",
    "    * Phase 1: Sortierung von Teilen der Daten\n",
    "    * Phase 2: Sortierter Teillisten zusammenführen\n",
    "* Beispiel: **Datenanalyse**\n",
    "    * Phase 1: Gruppierung\n",
    "    * Phase 2: Aggregation\n",
    "* Beispiel: **Index bauen**\n",
    "    * Phase 1: Teilmengen indizieren\n",
    "    * Phase 2: Indizes zusammenführen\n",
    "\n",
    "<img src=\"pictures/Skalierungsmuster.png\" alt=\"Skalierungsmuster\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8c619-36ea-4038-8bbb-b4c3183d7b28",
   "metadata": {},
   "source": [
    "## Map Reduce & Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247aa192",
   "metadata": {},
   "source": [
    "### “MapReduce is a programming model and an associated implementation for processing and generating large data sets.”\n",
    "\n",
    "<img src=\"pictures/MapReduce-Simplified-data-processing-on-large-clusters.png\" alt=\"MapReduce-Simplified-data-processing-on-large-clusters\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### What is Map/Reduce?\n",
    "\n",
    "* Programming model\n",
    "    * Borrows concepts from functional programming\n",
    "    * Suited for parallel execution\n",
    "        * Automatic parallelization & distribution of data and computational logic\n",
    "    * Clean abstraction for programmers\n",
    "* Functional programming influences\n",
    "    * Treats computation as the evaluation of mathematical functions\n",
    "    * No changes of states (no side effects)\n",
    "    * Output value of a function depends only on its arguments\n",
    "* Map and Reduce are higher-order functions (2nd order)\n",
    "    * Take user-defined functions as argument\n",
    "    * Return a function as result\n",
    "    * User implements the two functions\n",
    "\n",
    "### Grundbausteine\n",
    "\n",
    "* Datenmodel\n",
    "    * Schlüssel/Wert-Paare  \n",
    "        * “key/value pairs”\n",
    "        * Z.B. (int, string), oder(string, [string]), …\n",
    "\n",
    "* MapReduce Programm\n",
    "    * Input: Liste an Schlüssel/Wert-Paare \n",
    "    * Output: Liste an Werten\n",
    "* Zwei Herausforderungen\n",
    "    * Entwurf der Funktionen\n",
    "    * Verteilte, fehlertolerante und effiziente Ausführung des Programms\n",
    " \n",
    "* Nutzer definieren zwei Funktionen\n",
    "    * Map:\t\n",
    "        * Oft nur ein Paar \n",
    "    * Reduce:\t\n",
    "        * Meist nur ein Wert \n",
    "        * Meist auch  im Output, dadurch Verkettung von MapReduce-Schritten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8244244",
   "metadata": {},
   "source": [
    "### MapReduce workflow\n",
    "\n",
    "<img src=\"pictures/MapReduce-workflow.png\" alt=\"MapReduce-workflow\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ab009",
   "metadata": {},
   "source": [
    "#### Aufgabe: Bestimme für jedes Wort dessen Häufigkeit im Korpus\n",
    "\n",
    "```\n",
    "map(filename, line){\n",
    "\t  for each (word in line)\n",
    "\t     emit(word, 1);\t \t \n",
    "}\n",
    "\n",
    "reduce(word, numbers){\n",
    "\t  int sum = 0;\n",
    "\t  for each (value in numbers){\n",
    "\t    sum += value;\n",
    "\t  }\n",
    "\t  emit(word, sum);\n",
    "}\n",
    "\n",
    "```\n",
    "<img src=\"pictures/Aufgabe-Häufigkeit-bestimmen.png\" alt=\"Aufgabe-Häufigkeit-bestimmen\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6a36f",
   "metadata": {},
   "source": [
    "### Map Reduce Illustrated (2)\n",
    "\n",
    "<img src=\"pictures/MapReduce-Illustrated-2.png\" alt=\"MapReduce-Illustrated-2\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d802f",
   "metadata": {},
   "source": [
    "#### Aufgabe: Bestimme Liste gemeinsamer Bekannte für jedes Personenpaar\n",
    "\n",
    "```\n",
    "map(person, friendlist){\n",
    "\t  for each (friend in friendlist)\n",
    "      if(friend < person)\n",
    "\t        emit(, friendlist);\n",
    "      else \n",
    "         emit(, friendlist);\n",
    "}\n",
    "\n",
    "reduce(, friendlists){\n",
    "   emit(, friendlist[1] ∩ friendlist[2]);\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "* 2016: 1,4 Milliarden Facebook-Nutzer, durchschnittlich 155 Freunde\n",
    "    * 979.999.999.300.000.000 Paare\n",
    "\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "**Friends lists:** <br>\n",
    "A -> B C D <br>\n",
    "B -> A C D E <br>\n",
    "C -> A B D E <br>\n",
    "D -> A B C E <br>\n",
    "E -> B C D <br>\n",
    "\n",
    "**After mapping:**\n",
    "\n",
    "(A B) -> B C D <br>\n",
    "(A C) -> B C D <br>\n",
    "(A D) -> B C D <br>\n",
    " <br>\n",
    "(A B) -> A C D E <br>\n",
    "(B C) -> A C D E <br>\n",
    "(B D) -> A C D E <br>\n",
    "(B E) -> A C D E <br>\n",
    " <br>\n",
    "(A C) -> A B D E <br>\n",
    "(B C) -> A B D E <br>\n",
    "(C D) -> A B D E <br>\n",
    "(C E) -> A B D E <br>\n",
    " <br>\n",
    "(A D) -> A B C E <br>\n",
    "(B D) -> A B C E <br>\n",
    "(C D) -> A B C E <br>\n",
    "(D E) -> A B C E <br>\n",
    " <br>\n",
    "(B E) -> B C D <br>\n",
    "(C E) -> B C D <br>\n",
    "(D E) -> B C D <br>\n",
    "\n",
    "**After shuffling:** <br>\n",
    "(A B) -> (A C D E) (B C D) <br>\n",
    "(A C) -> (A B D E) (B C D) <br>\n",
    "(A D) -> (A B C E) (B C D) <br>\n",
    "(B C) -> (A B D E) (A C D E) <br>\n",
    "(B D) -> (A B C E) (A C D E) <br>\n",
    "(B E) -> (A C D E) (B C D) <br>\n",
    "(C D) -> (A B C E) (A B D E) <br>\n",
    "(C E) -> (A B D E) (B C D) <br>\n",
    "(D E) -> (A B C E) (B C D) <br>\n",
    "\n",
    "**After reducing:** <br>\n",
    "(A B) -> (C D) <br>\n",
    "(A C) -> (B D) <br>\n",
    "(A D) -> (B C) <br>\n",
    "(B C) -> (A D E) <br>\n",
    "(B D) -> (A C E) <br>\n",
    "(B E) -> (C D) <br>\n",
    "(C D) -> (A B E) <br>\n",
    "(C E) -> (B D) <br>\n",
    "(D E) -> (B C) <br>\n",
    "\n",
    "\n",
    "### Parallel DBMS vs. Map/Reduce\n",
    "\n",
    "<img src=\"pictures/Parallel_DBMS_vs_Map_Reduce.png\" alt=\"Parallel_DBMS_vs_Map_Reduce\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "\n",
    "### Relational Operators as Map/Reduce jobs\n",
    "\n",
    "* **SQL Query** \n",
    "\n",
    "```\n",
    "SELECT year, SUM(price)\n",
    "FROM   sales\n",
    "WHERE  area_code = “US”\n",
    "GROUP BY year\n",
    "```\n",
    "* **Map/Reduce job:**\n",
    "\n",
    "```\n",
    "map(key, tuple) {\n",
    "\t  int year = YEAR(tuple.date);\n",
    "\t  if (tuple.area_code = “US”)\n",
    "\t    emit(year, {‘price’ => tuple.price });\n",
    "\t}\n",
    "\n",
    "\treduce(key, tuples) {\n",
    "\t  double sum_price = 0;\n",
    "\t  foreach (tuple in tuples) {\n",
    "\t    sum_price += tuple.price;\n",
    "\t  }\n",
    "\t  emit(key, sum_price);\n",
    "\t}\n",
    "```\n",
    "\n",
    "* **Sorting with SQL Query:**\n",
    "\n",
    "```\n",
    "SELECT * \n",
    "FROM sales \n",
    "ORDER BY year\n",
    "```\n",
    "\n",
    "* **Map/Reduce job:**\n",
    "\n",
    "```\n",
    "map(key, tuple) {\n",
    "\t  emit(YEAR(tuple.date) div 10, tuple);\n",
    "\t}\n",
    "\n",
    "\treduce(key, tuples) {\n",
    "\t  emit(key, sort(tuples));\n",
    "\t}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6fc60",
   "metadata": {},
   "source": [
    "### Hadoop – A map/reduce Framework \n",
    "\n",
    "* Hadoop: Apache Top Level Project\n",
    "    * Open source\n",
    "    * Written in Java\n",
    "\n",
    "* Hadoop provides a stack of\n",
    "    * Distributed file system (HDFS) – modeled after the Google File System\n",
    "    * Map/Reduce engine\n",
    "    * Data processing languages (Pig Latin, Hive SQL)\n",
    "    * Plus very many packages\n",
    "\n",
    "* Runs on\n",
    "    * Linux, Mac OS/X, Windows, Solaris\n",
    "    * Commodity hardware\n",
    "\n",
    "<img src=\"pictures/hadoop.png\" alt=\"hadoop\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43d741",
   "metadata": {},
   "source": [
    "### Hadoop Distributed File System (HDFS)\n",
    "\n",
    "* Master-Slave Architecture\n",
    "    * Based on GFS architecture\n",
    "\n",
    "* HDFS Master “NameNode”\n",
    "    * Manages all file system metadata\n",
    "    * Transactions are logged, merged \n",
    "    * at startup\n",
    "    * Controls read/write access to files\n",
    "    * Manages block replication\n",
    "    * Can be replicated to avoid single-point-of-failure\n",
    "\n",
    "* HDFS Slave “DataNode”\n",
    "    * Communicates with the NameNode periodically via heartbeats\n",
    "    * Serves read/write requests from clients\n",
    "    * Performs replication tasks upon instruction by NameNode\n",
    "        * Default replication factor: 3\n",
    "\n",
    "<img src=\"pictures/HDFS.png\" alt=\"HDFS\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Hadoop Map/Reduce Engine\n",
    "\n",
    "Jobs are executed like a Unix pipeline: <br>\n",
    "* cat * | grep | sort | uniq -c | cat    > output <br>\n",
    "* Input | Map  | Sort & Shuffle | Reduce | Output\n",
    "\n",
    "Workflow\n",
    "1. Input phase: generates a number of FileSplits from input files (one per Map task)\n",
    "2. Map phase: executes a user function to transform input kv-pairs into a new set of kv-pairs\n",
    "3. Sort & shuffle phase: sort and distribute the kv-pairs to output nodes\n",
    "4. Reduce phase: combines all kv-pairs with the same key into new kv-pairs\n",
    "5. Output phase writes the resulting pairs to files\n",
    "\n",
    "All phases are distributed with many tasks doing the work\n",
    "* Framework handles scheduling of tasks on cluster\n",
    "* Framework handles recovery when a node fails\n",
    "\n",
    "\n",
    "* Master / Slave architecture\n",
    "\n",
    "* Map/Reduce Master: JobTracker\n",
    "    * Accepts jobs submitted by clients\n",
    "    * Assigns map and reduce tasks to TaskTrackers\n",
    "    * Monitors execution status, re-executes tasks upon failure\n",
    "\n",
    "* Map/Reduce Slave: TaskTracker\n",
    "    * Runs map / reduce tasks upon instruction from the task tracker\n",
    "    * Manage storage, sorting and transmission of intermediate output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38894c7c",
   "metadata": {},
   "source": [
    "### Hadoop Map/Reduce Engine\n",
    "\n",
    "<img src=\"pictures/Hadoop-Map_Reduce-engine.png\" alt=\"Hadoop-Map_Reduce-engine\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Fehlertoleranz\n",
    "\n",
    "* Viele Daten  …  in langen Prozessen  …  auf vielen Maschinen\n",
    "\n",
    "\n",
    "* Verteiltes Dateisystem (DFS / HDFS)\n",
    "    * Speichert verteilt und fehlertolerant durch Replikation\n",
    "    * Input ist redundant verfügbar\n",
    "* Speicherung von Zwischenergebnissen ins DFS\n",
    "    * Aufwändig, aber Fehlererholung im laufenden Prozess einfach und schnell\n",
    "* Abstürze\n",
    "    * Werden erkannt falls periodisches Signal ausfällt (heartbeat)\n",
    "    * Neustart des Mappers oder Reducers\n",
    "        * Auf anderer Maschine, mit Replikat des ursprünglichen Inputs\n",
    "     \n",
    "### When to use Hadoop?\n",
    "\n",
    "* Good fit for batch processing applications that need to touch all your data:\n",
    "    * Data mining\n",
    "    * Model tuning\n",
    "    * Text processing\n",
    "\n",
    "* Bad fit for applications that need to find/edit one particular record\n",
    "    * High overhead\n",
    "    * High latency\n",
    "\n",
    "* Bad fit for applications that need to communicate between processes\n",
    "    * Hadoop is oriented around independent units of work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa99f30",
   "metadata": {},
   "source": [
    "### In der Praxis: Komplexe (optimierte) MapReduce Workflows\n",
    "* Neue (relationale) Operatoren\n",
    "    * Join, Cross, Union, …\n",
    "* Planoptimierung & Re-optimierung\n",
    "* Scheduling & Lastbalancierung\n",
    "* Cross-Plattform Ausführung\n",
    "\n",
    "<img src=\"pictures/Komplexe-optimierte-MapReduce-workflows.png\" alt=\"Komplexe-optimierte-MapReduce-workflows\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74280512",
   "metadata": {},
   "source": [
    "### Hadoop vs. Parallel DBMS\n",
    "\n",
    "* 2012\n",
    "  \n",
    "<img src=\"pictures/Hadoop_vs_Parallel_DBMS.png\" alt=\"Hadoop_vs_Parallel_DBMS\" width=\"500\" style=\"background-color: white;\"/>\n",
    "  \n",
    "* 2014\n",
    "  \n",
    "<img src=\"pictures/Hadoop_vs_Parallel_DBMS_2.png\" alt=\"Hadoop_vs_Parallel_DBMS_2\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "\n",
    "<img src=\"pictures/Hadoop-vs-Parallel-DBMS.png\" alt=\"Hadoop-vs-Parallel-DBMS\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093d5f2",
   "metadata": {},
   "source": [
    "### In der Praxis: Viele Bibliotheken\n",
    "\n",
    "* Startpunkt: Hadoop\n",
    "    * Java, open-source\n",
    "    * Basis-Bibliotheken\t\t\t\tCommon, MapReduce\n",
    "    * Verteiltes Dateisystem\t\t\tHDFS\n",
    "    * Scheduling, Monitoring\t\t\tYarn\n",
    "\n",
    "* Erweiterungen\n",
    "    * Service- und Cluster-Verwaltung\t\tZooKeeper\n",
    "    * Datenspeicher\t\t\t\tHBase\n",
    "    * Datenbank und Anfragesprachen\t\tPig, Hive, Phoenix\n",
    "    * Bibliotheken für komplexe Verfahren\t\tMahout, Giraph, Solr\n",
    "    * Datenstromverarbeitung\t\t\tKafka, Flink, Spark\n",
    "    * …\n",
    "\n",
    "\n",
    "<img src=\"pictures/Viele-Bibliotheken.png\" alt=\"Viele-Bibliotheken\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e7461-dd8b-4016-b578-78899686aeda",
   "metadata": {},
   "source": [
    "## Outlook: What about updates/transactions?\n",
    "\n",
    "* OLTP style applications that are beyond relational databases' capabilities exist as well.\n",
    "* Some applications still require fast and efficient lookup and retrieval of small amounts of data\n",
    "    * Web index access, mail accounts, warehouse updates for resellers \n",
    "    * Addressed by Key/Value pair based storage systems (e.g. Google BigTable and Megastore)\n",
    "    * Can access the data only through a key\n",
    "    * Can apply only an additional filter on columns and timestamps\n",
    "* Some applications still need updates and certain guarantees about them\n",
    "    * No hard transactions, especially no multi record transactions.\n",
    "    * Eventual consistency model\n",
    "* See next set of slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddc406-ec67-4812-9780-cfc72bdda3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}