{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aba103-11e1-43f4-98a3-fa756243c505",
   "metadata": {},
   "source": [
    "# Large Scale Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981228f1-614d-415e-bb54-112dd93352e0",
   "metadata": {},
   "source": [
    "In diesem Kapitel geht es insbesondere darum, die Verfahren und Datenbankoperationen, die wir bisher kennengelernt haben, hinsichtlich paralleler Verarbeitung zu betrachten und auch die Kostenelemente, die dann eine Rolle spielen. \n",
    "\n",
    "Beim Large Scale Data Management geht es um sehr große Datenmengen. Da reicht es dann nicht mehr, nur eine Datenbank zu haben, man muss nun auch über die Verteilung, Server und Nebenläufigkeiten nachdenken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac2534-e148-4a3d-9ea9-a5d8768021d2",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"pictures/Large-scale-Data-Management.png\" alt=\"Large-scale-Data-Management\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Zur Wiederholung einmal die Frage: Was ist Big Data? Big Data wird anhand von Dimensionen spezifiziert- die sogenannten V's: **Volume** (Menge von Daten), **Velocity** (Schnelligkeit der Datenverarbeitung), **Variety** (Heterogenität der Daten), **Verocity** (Daten, bei denen die Korrektheit ungewiss ist) und **Value** (die Wertigkeit der Daten).\n",
    "\n",
    "Nun gibt es Big Data in zwei Varianten - **Operational** und **Analytic**. In der ersten Variante geht es um operationelle Sachen, also dem Transaktionsmanagement. In der zweiten Variante geht es darum, Daten zu analysieren, Insights aus Daten herzustellen und neue Erkenntnisse zu gewinnen.\n",
    "\n",
    "Zur Verdeutlichung, über was für Datenmengen wir bei Big Data reden:\n",
    "\n",
    "Google ist ein klassisches Beispiel für ein Datenproduzierendes und -verwaltendes Unternehmen. Dort werden jeden Tag 20 PB an Daten verarbeitet. Das sind Billionen von Zeilen, Tausende/Millionen Spalten und Tabellen, aber auch strukturierte Daten wie Text, Bilder und Videos. Würde man versuchen, diese 20 PB mit 50 MB/s zu lesen, würde das 12 Jahre dauern. Aus diesem Grund werden die Daten partioniert und verteilt verarbeitet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b4e51",
   "metadata": {},
   "source": [
    "## Key enabler: Virtulization\n",
    "\n",
    "Die beiden Varianten Operational und Analytic lassen sich mit der Virtualization managen. Hierbei versucht man entweder ein logisches System auf viele physische Systeme (Load Balancing) oder andersherum mehrere logische Systeme auf ein physisches System abzubilden (Multy-Tenancy).\n",
    "\n",
    "\n",
    "<img src=\"pictures/Virtualization.png\" alt=\"Virtualization\" width=\"500\" style=\"background-color: white;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b2968-007a-405a-b4d5-3e4d36a6405e",
   "metadata": {},
   "source": [
    "## Parallel Data Processing\n",
    "\n",
    "<img src=\"pictures/Overview_3.png\" alt=\"Overview_3\" width=\"300\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3562a51-ec56-4ad4-8501-03942de6c2de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Was bisher geschah: Serielle Verarbeitung/Single Threaded\n",
    "\n",
    "Bisher haben wir immer von einem Computer mit mehreren Festplatten gesprochen und damit auch ein wenig über parallele Plattenzugriffe. Diese hatten insbesondere auch immer nur einen Kern. Das heißt, bei jeder Operation wurden die Blöcke nacheinander durch nur einen Kern abgearbeitet. Außerdem spielten auch Synchronisation und Kommunikation keine Rolle, da Anfragen in nur einem Thread bearbeitet wurden. Dies wollen wir nun erweitern.\n",
    "\n",
    "<img src=\"pictures/serial-single-threaded.png\" alt=\"serial-single-threaded\" width=\"300\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Was wir verschwiegen haben...\n",
    "\n",
    "Das Datenvolumen wächst stetig. Data Warehouses mit 1 EB sind nicht untypisch. Manche Organisationen produzieren täglich mehr als 1 PB an neuen Daten. Das entspricht 1.000.000.000.000.000 Byte (1 quadrillion).\n",
    "Manche Systeme, wie beispielsweise Finanzinstitute, Onlineshops und soziale Netzwerke, haben einen sehr hohen Durchsatz (throughput) von Transaktionen. \n",
    "Deshalb ist es wichtig zu überlegen, wie die Zugriffe über die Netzwerke verteilt werden. \n",
    "Auch Analyseanfragen werden immer komplexer. Eine statistische Mustererkennung ist teuer und über die Daten muss mehrfach iteriert werden. Da reicht eine Single-CPU- oder Single-Node-Architektur nicht mehr aus und auch Moore's Law ist hier nicht mehr anwendbar. Die Lösung: **Parallele Datenverarbeitung**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae1665",
   "metadata": {},
   "source": [
    "### Grundlagen der Parallelen Datenverarbeitung (Parellel Processing)\n",
    "\n",
    "Bei der parallelen Datenverarbeitung kommt Amdahl's law zum Einsatz, welches die Grenzen bei der parallelen Beschleunigung definiert. Es gibt außerdem verschiedenen Stufen der Parallelisierung, mit denen auf unterschiedlichen Ebenen parallelisiert werden kann. Des Weiteren existieren noch verschiedenen Varianten der Anfrage-Parallelisierung. Dadurch können mehrere Anfragen parallel verarbeitet werden (Inter-Query) oder nur eine Anfrage (Intra-Query).  \n",
    "\n",
    "### Parallel Speedup – Amdahl‘s law\n",
    "\n",
    "Die Frage die sich bei Amdahl's law stellt ist, wie viel wir an Geschwindigkeit überhaupt dazugewinnen können. Berechnen lässt sich das zum einen mit der sequentiellen Laufzeit $T_1$ (1 Prozessor) und zum anderen mit der parallelen Laufzeit $T_p$ (*p* Prozessoren): $S_p$ = $\\frac{T_1}{T_p}$ . Die maximale Beschleunigung ist durch den nicht-parallelisierbaren Anteil des Programms begrenzt. Wie hoch diese ist, lässt sich folgendermaßen berechnen: $S_p$ = $\\frac{1}{(1 - f) + \\frac{f}{p}}$ . <br>\n",
    "*f* entspricht prozentual dem parallelisierbaren Anteil. Die ideale Beschleunigung wäre *S = p* für *f = 1*. Oft ist *f* < 1 während *S* durch eine Konstante begrenzt wird. Beispiel: *f = 0,9* und 10/20 Server. $S_p$ = $\\frac{1}{(1 - f) + \\frac{f}{p}}$ = $\\frac{1}{(1 - 0,9) + \\frac{0,9}{10}} \\approx $ 5,3 und $S_p$ = $\\frac{1}{(1 - 0,9) + \\frac{0,9}{20}} \\approx $ 6,9 . Lassen wir hier unsere Prozessoren gegen unendlich laufen, ist unser $S_p$ = 10. Das bedeutet, wir können weitere Server hinzufügen, aber es bleibt bei der 10-fachen Geschwindigkeit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb1bca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Parallel Speedup\n",
    "\n",
    "Hier sehen wir, wie sich die parallele Beschleunigung nach Amdahl's law je nach Prozessorzahl verhält. \n",
    "\n",
    "<img src=\"pictures/Parallel-Speedup.png\" alt=\"Parallel-Speedup\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Parallelisierungsstufen auf der Hardware\n",
    "\n",
    "Es gibt unterschiedliche Stufen der Parallelisierung auf der Hardware. Zum einen gibt es *Instruction-level parallelism* (Prozessoranweisungen). Dabei werden Prozessorbefehle durch die CPU-Architektur automatisch parallelisiert. Zum anderen gibt es *Data parallelism* (Daten). Jeder Prozessor verarbeitet die gleichen Befehle auf seiner eigenen Partition der Daten. Dadurch können unterschiedliche Daten parallel verarbeitet werden, beispielsweise durch verteilte Schleifeniterationen auf mehreren Prozessoren oder GPU processing. Auf der letzten Stufe der Parallelisierung haben wir *Task parallelism* (Aufgaben). Hierbei erhält jeder Prozessor/Knoten eine andere Aufgabe.\n",
    "\n",
    "### Varianten der Anfrage-Parallelisierung\n",
    "\n",
    "Es existieren unterschiedliche Varianten der Anfrage-Parallelisierung. Die erste Variante ist *Inter-Query parallelism* (mehrere nebenläufige Anfragen). Dies ist wichtig für eine effiziente Ressourcennutzung. Wartet eine Anfrage z.B. auf I/O, kann in der Zeit eine andere Anfrage ausgeführt werden. Dies erfordert *concurrency control*, also das Sperren, um Transaktionseigenschaften zu garantieren. Das ist auch wichtig für OLTP. Die zweite Variante ist *Intra-Query parallelism* (parallele Verarbeitung einer einzelnen Anfrage). Dieser unterteilt sich nochmal in *I/O parallelism, Intra-Operator parallelism* und *Inter-Operator parallelism*. Beim *I/O parallelism* werden nebenläufig mehrere Platten gelesen. Dabei wird mit spanned tablespaces und Partitionierung gearbeitet und eventuell auch mit Hardware RAIDs (versteckt). Beim *Intra-Operator parallelism* arbeiten mehrere Threads für den selben Operator, wie beispielsweise beim parallel sort, während beim *Inter-Operator parallelism* mehrere Teile eines Anfrageplans parallel laufen (pipeline). Letzteres ist wichtig für komplexe analytische Aufgaben (OLAP). \n",
    "\n",
    "Schauen wir uns als nächstes an, wie Inter-Operator parallelism genauer funktioniert. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6afae2-8b04-4930-80cf-44e3b959ef2e",
   "metadata": {},
   "source": [
    "### Pipeline Parallelism\n",
    "\n",
    "<img src=\"pictures/Pipeline-Parallelism.png\" alt=\"Pipeline-Parallelism\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "Im ersten Schritt können zwei Threads (T2 und T3) je einen Base Table scannen und für die Joins einen Hash Table bauen. </br>\n",
    "Dann scannt ein Thread den ersten Table. Der Thread prüft die Hashtabellen auf Kollisionen und versucht eine alternative Stelle zu finden (Probing). Bei Hash Tabellen kann es zu Kollisionen kommen, wenn zwei verschiedene Schlüssel auf denselben Hashwert abgebildet werden. Der zweite Thread fängt an die Sublisten zu sortieren (Sort) und fügt die ersten Listen zusammen. </br>\n",
    "Am Ende ist nur noch ein Thread vorhanden. Dieser gibt das Ergebnis zurück (Return) und arbeitet wieder wie zuvor weiter. </br>\n",
    "</br>\n",
    "\n",
    "Pipeline Parallelism ist auch bekannt als inter-operator parallelism: Eine Parallelisierung der Operatoren. \n",
    "Inter Operator bedeutet dabei: Während man an etwas arbeitet, gibt man die Ergebnisse weiter an andere Threads. Dadurch können diese schon früher andere Operationen auf den Daten durchführen.\n",
    "Es können somit mehrere Pipelines gleichzeitig ausgeführt werden, sofern mehrere vorhanden sind und auch nicht voneinander abhängen. </br>\n",
    "</br>\n",
    "\n",
    "Zudem hat Pipeline Parallelism einige Probleme. \n",
    "Der Synchronisationsaufwand ist sehr hoch, wenn Fehler gemacht werden oder auf Threads gewartet wird, die noch nicht fertiggestellt wurden. </br>\n",
    "Häufig kann auch nur wenig parallelisiert werden, sodass der Parallelisierungsgrad gering ist (degree of parallelism). Wenn man beispielsweise eine Anfrage mit fünf Operationen hat, kann man sie maximal mit Faktor 5 parallelisieren. Dazu kommen noch die Kosten der einzelnen Operationen: Nicht jede Operation kostet gleich viel. Wenn eine Operation zwar sehr schnell ausgeführt werden kann, aber eine andere Operation sehr lange braucht, muss trotzdem auf die längere gewartet werden. </br>\n",
    "Das Verfahren ist eher nur für Shared-Memory-Architekturen geeignet. Dabei spielen die I/O-Kosten eine untergeordnete Rolle. </br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975d690-6d98-4774-92bb-03bb21eec988",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Parallelism\n",
    "\n",
    "Pipeline Parallelism ist nicht immer anwendbar. Die Alternative bietet Data Parallelism. Die Daten werden in Teilmengen partitioniert. Sie werden also auf verschiedenen Rechnern oder Disks gespeichert. Die Idee hierbei ist, dass es Operationen gibt, die im selben Kontext nicht alles sehen müssen. Mit anderen Worten: Die Operationen werden geteilt auf Rechnern oder auch auf Prozessoren ausgeführt. Die entstehenden Ergebnisse auf den verschiedenen Rechnern/Prozessoren müssen dann nur noch zusammengefügt werden. Dadurch können Teilmengen unanhängig und parallel verarbeitet werden. </br>\n",
    "</br>\n",
    "Ein kleines Beispiel bei einer Selektion: </br>\n",
    "Man teilt einen Stapel Klausuren auf 5 Stapel auf. Gesucht werden alle 1er Kandidaten. Nun stellt man an jeden Stapel eine Person, die diesen Stapel Klausur für Klausur durchsucht und die Klausuren mit einer 1 herausnimmt. Das Ergebnis ist trotz mehrerer Teilstapel am Ende korrekt. Die Klausuren müssen nur noch zusammengelegt werden. \n",
    "An diesem Beispiel kann man nun sehen, dass die Selektion jedes Tupels unabhängig ist. </br>\n",
    "</br>\n",
    "Der maximale Parallelisierungsgrad hängt von der maximalen Anzahl von Teilmengen ab. Bei einer Selektion wäre es somit die Anzahl der Tupel.\n",
    "</br>\n",
    "Andere Operationen brauchen eine umfassendere Sicht auf die Daten. Dazu zählen z.B. die Gruppierung oder die Aggregation. In dem Beispiel bräuchten die Personen einen Blick in die anderen Stapel, um die jeweilige Operation auszuführen. Es reicht nicht mehr nur seinen eigenen Stapel zu betrachten. Sie benötigen also unterschiedliche Mengen, um korrekt zu funktionieren.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86925bb3-16b0-4747-8590-e2328358cf7b",
   "metadata": {},
   "source": [
    "### Grundlagen der Parallelen Anfragebearbeitung (Parallel Query Processing) \n",
    "\n",
    "Nun werden die Grundlagen der Parallelen Anfragebearbeitung genauer thematisiert. Hier sei zunächst ein kleiner Überblick über die Parallelen Architekturen, die Datenpartitionierungsstrategien und über die Kosten gegeben. </br>\n",
    "</br>\n",
    "\n",
    "Bei der parallelen Anfragebearbeitung kommt es darauf an, welche gemeinsamen Ressourcen man zur Verfügung hat. Es gibt drei Stufen von gemeinsam genutzten Ressourcen (Ressource Sharing). Diese werden auch Parallele Architekturen genannt. \n",
    "1. Shared-Memory (Hauptspeicher)\n",
    "2. Shared-Disk\n",
    "3. Shared-Nothing\n",
    "Parallele Architekturen haben den Vorteil, dass man keine teuren Großrechner benötigt auf denen man die gesamten Daten speichert. Bei Vergrößerung des teuren Großrechners würden unter Anderem die Kosten exponentiell steigen. Zudem ist die Reparatur sehr komplex. Aus diesem Grund verwendet man Parallelen Architekturen.</br>\n",
    "</br>\n",
    "\n",
    "Zudem gibt es noch unterschiedliche Partitionierungsvarianten bei der parallelen Anfragebearbeitung. Die Daten können zufällig mit Round-Robin verteilt werden, mit einer Hash-Funktion oder mit einer Bereichsfunktion.</br>\n",
    "</br>\n",
    "\n",
    "Je nach parallelem Operator variieren die Kosten. \n",
    "Eine Selektion verarbeitet beispielsweise nur ein Tupel auf einmal. Andere Operatoren benötigen zusätzlich eine Sortierung, Projektion, Gruppierung, Aggregation oder auch einen Join. </br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e578380",
   "metadata": {},
   "source": [
    "### Parallele Architekturen – Shared Memory\n",
    "\n",
    "Unter den Parallelen Architekturen ist Shared Memory die einfachste Architektur. Mehrere CPUs teilen sich einen einzigen Speicher und eigene Disks (Array). Die Kommunikation wird über einen einzigen gemeinsamen Bus betrieben. In der Realität hat jeder Prozessor nocht zusätzlich einen eigenen privaten Speicher (NUMA: non-uniform memory access). \n",
    "\n",
    "\n",
    "<img src=\"pictures/shared-memory.png\" alt=\"shared-memory\" width=\"350\" style=\"background-color: white;\"/>\n",
    "\n",
    "Man hat ein Interface auf den Speicher (M = Memory). Alle Prozessoren (P) können darüber auf den ganzen Speicher zugreifen. Entweder greifen sie auf diesselbe oder eben auch auf unterschiedliche Disks zu. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f7b36",
   "metadata": {},
   "source": [
    "### Parallel Architectures – Shared Disk\n",
    "\n",
    "Bei der parallelen Architektur Shared Disks existieren mehrere Knoten mit mehreren CPUs. Jeder Knoten besitzt eigene private Hauptspeicheradressen oder Hauptspeicheradressbereiche. Die Knoten greifen alle auf die gleichen Disks zu und nutzen die gleiche Datenbank. Diese Architektur wird oft bei NAS, SAN oder anderen Systemen verwendet. \n",
    "\n",
    "<img src=\"pictures/shared-disk.png\" alt=\"shared-disk\" width=\"350\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d013f6e-74f1-4c80-902a-dbe88e9bce1a",
   "metadata": {},
   "source": [
    "### Parallel Architectures – Shared Nothing\n",
    "\n",
    "Shared Nothing ist die meist genutzte Architektur für skalierbare Datenverarbeitung (Large-Scale Data Management). Bei der Shared Nothing Architektur hat jeder Knoten seinen eigenen Satz an CPUs, Speicher und Disks. Im Prinzip ist jeder Knoten somit ein eigener Server. \n",
    "Um die Vorteile der Architektur im vollem Umfang nutzen zu können, müssen die Daten über die Knoten partitioniert werden. Hierbei gibt es unterschiedliche Partitionierungsvarianten, die im Anschluss noch weiter thematisiert werden. Für die Partitionierung werden die Daten direkt über eine Knoten-zu-Knoten Kommunikation ausgetauscht. Die Nachrichten haben dabei einen signifikanten Overhead. \n",
    "Es werden somit doch Daten zwischen den Knoten ausgetauscht. Die Knoten teilen sich nämlich immernoch das Netzwerk. Der Name Shared Nothing ist also etwas irreführend.\n",
    "\n",
    "<img src=\"pictures/shared-nothing.png\" alt=\"shared-nothing\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14157ef-42e5-4f0d-824e-1931f4db6130",
   "metadata": {},
   "source": [
    "### Partitionierung\n",
    "\n",
    "Partitionierung bedeutet möglichst disjunkte Teilmenge zu generieren. Je nach Fall können die Daten unterschiedlich partitioniert werden. Bei Verkaufsdaten kann zum Beispiel jedes Jahr seine eigene Partition erhalten. \n",
    "\n",
    "Für eine Shared Nothing Architektur müssen die Daten auf mehreren Knoten verteilt werden. Würde man die Daten einfach replizieren, kann man aus der Sicht der Anfragebearbeitung auch von einer Shared-Disk sprechen. Es ist so als würde man alle Daten auf einer Disk speichern. Die lokalen Disks verhalten sich dann wie Caches. Außerdem muss bei der Replikation die Konsistenz sichergestellt werden.  \n",
    "\n",
    "Manche Datenbankanfragen können auf bestimmte Bereiche eingegrenzt werden, sofern man sicherstellen kann, dass alle relevanten Daten in der entsprechenden Partition zu finden sind. Die Partitionierung gewinnt durch solche Eigenschaften an Vorteil. \n",
    "\n",
    "In der Datenbankadministration entsteht durch Partitionierung ein weiterer Vorteil. Nicht mehr benötigte Partitionen wie alte Verkäufe usw. können einfach verworfen werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88416b-e137-4355-8149-8a862b0bda9e",
   "metadata": {},
   "source": [
    "### Partitionierungsstrategien\n",
    "\n",
    "Bei der Durchführung der Partitionierung können verschiedene Strategien gewählt werden. Jede bietet andere Vorteile.\n",
    "\n",
    "Bei **Round Robin** erhält jede Partition ein Tupel pro Runde. Dadurch haben alle Teilmengen garantiert eine möglichst gleiche Anzahl von Tupeln. Zwischen den einzelnen Tupeln in einer Partition herrscht widerrum keine explizite Beziehung.  \n",
    "\n",
    "Bei **Hash Partitioning** wird eine Menge von Partitionsspalten definiert. Für jede Spalte wird ein Hashwert generiert. Dieser Hashwert wird verwendet, um zu entscheiden, welche Partition als Ziel für die Tupel gewählt wird. Der Hashwert wird für jede Zeile berechnet, basierend auf den Werten der Partitionsspalten. Das Tupel wird dann der Partition zugeordnet, die dem berechneten Hashwert entspricht.\n",
    "    \n",
    "**Range Partitioned** bedeutet, dass eine Menge von Partitionsspalten definiert wird. Die Domäne der Spalten wird in Bereiche aufgeteilt. Die Tupel werden den Partitionen basierend auf den Bereichswerten zugeordnet. Alle Tupel aus einer Partition stammen somit aus dem gleichen Bereich. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225daf3a",
   "metadata": {},
   "source": [
    "### Data Parallelism: Beispiel \n",
    "\n",
    "<img src=\"pictures/Data-Parallelism-example.png\" alt=\"Data-Parallelism-example\" width=\"400\" style=\"background-color: white;\"/>\n",
    "\n",
    "Ein Client schickt eine SQL-Anfrage an einen Cluster-Knoten. Dieser Cluster-Knoten wird dann der Coordinator. Der Coordinater kompiliert die Anfrage. Er parst, überprüft und optimiert also die Anfrage. Zudem überlegt er sich wie die Anfrage parallelisiert werden kann. Die Teilpläne werden an andere Knoten geschickt und der Coordinator führt den eignen Teilplan auch aus. Am Ende sammelt der Coordinator alle Teilergebnisse und finalisiert diese für die Ausgabe. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0618ed-3125-4baa-a3fa-41d78da8ad23",
   "metadata": {},
   "source": [
    "### Data Parallelism: Beispiel \n",
    "\n",
    "<img src=\"pictures/Data-Parallelism-example_2.png\" alt=\"Data-Parallelism-example_2\" width=\"400\" style=\"background-color: white;\"/>\n",
    "\n",
    "Data Parallelism ist eine Strategie, die in shared-nothing und shared-disk Architekturen genutzt wird. Dabei führen multiple Instanzen eines Verarbeitungsplans gleichzeitig auf verschiedenen Knoten des Systems Operationen auf unterschiedlichen Datenpartitionen durch. Die Ergebnisse werden anschließend zusammengeführt. Bei komplexen Anfragen können die Ergebnisse auch für weitere parallele Verarbeitungsschritte neu verteilt werden. Dies ermöglicht eine effiziente und beschleunigte Verarbeitung großer Datensätze in verteilten Systemen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce53f70-5a87-413d-97b0-bfab3ab6abb1",
   "metadata": {},
   "source": [
    "### Parallel Operators\n",
    "\n",
    "Bei der Verarbeitung von Daten in parallelen Systemen strebt man idealerweise an, dass parallele Operatoren auf unterschiedlichen Partitionen der Daten laufen. Dies bedeutet, dass Operationen direkt zu den Daten geschickt werden können. Dies ist besonders einfach für grundlegende \"tuple-at-a-time\" Operatoren wie Scans, Index-Scans und Selektionen.\n",
    "\n",
    "Allerdings gibt es Herausforderungen bei sogenannten \"Blocking Operatoren\", die alle Daten sehen müssen. Beispielsweise können Sortier- und Aggregationsoperatoren nur parallel vorverarbeitet werden. Der finale Schritt einer Sortierung und Aggregation wird auf einem einzelnen Knoten durchgeführt, es sei denn, es handelt sich um Teilpläne, die dies nicht erfordern.\n",
    "\n",
    "Ein Beispiel hierfür sind Joins, die passende Tupel benötigen. Dies kann durch die Organisation der Eingabedaten oder durch die Ausführung des Joins am Koordinator nach Zusammenführung der Teilergebnisse erreicht werden. Letzteres führt jedoch zu einer nicht mehr parallelen Verarbeitung. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748cca7a-1cbb-493d-9490-b5ad6dabeae4",
   "metadata": {},
   "source": [
    "### Notationen und Annahmen\n",
    "\n",
    "- S\tRelation S\n",
    "- S[i,h] Partition i der Relation S basierend auf Partitionierungsschema h\n",
    "- B(S) Anzahl der Blöcke von S\n",
    "- p\tAnzahl der Knoten\n",
    "\n",
    "In der Annahme einer Shared-Nothing-Architektur, bei der jeder Knoten über eigene lokale Ressourcen verfügt und Datenpartitionen unabhängig voneinander verarbeitet, sind Netzwerktransfers und Diskzugriffe als kostenequivalent angenommen. Obwohl die Netzwerkkosten gelegentlich höher sein können, wird davon ausgegangen, dass die Netzwerk- und Diskbandbreite heutzutage etwa gleichwertig sind.\n",
    "\n",
    "Es ist jedoch zu berücksichtigen, dass das Netzwerk geteilt wird und durch Durchsatzlimits an Switches und Routern begrenzt ist. Diese Einschränkung kann Auswirkungen auf die Gesamtleistung haben, insbesondere wenn viele Knoten gleichzeitig auf das Netzwerk zugreifen.\n",
    "\n",
    "Eine weitere Annahme betrifft Partitionierungsansätze wie Hashing oder Range-Partitionierung Sie erzeugen ungefähr gleichgroße Partitionen auf den Servern. Dies ist wichtig, um eine gleichmäßige Lastverteilung und effiziente parallele Verarbeitung zu gewährleisten. Bei den folgenden Berechnungen wird von diesem Fall ausgegangen und nicht davon, dass die Daten ungleichmßig verteilt liegen. \n",
    "\n",
    "Die letzte Annahme, dass S[i,h] > M, bedeutet, dass die Größe der Daten auf einem bestimmten Knoten i größer ist als der verfügbare Hauptspeicher (M). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff0698-9bf9-4c88-8089-37d6f45791b0",
   "metadata": {},
   "source": [
    "### Parallel Selection / Projection\n",
    "\n",
    "Die parallele Durchführung von Selektion und Projektion erweist sich als äußerst effizient und einfach. Daher werden diese Operationen auch als \"Embarrassingly parallel\" bezeichnet. Jeder Knoten kann die Operation unabhängig auf seiner eigenen existierenden Datenpartition ausführen. Besonders vorteilhaft ist, dass die Selektion keinen Kontext benötigt, und die Daten willkürlich partitioniert vorliegen können.\n",
    "\n",
    "In diesem Szenario werden die Teilergebnisse am Ende einfach zusammengeführt. Die Kosten für diese Parallelität werden durch die Formel $B(S)/p + Transfer$ bestimmt, wobei B(S) die Datenmenge für die Operation ist und p die Anzahl der beteiligten Knoten darstellt. Die Daten liegen verteilt auf den Blöcken vor. Diese müssen nur parallel gelesen werden, also durch $p$ geteilt werden. Hinzu kommen die Transferkosten. Diese basieren auf der Selektivität, also wie viele Tupel weitergegeben werden müssen. \n",
    "Insgesamt ermöglicht die parallele Ausführung von Selektion und Projektion eine effiziente Verarbeitung großer Datenmengen in verteilten Umgebungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf285b-c8b7-4ad3-86d1-651bd6e199ac",
   "metadata": {},
   "source": [
    "### Parallele Gruppierung & Aggregation\n",
    "\n",
    "Die parallele Gruppierung und Aggregation in verteilten Systemen erfolgt in zwei Phasen, um eine effiziente Verarbeitung zu gewährleisten. In der ersten Phase wird eine lokale Gruppierung und Aggregation auf jeder Partition durchgeführt. Anschließend erfolgt in der zweiten Phase ein Zusammenführen der Ergebnisse.\n",
    "\n",
    "Die Kosten dieses Prozesses setzen sich zusammen aus den lokalen Algorithmuskosten (3 B(S)/p), dem Transfer der (kleinen) Ergebnisse zwischen den Partitionen und der schnellen Zusammenführung, die schon fast vernachlässigbar ist. Also insgesamt ist die Formel: $$3 B(S)/p lokaler Algorithmus + Transfer (kleiner) Ergebnisse + (schnelle) Zusammenfuehrung$$\n",
    "\n",
    "Dieser Ansatz funktioniert besonders gut für assoziative Aggregationsoperationen wie MIN, MAX, SUM und COUNT, sowie für AVG, das sich als SUM / COUNT berechnen lässt.\n",
    "\n",
    "Um eine kostspielige zweite Phase zu vermeiden, kann Hashing verwendet werden, um die Gruppierungsspalten zu partitionieren. Alternativ ist auch eine Parallelisierung der Merge-Phase möglich. Beide Ansätze tragen dazu bei, die Gesamtkosten der parallelen Gruppierung und Aggregation zu minimieren und eine effiziente Verarbeitung großer Datenmengen in verteilten Umgebungen zu ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc544fa5-0799-4d54-8974-7d4cb8cd0b7f",
   "metadata": {},
   "source": [
    "### Parallele Sortierung\n",
    "\n",
    "Die parallele Sortierung in verteilten Systemen kann durch verschiedene Ansätze realisiert werden. Beispielsweise wird bei der \"**Range partitioned sort**\" Methode die Relation anhand der Sortierattribute in unterschiedliche Bereiche partitioniert. Diese Partitionen werden dann lokal sortiert, beispielsweise mit dem Two-Phase Multiway Merge Sort (TPMMS). Die Gesamtkosten dieses Ansatzes setzen sich aus den Kosten für die Partitionierung, den Transfer der Partitionen und der lokalen Sortierung (3 B(S)/p) zusammen. Eine Herausforderung besteht jedoch darin, eine Partitionierung mit gleichgroßen Bereichen zu finden. Die komplette Formel sieht dann wie folgt aus: $$B(S) Partitionierung + B(S) Transfer + 3 B(S)/p lokale Sortierung$$ \n",
    "\n",
    "Eine alternative Methode ist der \"**Parallel external sort-merge**\". Hier wird die existierende Partitionierung genutzt, und jede Partition wird lokal sortiert. Für die Sortierung kann auch wieder der Two-Phase Multiway Merge Sort (TPMMS) genutzt werden. Die sortierten Partitionen müssen dann gemergt werden, wobei die Kosten für den Pair-wise Merge aus den lokalen Sortierungen, dem Transfer der Daten und dem lokalen Merge resultieren. \n",
    "Dies kann durch einen log2(p)-stufigen Merge-Prozess erfolgen, wobei p die Anzahl der beteiligten Knoten ist. Die Gesamtkosten für den Merge können durch Multi-way Merge-Verfahren weiter optimiert werden. Zusammen ergibt sich für die Kosten des Pair-wise Merge die folgende Formel: $$3 B(S)/p lokale Sortierung + log2(p)*B(S)/2 Transfer + log2(p)*B(S) lokaler Merge$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4291b4-b51f-4af8-8065-0dd6b996aab8",
   "metadata": {},
   "source": [
    "### Symmetric Fragment-and-Replicate Join\n",
    "\n",
    "<img src=\"pictures/Symmetric-Fragment-and-Replicate-Join.png\" alt=\"Symmetric-Fragment-and-Replicate-Join\" width=\"400\" style=\"background-color: white;\"/>\n",
    "\n",
    "Der Symmetric Fragment-and-Replicate Join ist eine spezielle Methode, um das Joinen von Relationen R und S effizienter zu gestalten. Im herkömmlichen Fall müsste jedes Tupel des kartesischen Produkts betrachtet werden, was in parallelen DBMS dazu führen würde, dass jede Partition von R mit jeder Partition von S kombiniert werden müsste.\n",
    "\n",
    "Bei der Symmetric Fragment-and-Replicate Methode wird R in m Partitionen und S in n Partitionen fragmentiert. Diese Partitionen werden dann repliziert, wobei jede Partition von R n-mal und jede Partition von S m-mal repliziert wird. Auf einem System mit m * n = p Knoten kann dann jeder Knoten lokal genau ein Partitionspaar von R und S verbinden.\n",
    "\n",
    "Die Gesamtkosten dieses Algorithmus setzen sich aus den Fragmentierungskosten, den Transferkosten und den lokalen Join-Kosten zusammen. Die Fragmentierungskosten beinhalten die Größe von R und S ($B(R) + B(S)$), während die Transferkosten die Kosten für das Übertragen der replizierten Partitionen darstellen mit $\\frac{B(R)}{m} * n + \\frac{B(S)}{n} * m$. Der lokale Join wird auf jedem Knoten durchgeführt. Die Kosten variieren je nach Wahl des Joins. \n",
    "Hier seien die Kosten nochmal zusammengefasst dargestellt:\n",
    "- Fragementierungskosten: $B(R) + B(S)$\n",
    "- Transferkosten: $\\frac{B(R)}{m} * n + \\frac{B(S)}{n} * m$\n",
    "- Kosten des lokalen Joins: $???$\n",
    "\n",
    "Der Parallel-Join-Algorithmus bietet den Vorteil, dass er für alle Join-Varianten, einschließlich Theta-Joins, funktioniert. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f361c36",
   "metadata": {},
   "source": [
    "### Asymmetric Fragment-and-Replicate Join\n",
    "\n",
    "<img src=\"pictures/Asymmetric-Fragment-and-Replicate-Join.png\" alt=\"Asymmetric-Fragment-and-Replicate-Join\" width=\"200\" style=\"background-color: white;\"/>\n",
    "\n",
    "Der Asymmetric Fragment-and-Replicate Join ist eine Optimierungsmöglichkeit, insbesondere wenn die Relation S deutlich kleiner als die Relation R ist. Die grundlegende Idee besteht darin, die existierende Partitionierung von R zu nutzen und die Relation S auf jedem Knoten zu replizieren.\n",
    "\n",
    "Die Kosten dieses Ansatzes setzen sich hauptsächlich aus den Transferkosten zusammen, da S auf jedem Knoten repliziert wird. Die Formel $p * B(S)$ beschreibt die Transferkosten, wobei p die Anzahl der beteiligten Knoten und $B(S)$ die Größe von S ist. Die Kosten sind somit insgesamt:\n",
    "\n",
    "- Transferkosten: $p*B(S)$\n",
    "- Lokaler Join: $???$\n",
    "\n",
    "Es ist wichtig zu beachten, dass der Asymmetric Fragment-and-Replicate Join als ein Spezialfall des Symmetric Fragment-and-Replicate Algorithmus betrachtet werden kann, bei dem m (Anzahl der Partitionen von R) gleich p ist und n (Anzahl der Partitionen von S) gleich 1 ist. Dieser Ansatz ist besonders effizient, wenn die Größe von S im Vergleich zu R vernachlässigbar ist und ermöglicht eine optimierte Verarbeitung von Join-Operationen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed41567-2d2b-43c1-b0c2-d36b5e1f5154",
   "metadata": {},
   "source": [
    "### Parallele Equi-Joins\n",
    "    \n",
    "Parallele Equi-Joins, insbesondere Natural Joins und Equi-Joins, lassen sich effizient parallelisieren. Die grundlegende Idee besteht darin, die Relationen R und S mit derselben Partitionierungsstrategie, anhand des Join-Schlüssels, zu partitionieren.\n",
    "\n",
    "Durch diese Vorgehensweise landen alle Tupel aus R und S mit dem gleichen Joinattribut auf den gleichen Knoten. Dadurch sind keine weiteren Broadcasts oder Replikationen notwendig sind. Die Joins können somit lokal auf den jeweiligen Knoten ausgeführt werden.\n",
    "\n",
    "Es gibt drei Varianten von parallelen Equi-Joins, die auf der existierenden Partitionierung aufbauen. Die Strategien bieten verschiedene Möglichkeiten, um Join-Operationen effizient in verteilten Systemen durchzuführen, wobei die Auswahl zwischen den Varianten von der vorhandenen Partitionierung und den spezifischen Anforderungen abhängt.\n",
    "\n",
    "1. **Co-Located Join:** Wenn sowohl die Relationen R als auch S bereits anhand des Joinattributs partitioniert sind, ermöglicht der Co-Located Join eine effiziente lokale Durchführung der Join-Operation. Es ist keine Neupartitionierungn notwendig. Die Kosten für diese Variante sind in erster Linie durch die lokalen Join-Kosten geprägt:\n",
    "\n",
    "- Lokale Joinkosten: $???$\n",
    "\n",
    "</br>\n",
    "2. **Directed Join:** Falls nur eine der Relationen, beispielsweise R, anhand des Joinattributes partitioniert ist, kann der Directed Join angewendet werden. Hier wird die andere Relation, in diesem Fall S, neu partitioniert, um die gleiche Partitionierung wie R zu erhalten. Die Kosten setzen sich aus den Partitionierungskosten von S und den Transferkosten der neu partitionierten Daten zusammen, gefolgt von den lokalen Joinkosten.\n",
    "\n",
    "- Partitionierungskosten: $B(S)$\n",
    "- Transferkosten: $B(S)$\n",
    "- Lokale Joinkosten: $???$\n",
    "    \n",
    "</br>\n",
    "3. **Re-Partitioning Join:** Wenn keine der Relationen passend partitioniert ist, erfolgt ein Repartition Join. Beide Relationen R und S werden anhand des Joinattributes neu partitioniert, um eine gemeinsame Partitionierung zu erreichen. Die Kosten umfassen die Partitionierungskosten beider Relationen, die Transferkosten für die neu partitionierten Daten und schließlich die lokalen Joinkosten.\n",
    "\n",
    "- Partitionierungskosten: $B(S) + B(R)$\n",
    "- Transferkosten: $B(S) + B(R)$\n",
    "- Lokale Joinkosten: $???$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ef8df-54c1-45f9-8b07-057f59512663",
   "metadata": {},
   "source": [
    "### Grenzen der Parallelen Datenbanken\n",
    "\n",
    "Parallele Datenbanken weisen gewisse Grenzen auf, die ihre Skalierbarkeit beeinträchtigen:\n",
    "\n",
    "1. **Begrenzte Skalierung von Datenbank-Clustern:** Die Skalierbarkeit von Datenbank-Clustern zeigt oft eine abflachende Beschleunigungskurve jenseits von etwa 128 Knoten. Dies liegt daran, dass der Kommunikationsmehraufwand mit zunehmender Knotenzahl die Beschleunigung reduziert. Ein Beispiel hierfür ist das harte Limit von 1000 Knoten für DB2 im Jahr 2010.\n",
    "\n",
    "2. **Shared Disk-Architektur:** Shared Disk-Systeme haben Skalierungsgrenzen, da der Bus- und Synchronisationsaufwand mit zunehmender Größe Overhead verursacht. Für Updates entstehen Cache-Coherency-Probleme, während für Lesevorgänge die I/O-Bandbreite begrenzt ist.\n",
    "\n",
    "3. **Shared Nothing-Architektur:** Obwohl Shared Nothing-Systeme eine bessere Skalierbarkeit bieten, können sie den Verlust von Knoten nicht leicht kompensieren. In großen Clustern sind Ausfälle häufig, was bedeutet, dass der Verlust von Knoten auch einen Verlust von Daten mit sich bringen kann, es sei denn, die Daten sind repliziert. Die Replikation von Daten bringt jedoch einen Mehraufwand mit sich, da die Konsistenz der replizierten Daten aufrechterhalten werden muss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1458e3f-42ee-4a98-9c9e-5192864b0f32",
   "metadata": {},
   "source": [
    "### Wann passen traditionelle Datenbanken nicht gut?\n",
    "\n",
    "Traditionelle Datenbanken stoßen an ihre Grenzen in verschiedenen Szenarien. Zum Einen sind sie weniger geeignet für die Analyse unstrukturierter Daten, insbesondere im Fall von Textdaten oder bei Dokumenten. Wenn kein relationales Schema vorhanden ist, erweisen sich traditionelle Datenbanken ebenfalls als ungeeignet.\n",
    "\n",
    "Ein weiterer Aspekt ist die Kosteneffektivität. Wenn lediglich einfache Hardware vorhanden ist oder veränderbare Cluster mit horizontaler Skalierung (horizontal scaling) benötigt werden, können traditionelle Datenbanken an Effizienz verlieren. Wenn beispielsweise ab und zu ein Server hinzugefügt oder entfernt wird, bedeutet das jedes Mal für die Datenbank eine Umorganisation. Das inkrementelle Wachstum, also das Hinzufügen von Knoten ohne zusätzlichen Aufwand, ist oft eine Herausforderung für herkömmliche Datenbankmodelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837834e-a12f-41ac-b647-40a6cd192c77",
   "metadata": {},
   "source": [
    "### Webindex für eine Suchmaschine - Beispiel\n",
    "\n",
    "Bei der Entwicklung eines Webindex für eine Suchmaschine treten spezifische Anforderungen auf. Die Suchmaschine durchsucht das Internet, speichert Dokumente mit Worten und Links ab. Das Speichern der Daten kann man sich wie folgt vorstellen:\n",
    "\n",
    "- Dokumente beinhalten Wörter: ```(Doc-URL, [list of words])```\n",
    "- Dokumente beinhalten Links: ```(Doc-URL, [Target-URLs])```\n",
    "\n",
    "Um einen effizienten Index zu erstellen, werden die Dateien invertiert, was eine Zuordnung von Wörtern zu URLs ermöglicht. \n",
    "\n",
    "- Invertiere die Dateien: ```(word, [list of URLs])```\n",
    "\n",
    "Angenommen man möchte mit einer Anfrage nach der 'Universität Hannover' suchen. Eine Suchmaschine soll alle möglichen Websiten finden in denen 'Universität Hannover' vorkommt und es noch nach Relevanz sortieren. Um es nach der Relevanz sortieren zu können, muss ein Ranking erstellt werden. Dieses wird durch Berechnung eines invertierten Graphen erstellt. Es sind im Prinzip verschiedene Websiten, die auf andere Seiten mit der URL verweisen. Dadurch kann man z.B. die Relevanz von Dokumenten etc. einschätzen. \n",
    "Ein bekanntes Beispiel ist das Page Rank von Google, das nach Larry Page, dem Co-Founder von Google, benannt worden ist.  \n",
    "\n",
    "- ```(Doc-URL, [URLs-pointing-to-it])```\n",
    "\n",
    "Traditionelle relationale Datenbankmanagementsysteme (RDBMS) sind jedoch für diese Aufgabe weniger geeignet. Das relationale Schema passt nicht optimal, und der Import sowie die Konvertierung der Dokumente erweisen sich als kostspielig. \n",
    "\n",
    "RDBMS sind primär für Transaktionsverarbeitung konzipiert und bieten Garantien hinsichtlich absoluter Konsistenz, was für die Analyse von Webdokumenten, die eher read-only sind, nicht unbedingt erforderlich ist. Daher sind alternative Ansätze besser geeignet, um den spezifischen Anforderungen eines Webindex gerecht zu werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda8668-8364-4ded-b027-e068d8448464",
   "metadata": {},
   "source": [
    "### Fortlaufende Neuentwicklung\n",
    "\n",
    "Die kontinuierliche Weiterentwicklung von Technologien hat dazu geführt, dass führende FAANG-Unternehmen zunehmend auf stark verteilte Systeme setzen. Ein bemerkenswertes Beispiel ist Google, das bereits im Jahr 2006 auf 450.000 kostengünstige Commodity-Server setzte, die in Clustern mit 1000 bis 5000 Knoten organisiert waren. Der Fokus bei der Neugestaltung solcher Systeme liegt auf Hochskalierbarkeit und Ausfalltoleranz. Auch hier werden die Daten wieder häufig repliziert und vielen verschiedenen Servern gespeichert, um die Ausfalltoleranz zu gewährleisten. \n",
    "\n",
    "Ein entscheidendes Merkmal ist ein generisches und schemafreies Datenmodell, das die Flexibilität erhöht. Der Einstieg erfolgt oft mit einem Datenablagesystem als Ausgangspunkt. Der nächste Schritt in dieser Entwicklung ist eine verteilte Analyse. \n",
    "Dieser Ansatz ermöglicht es den Unternehmen, sich an die sich ständig verändernden Anforderungen anzupassen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f6e24-740a-4757-b2d3-6fbeb08dd4bd",
   "metadata": {},
   "source": [
    "### Anforderungen an die Speicherung\n",
    "\n",
    "Die Anforderungen an die Speicherung von Daten in modernen Umgebungen sind vielfältig und anspruchsvoll. Insbesondere bei sehr großen Dateien im Bereich von Terabytes bis Petabytes ist eine robuste Speicherlösung entscheidend. Neben der Skalierbarkeit sind hohe Verfügbarkeit und Replikation entscheidend, um Ausfälle zu vermeiden.\n",
    "\n",
    "Ein weiterer kritischer Punkt ist der hohe Durchsatz, wobei Lese- und Schreiboperationen nicht durch andere Server gehen sollen. Idealerweise wird versucht alles lokal zu berechnen. \n",
    "Um einzelne Ausfallpunkte (Single Point of Failure) zu vermeiden, müssen Koordinatoren redundant sein. In diesem Kontext hat das Google Filesystem (GFS) als Referenzpunkt gedient, indem es eine Architektur mit hohem Durchsatz und hoher Verfügbarkeit bereitstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad6ccd-d92d-4788-ab43-22a7ae81d8a0",
   "metadata": {},
   "source": [
    "### The Storage Model – Distributed File System\n",
    "\n",
    "<img src=\"pictures/The-Storage-Model.png\" alt=\"The-Storage-Model\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "Das Storage-Modell eines verteilten Dateisystems basiert auf mehreren Schlüsselkomponenten. Das Dateisystem selbst ist auf mehrere Knoten, auch DataNodes genannt, verteilt und verfügt über einen gemeinsamen Namensraum für den gesamten Cluster. Die Verwaltung der Metadaten erfolgt auf einem speziellen Knoten, dem NameNode. Das Zugriffsmodell ist als \"Write-once-read-many\" konzipiert.\n",
    "\n",
    "Die Dateien werden in Blöcke von 128 MB aufgeteilt, wobei jeder Block auf mehreren DataNodes repliziert wird. Der Client kann den Standort eines Blocks identifizieren und direkt über das Netzwerk die Daten von einem DataNode anfordern. \n",
    "\n",
    "Eine Herausforderung besteht jedoch in der begrenzten Bandbreite zum Zugriff auf die Daten. Das Abrufen von Daten aus einem entfernten Speicher ist teurer als lokale Zugriffe (50 MB/s remote access vs. 150-200 MB/s local access). \n",
    "Um dieses Problem zu umgehen, versucht das Map/Reduce-Framework, Berechnungen möglichst nah an den Daten auszuführen. Die Berechnung zu bewegen ist billiger als die Daten zu bewegen. In diesem Konzept sind die Knoten sowohl für die Speicherung als auch für die Berechnungen verantwortlich."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442e894-2197-45ed-ba6c-669b6df550b7",
   "metadata": {},
   "source": [
    "### Retrieving and Analyzing Data\n",
    "\n",
    "Die Suche und Analyse von Daten in modernen Systemen erfordert eine spezielle Herangehensweise. Die Daten liegen oft in maßgeschneiderten Einheiten (Records) innerhalb von Dateien vor, was ein sehr generisches Datenmodell darstellt. Ein häufig verwendetes Modell ist das \"Key/Value-Modell\".\n",
    "\n",
    "Um Analyse- und Transformationsaufgaben durchzuführen, wurden zuvor einfache SQL-Anfragen genutzt. Nun werden direkt ganze Programme geschrieben. Diese Programme müssen jedoch parallel, fehlertolerant und hochskalierbar sein, was die Programmierung erschwert. Hier kommt das Map/Reduce-Programmiermodell ins Spiel, das speziell für diese Anforderungen entwickelt wurde. Es ermöglicht die effiziente Verarbeitung von großen Datenmengen, indem es Programme in kleine, parallel ausführbare Aufgaben aufteilt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62756e90",
   "metadata": {},
   "source": [
    "### Skalierungsmuster\n",
    "\n",
    "Skalierungsmuster spielen eine entscheidende Rolle bei der Verarbeitung großer Datenmengen. Dieser Prozess kann in verschiedene Phasen unterteilt werden.\n",
    "\n",
    "<img src=\"pictures/Skalierungsmuster.png\" alt=\"Skalierungsmuster\" width=\"300\" style=\"background-color: white;\"/>\n",
    "    \n",
    "In der **Phase 0** erfolgt die Verteilung der Daten, auch als **Split** bezeichnet. Hier werden die Daten in Teilmengen aufgeteilt, um die Verarbeitung zu erleichtern. \n",
    "In der darauffolgenden **Phase 1** werden Berechnungen auf diesen Teilmengen durchgeführt, was als **Map** bezeichnet wird. Jede Teilmenge wird individuell analysiert oder bearbeitet. \n",
    "In der **Phase 2** erfolgt die Zusammenführung der Teilmengen, bekannt als **Reduce**. Hier werden die Ergebnisse der vorherigen Phase in einer gemeinsamen Betrachtung zusammengeführt. \n",
    "\n",
    "Ein Beispiel für dieses Muster ist der Two-Phase-Multiway-Mergesort (TPMMS), bei dem in Phase 1 Teile der Daten sortiert werden und in Phase 2 die sortierten Teillisten zusammengeführt werden.\n",
    "\n",
    "Dieses Skalierungsmuster findet auch Anwendung in verschiedenen Anwendungsfällen wie Datenanalyse oder dem Bau von Indizes. Bei der Datenanalyse werden in Phase 1 Gruppierungen durchgeführt, gefolgt von der Aggregation in Phase 2. Beim Bau von Indizes werden in Phase 1 Teilmengen indiziert und in Phase 2 die Indizes zusammengeführt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d070f02-e018-4d1f-9ba0-6fd8586658d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Map Reduce & Hadoop\n",
    "\n",
    "### “MapReduce is a programming model and an associated implementation for processing and generating large data sets.”\n",
    "\n",
    "<img src=\"pictures/MapReduce-Simplified-data-processing-on-large-clusters.png\" alt=\"MapReduce-Simplified-data-processing-on-large-clusters\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "Das zugrunde liegende Paper zu MapReduce ist von Jeffrey Dean und Sanjay Ghemawat. Es stammt aus dem Jahr 2006. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93701b9b-c94a-4698-88f7-66e71af5395c",
   "metadata": {},
   "source": [
    "### Was ist Map/Reduce?\n",
    "\n",
    "Map/Reduce ist ein Programmiermodell, das auf Konzepten der funktionalen Programmierung basiert und sich besonders gut für parallele Verarbeitung eignet. Es ermöglicht automatische Parallelisierung und Verteilung von Daten und Berechnungslogik. Diese saubere Abstraktion erleichtert die Programmierung für Entwickler und Entwicklerinnen.\n",
    "\n",
    "In der funktionalen Programmierung erfolgt die Berechnung durch die Evaluierung mathematischer Funktionen. Ein zentrales Prinzip dabei ist die Vermeidung von Zustandsänderungen, also keine Seiteneffekte. Das Ergebnis einer Funktion hängt ausschließlich von den Eingangsparametern ab.\n",
    "\n",
    "Map und Reduce sind sogenannte Higher-Order-Funktionen zweiter Ordnung, die benutzerdefinierte Funktionen als Parameter nutzen und selbst eine Funktion als Ergebnis liefern. Programmierende müssen lediglich zwei Funktionen implementieren: die Map-Funktion, die auf jeden Datensatz angewendet wird, und die Reduce-Funktion, die die aggregierten Ergebnisse zusammenführt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b3012-8fcb-4b7d-a0a0-6c787488990e",
   "metadata": {},
   "source": [
    "### Grundbausteine\n",
    "\n",
    "Die Grundbausteine von Map/Reduce umfassen ein einfaches Datenmodell in Form von Schlüssel/Wert-Paaren $(K \\times V)$, auch als \"key/value pairs\" bekannt. Diese Paare können verschiedene Formen annehmen, wie zum Beispiel (int, string) oder (string, [string]).\n",
    "\n",
    "Nutzer definieren zwei wesentliche Funktionen im Map/Reduce-Programm. Die Map-Funktion $(k', v') \\rightarrow list(k_{1}, v_{1})$ wird auf jedes Schlüssel/Wert-Paar angewendet und gibt oft nur ein neues Paar $(k_{1}, v_{1})$ aus. \n",
    "\n",
    "Die Reduce-Funktion $(k_{1}, list(v1)) \\rightarrow list(v_{2})$ agiert meist auf ein einzelnes Wertpaar $(v_{2}$, oft auch mit Rückgabe des ursprünglichen Schlüssels $k_{1}$. Dadurch ist eine Verkettung von Map/Reduce-Schritten möglich.\n",
    "\n",
    "Ein MapReduce-Programm nimmt eine Liste von Schlüssel/Wert-Paaren als Eingabe und gibt eine Liste von Werten als Ausgabe aus. Die Ausgabe erfolgt erst am Ende nachdem sowohl die Map- als auch die Reduce-Funktion ausgeführt wurde. \n",
    "Dabei stehen Nutzern und Nutzerinnen zwei Herausforderungen bevor: das Entwerfen geeigneter Map- und Reduce-Funktionen sowie die Gewährleistung einer verteilten, fehlertoleranten und effizienten Ausführung des Programms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8244244",
   "metadata": {},
   "source": [
    "### MapReduce Workflow\n",
    "\n",
    "<img src=\"pictures/MapReduce-workflow.png\" alt=\"MapReduce-workflow\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "Der MapReduce Workflow beinhaltet einige Schritte. Zunächst werden aus dem Input Datafile ein Key und Value gelesen und der Mapper-Funktion übergeben. Das Ergebnis daraus in Form von einem Key und Value wird an eine Key-Sorter-Funktion übergeben. Diese Funktion gibt einen Key mit einer Liste an Werten an eine Reducer-Funktion weiter. Das Resultat aus der Reducer-Funktion in Form von einem key und value wird in eine Output Datafile geschrieben. Den ganzen Workflow kann man mehrmals wiederholen und aneinander ketten, wenn dies je nach Situation verlangt wird. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ab009",
   "metadata": {},
   "source": [
    "#### Aufgabe: Bestimme für jedes Wort dessen Häufigkeit im Korpus\n",
    "\n",
    "<img src=\"pictures/Aufgabe-Häufigkeit-bestimmen.png\" alt=\"Aufgabe-Häufigkeit-bestimmen\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "Um die Aufgabe zu lösen und für jedes Wort die Häufigkeit im Korpus zu bestimmen, müssen zwei Funktionen geschrieben werden: Eine Map- und eine Reduce-Funktion. \n",
    "\n",
    "Zunächst wird eine Map-Funktion geschrieben, die jedes Wort zusammen mit einer 1 zurückgibt. \n",
    "\n",
    "```\n",
    "map(filename, line){\n",
    "\t  for each (word in line)\n",
    "\t     emit(word, 1);\t \t \n",
    "}\n",
    "```\n",
    "\n",
    "Als nächstes wird eine Reduce-Funktion programmiert. Diese Funktion bekommt einen Key, in dem Fall word, und eine Liste an Values, hier ist es numbers, übergeben. Zu jedem Wort zählt die Reduce-Funktion die Summe an Einsen in der numbers-Liste. Am Schluss gibt die Funktion das Wort zusammen mit der Häufigkeit des Wortes aus. \n",
    "\n",
    "```\n",
    "reduce(word, numbers){\n",
    "\t  int sum = 0;\n",
    "\t  for each (value in numbers){\n",
    "\t    sum += value;\n",
    "\t  }\n",
    "\t  emit(word, sum);\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6a36f",
   "metadata": {},
   "source": [
    "### Map Reduce Illustrated (2)\n",
    "\n",
    "36min\n",
    "\n",
    "<img src=\"pictures/MapReduce-Illustrated-2.png\" alt=\"MapReduce-Illustrated-2\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a30972-f8cc-4c37-882e-ad854a77354e",
   "metadata": {},
   "source": [
    "#### Aufgabe: Bestimme Liste gemeinsamer Bekannte für jedes Personenpaar\n",
    "\n",
    "```\n",
    "map(person, friendlist){\n",
    "\t  for each (friend in friendlist)\n",
    "      if(friend < person)\n",
    "\t        emit(, friendlist);\n",
    "      else \n",
    "         emit(, friendlist);\n",
    "}\n",
    "\n",
    "reduce(, friendlists){\n",
    "   emit(, friendlist[1] ∩ friendlist[2]);\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "* 2016: 1,4 Milliarden Facebook-Nutzer, durchschnittlich 155 Freunde\n",
    "    * 979.999.999.300.000.000 Paare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ea01e-d2e6-49be-b8fc-c2b302f69117",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "**Friends lists:** <br>\n",
    "A -> B C D <br>\n",
    "B -> A C D E <br>\n",
    "C -> A B D E <br>\n",
    "D -> A B C E <br>\n",
    "E -> B C D <br>\n",
    "\n",
    "**After mapping:**\n",
    "\n",
    "(A B) -> B C D <br>\n",
    "(A C) -> B C D <br>\n",
    "(A D) -> B C D <br>\n",
    " <br>\n",
    "(A B) -> A C D E <br>\n",
    "(B C) -> A C D E <br>\n",
    "(B D) -> A C D E <br>\n",
    "(B E) -> A C D E <br>\n",
    " <br>\n",
    "(A C) -> A B D E <br>\n",
    "(B C) -> A B D E <br>\n",
    "(C D) -> A B D E <br>\n",
    "(C E) -> A B D E <br>\n",
    " <br>\n",
    "(A D) -> A B C E <br>\n",
    "(B D) -> A B C E <br>\n",
    "(C D) -> A B C E <br>\n",
    "(D E) -> A B C E <br>\n",
    " <br>\n",
    "(B E) -> B C D <br>\n",
    "(C E) -> B C D <br>\n",
    "(D E) -> B C D <br>\n",
    "\n",
    "**After shuffling:** <br>\n",
    "(A B) -> (A C D E) (B C D) <br>\n",
    "(A C) -> (A B D E) (B C D) <br>\n",
    "(A D) -> (A B C E) (B C D) <br>\n",
    "(B C) -> (A B D E) (A C D E) <br>\n",
    "(B D) -> (A B C E) (A C D E) <br>\n",
    "(B E) -> (A C D E) (B C D) <br>\n",
    "(C D) -> (A B C E) (A B D E) <br>\n",
    "(C E) -> (A B D E) (B C D) <br>\n",
    "(D E) -> (A B C E) (B C D) <br>\n",
    "\n",
    "**After reducing:** <br>\n",
    "(A B) -> (C D) <br>\n",
    "(A C) -> (B D) <br>\n",
    "(A D) -> (B C) <br>\n",
    "(B C) -> (A D E) <br>\n",
    "(B D) -> (A C E) <br>\n",
    "(B E) -> (C D) <br>\n",
    "(C D) -> (A B E) <br>\n",
    "(C E) -> (B D) <br>\n",
    "(D E) -> (B C) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae77756-18f5-4fa3-a279-da74a4804357",
   "metadata": {},
   "source": [
    "### Parallel DBMS vs. Map/Reduce\n",
    "\n",
    "<img src=\"pictures/Parallel_DBMS_vs_Map_Reduce.png\" alt=\"Parallel_DBMS_vs_Map_Reduce\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530a03a-0580-4884-b0a5-7888017a464e",
   "metadata": {},
   "source": [
    "### Relational Operators as Map/Reduce jobs\n",
    "\n",
    "* **SQL Query** \n",
    "\n",
    "```\n",
    "SELECT year, SUM(price)\n",
    "FROM   sales\n",
    "WHERE  area_code = “US”\n",
    "GROUP BY year\n",
    "```\n",
    "* **Map/Reduce job:**\n",
    "\n",
    "```\n",
    "map(key, tuple) {\n",
    "\t  int year = YEAR(tuple.date);\n",
    "\t  if (tuple.area_code = “US”)\n",
    "\t    emit(year, {‘price’ => tuple.price });\n",
    "\t}\n",
    "\n",
    "\treduce(key, tuples) {\n",
    "\t  double sum_price = 0;\n",
    "\t  foreach (tuple in tuples) {\n",
    "\t    sum_price += tuple.price;\n",
    "\t  }\n",
    "\t  emit(key, sum_price);\n",
    "\t}\n",
    "```\n",
    "\n",
    "* **Sorting with SQL Query:**\n",
    "\n",
    "```\n",
    "SELECT * \n",
    "FROM sales \n",
    "ORDER BY year\n",
    "```\n",
    "\n",
    "* **Map/Reduce job:**\n",
    "\n",
    "```\n",
    "map(key, tuple) {\n",
    "\t  emit(YEAR(tuple.date) div 10, tuple);\n",
    "\t}\n",
    "\n",
    "\treduce(key, tuples) {\n",
    "\t  emit(key, sort(tuples));\n",
    "\t}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6fc60",
   "metadata": {},
   "source": [
    "### Hadoop – A map/reduce Framework \n",
    "\n",
    "* Hadoop: Apache Top Level Project\n",
    "    * Open source\n",
    "    * Written in Java\n",
    "\n",
    "* Hadoop provides a stack of\n",
    "    * Distributed file system (HDFS) – modeled after the Google File System\n",
    "    * Map/Reduce engine\n",
    "    * Data processing languages (Pig Latin, Hive SQL)\n",
    "    * Plus very many packages\n",
    "\n",
    "* Runs on\n",
    "    * Linux, Mac OS/X, Windows, Solaris\n",
    "    * Commodity hardware\n",
    "\n",
    "<img src=\"pictures/hadoop.png\" alt=\"hadoop\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117bf92-42cb-416b-99c5-273e95a3e4c0",
   "metadata": {},
   "source": [
    "### Hadoop Distributed File System (HDFS)\n",
    "\n",
    "* Master-Slave Architecture\n",
    "    * Based on GFS architecture\n",
    "\n",
    "* HDFS Master “NameNode”\n",
    "    * Manages all file system metadata\n",
    "    * Transactions are logged, merged \n",
    "    * at startup\n",
    "    * Controls read/write access to files\n",
    "    * Manages block replication\n",
    "    * Can be replicated to avoid single-point-of-failure\n",
    "\n",
    "* HDFS Slave “DataNode”\n",
    "    * Communicates with the NameNode periodically via heartbeats\n",
    "    * Serves read/write requests from clients\n",
    "    * Performs replication tasks upon instruction by NameNode\n",
    "        * Default replication factor: 3\n",
    "\n",
    "<img src=\"pictures/HDFS.png\" alt=\"HDFS\" width=\"350\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912c687-8a9d-48d4-979a-5a0c93477725",
   "metadata": {},
   "source": [
    "### Hadoop Map/Reduce Engine\n",
    "\n",
    "Jobs are executed like a Unix pipeline: <br>\n",
    "* cat * | grep | sort | uniq -c | cat    > output <br>\n",
    "* Input | Map  | Sort & Shuffle | Reduce | Output\n",
    "\n",
    "Workflow\n",
    "1. Input phase: generates a number of FileSplits from input files (one per Map task)\n",
    "2. Map phase: executes a user function to transform input kv-pairs into a new set of kv-pairs\n",
    "3. Sort & shuffle phase: sort and distribute the kv-pairs to output nodes\n",
    "4. Reduce phase: combines all kv-pairs with the same key into new kv-pairs\n",
    "5. Output phase writes the resulting pairs to files\n",
    "\n",
    "All phases are distributed with many tasks doing the work\n",
    "* Framework handles scheduling of tasks on cluster\n",
    "* Framework handles recovery when a node fails\n",
    "\n",
    "\n",
    "* Master / Slave architecture\n",
    "\n",
    "* Map/Reduce Master: JobTracker\n",
    "    * Accepts jobs submitted by clients\n",
    "    * Assigns map and reduce tasks to TaskTrackers\n",
    "    * Monitors execution status, re-executes tasks upon failure\n",
    "\n",
    "* Map/Reduce Slave: TaskTracker\n",
    "    * Runs map / reduce tasks upon instruction from the task tracker\n",
    "    * Manage storage, sorting and transmission of intermediate output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd6295-c968-41fa-a31c-de2965916807",
   "metadata": {},
   "source": [
    "### Hadoop Map/Reduce Engine\n",
    "\n",
    "<img src=\"pictures/Hadoop-Map_Reduce-engine.png\" alt=\"Hadoop-Map_Reduce-engine\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740d0bd-f118-4587-8e7c-4d8c9b240344",
   "metadata": {},
   "source": [
    "### Fehlertoleranz\n",
    "\n",
    "* Viele Daten  …  in langen Prozessen  …  auf vielen Maschinen\n",
    "\n",
    "\n",
    "* Verteiltes Dateisystem (DFS / HDFS)\n",
    "    * Speichert verteilt und fehlertolerant durch Replikation\n",
    "    * Input ist redundant verfügbar\n",
    "* Speicherung von Zwischenergebnissen ins DFS\n",
    "    * Aufwändig, aber Fehlererholung im laufenden Prozess einfach und schnell\n",
    "* Abstürze\n",
    "    * Werden erkannt falls periodisches Signal ausfällt (heartbeat)\n",
    "    * Neustart des Mappers oder Reducers\n",
    "        * Auf anderer Maschine, mit Replikat des ursprünglichen Inputs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4be8b-2364-4f53-b34e-94a0bc9f7a85",
   "metadata": {},
   "source": [
    "### When to use Hadoop?\n",
    "\n",
    "* Good fit for batch processing applications that need to touch all your data:\n",
    "    * Data mining\n",
    "    * Model tuning\n",
    "    * Text processing\n",
    "\n",
    "* Bad fit for applications that need to find/edit one particular record\n",
    "    * High overhead\n",
    "    * High latency\n",
    "\n",
    "* Bad fit for applications that need to communicate between processes\n",
    "    * Hadoop is oriented around independent units of work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa99f30",
   "metadata": {},
   "source": [
    "### In der Praxis: Komplexe (optimierte) MapReduce Workflows\n",
    "* Neue (relationale) Operatoren\n",
    "    * Join, Cross, Union, …\n",
    "* Planoptimierung & Re-optimierung\n",
    "* Scheduling & Lastbalancierung\n",
    "* Cross-Plattform Ausführung\n",
    "\n",
    "<img src=\"pictures/Komplexe-optimierte-MapReduce-workflows.png\" alt=\"Komplexe-optimierte-MapReduce-workflows\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74280512",
   "metadata": {},
   "source": [
    "### Hadoop vs. Parallel DBMS\n",
    "\n",
    "* 2012\n",
    "  \n",
    "<img src=\"pictures/Hadoop_vs_Parallel_DBMS.png\" alt=\"Hadoop_vs_Parallel_DBMS\" width=\"500\" style=\"background-color: white;\"/>\n",
    "  \n",
    "* 2014\n",
    "  \n",
    "<img src=\"pictures/Hadoop_vs_Parallel_DBMS_2.png\" alt=\"Hadoop_vs_Parallel_DBMS_2\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "\n",
    "<img src=\"pictures/Hadoop-vs-Parallel-DBMS.png\" alt=\"Hadoop-vs-Parallel-DBMS\" width=\"300\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093d5f2",
   "metadata": {},
   "source": [
    "### In der Praxis: Viele Bibliotheken\n",
    "\n",
    "* Startpunkt: Hadoop\n",
    "    * Java, open-source\n",
    "    * Basis-Bibliotheken\t\t\t\tCommon, MapReduce\n",
    "    * Verteiltes Dateisystem\t\t\tHDFS\n",
    "    * Scheduling, Monitoring\t\t\tYarn\n",
    "\n",
    "* Erweiterungen\n",
    "    * Service- und Cluster-Verwaltung\t\tZooKeeper\n",
    "    * Datenspeicher\t\t\t\tHBase\n",
    "    * Datenbank und Anfragesprachen\t\tPig, Hive, Phoenix\n",
    "    * Bibliotheken für komplexe Verfahren\t\tMahout, Giraph, Solr\n",
    "    * Datenstromverarbeitung\t\t\tKafka, Flink, Spark\n",
    "    * …\n",
    "\n",
    "\n",
    "<img src=\"pictures/Viele-Bibliotheken.png\" alt=\"Viele-Bibliotheken\" width=\"400\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e7461-dd8b-4016-b578-78899686aeda",
   "metadata": {},
   "source": [
    "## Outlook: What about updates/transactions?\n",
    "\n",
    "* OLTP style applications that are beyond relational databases' capabilities exist as well.\n",
    "* Some applications still require fast and efficient lookup and retrieval of small amounts of data\n",
    "    * Web index access, mail accounts, warehouse updates for resellers \n",
    "    * Addressed by Key/Value pair based storage systems (e.g. Google BigTable and Megastore)\n",
    "    * Can access the data only through a key\n",
    "    * Can apply only an additional filter on columns and timestamps\n",
    "* Some applications still need updates and certain guarantees about them\n",
    "    * No hard transactions, especially no multi record transactions.\n",
    "    * Eventual consistency model\n",
    "* See next set of slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddc406-ec67-4812-9780-cfc72bdda3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}