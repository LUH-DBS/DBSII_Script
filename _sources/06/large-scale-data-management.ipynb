{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aba103-11e1-43f4-98a3-fa756243c505",
   "metadata": {},
   "source": [
    "# Large Scale Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981228f1-614d-415e-bb54-112dd93352e0",
   "metadata": {},
   "source": [
    "In diesem Kapitel geht es insbesondere darum, die Verfahren und Datenbankoperationen, die wir bisher kennengelernt haben, hinsichtlich paralleler Verarbeitung zu betrachten und auch die Kostenelemente, die dann eine Rolle spielen. \n",
    "\n",
    "Beim Large Scale Data Management geht es um sehr große Datenmengen. Da reicht es dann nicht mehr, nur eine Datenbank zu haben, man muss nun auch über die Verteilung, Server und Nebenläufigkeiten nachdenken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac2534-e148-4a3d-9ea9-a5d8768021d2",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"pictures/Large-scale-Data-Management.png\" alt=\"Large-scale-Data-Management\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Zur Wiederholung einmal die Frage: Was ist Big Data? Big Data wird anhand von Dimensionen spezifiziert- die sogenannten V's: **Volume** (Menge von Daten), **Velocity** (Schnelligkeit der Datenverarbeitung), **Variety** (Heterogenität der Daten), **Verocity** (Daten, bei denen die Korrektheit ungewiss ist) und **Value** (die Wertigkeit der Daten).\n",
    "\n",
    "Nun gibt es Big Data in zwei Varianten - **Operational** und **Analytic**. In der ersten Variante geht es um operationelle Sachen, also dem Transaktionsmanagement. In der zweiten Variante geht es darum, Daten zu analysieren, Insights aus Daten herzustellen und neue Erkenntnisse zu gewinnen.\n",
    "\n",
    "Zur Verdeutlichung, über was für Datenmengen wir bei Big Data reden:\n",
    "\n",
    "Google ist ein klassisches Beispiel für ein Datenproduzierendes und -verwaltendes Unternehmen. Dort werden jeden Tag 20 PB an Daten verarbeitet. Das sind Billionen von Zeilen, Tausende/Millionen Spalten und Tabellen, aber auch strukturierte Daten wie Text, Bilder und Videos. Würde man versuchen, diese 20 PB mit 50 MB/s zu lesen, würde das 12 Jahre dauern. Aus diesem Grund werden die Daten partioniert und verteilt verarbeitet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b4e51",
   "metadata": {},
   "source": [
    "## Key enabler: Virtulization\n",
    "\n",
    "Die beiden Varianten Operational und Analytic lassen sich mit der Virtualization managen. Hierbei versucht man entweder ein logisches System auf viele physische Systeme (Load Balancing) oder andersherum mehrere logische Systeme auf ein physisches System abzubilden (Multy-Tenancy).\n",
    "\n",
    "\n",
    "<img src=\"pictures/Virtualization.png\" alt=\"Virtualization\" width=\"500\" style=\"background-color: white;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b2968-007a-405a-b4d5-3e4d36a6405e",
   "metadata": {},
   "source": [
    "## Parallel Data Processing\n",
    "\n",
    "<img src=\"pictures/Overview_3.png\" alt=\"Overview_3\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3562a51-ec56-4ad4-8501-03942de6c2de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Was bisher geschah: Serielle Verarbeitung/Single Threaded\n",
    "\n",
    "Bisher haben wir immer von einem Computer mit mehreren Festplatten gesprochen und damit auch ein wenig über parallele Plattenzugriffe. Diese hatten insbesondere auch immer nur einen Kern. Das heißt, bei jeder Operation wurden die Blöcke nacheinander durch nur einen Kern abgearbeitet. Außerdem spielten auch Synchronisation und Kommunikation keine Rolle, da Anfragen in nur einem Thread bearbeitet wurden. Dies wollen wir nun erweitern.\n",
    "\n",
    "<img src=\"pictures/serial-single-threaded.png\" alt=\"serial-single-threaded\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Was wir verschwiegen haben...\n",
    "\n",
    "Das Datenvolumen wächst stetig. Data Warehouses mit 1 EB sind nicht untypisch. Manche Organisationen produzieren täglich mehr als 1 PB an neuen Daten. Das entspricht 1.000.000.000.000.000 Byte (1 quadrillion).\n",
    "Manche Systeme, wie beispielsweise Finanzinstitute, Onlineshops und soziale Netzwerke, haben einen sehr hohen Durchsatz (throughput) von Transaktionen. \n",
    "Deshalb ist es wichtig zu überlegen, wie die Zugriffe über die Netzwerke verteilt werden. \n",
    "Auch Analyseanfragen werden immer komplexer. Eine statistische Mustererkennung ist teuer und über die Daten muss mehrfach iteriert werden. Da reicht eine Single-CPU- oder Single-Node-Architektur nicht mehr aus und auch Moore's Law ist hier nicht mehr anwendbar. Die Lösung: **Parallele Datenverarbeitung**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3e1b5-0aa3-417a-b060-6b08e3a1b6cc",
   "metadata": {},
   "source": [
    "### Prominent user of parallel data processing:\n",
    "\n",
    "* Big Tech (Google, Facebook)\n",
    "    * Estimated 450,000 low-cost commodity servers in 2006\n",
    "    * In 2005 Google indexed 8 billion web pages\n",
    "    * Over 200 GFS clusters at Google. 1 cluster: 1,000 – 5,000 machines.\n",
    "    * Pools of tens of thousands of machines retrieve data from GFS clusters that run as large as 5 peta-bytes of storage.\n",
    "    * Aggregate read/write throughput up to 40 gigabytes/second across the cluster.\n",
    "    * 6,000 MapReduce applications\n",
    "\n",
    "* Manufacturing (Volkswagen)\n",
    "    * the average connected vehicle will generate 280 petabytes of data annually\n",
    "    * with four terabytes of data being generated in a day at the very least.\n",
    "    * The research also states that around 470 million connected vehicles will be deployed by 2025\n",
    " \n",
    "* Gene Sequencing\n",
    "  \n",
    "* Sensor data processing\n",
    "\n",
    "Amazon Aurora – On Prime Day, 5,326 database instances running the PostgreSQL-compatible and MySQL-compatible editions of Amazon Aurora processed 288 billion transactions, stored 1,849 terabytes of data, and transferred 749 terabytes of data.\n",
    "\n",
    "Amazon DynamoDB – DynamoDB powers multiple high-traffic Amazon properties and systems including Alexa, the Amazon sites, and all Amazon fulfillment centers. Over the course of Prime Day, these sources made trillions of calls to the DynamoDB API. DynamoDB maintained high availability while delivering single-digit millisecond responses and peaking at 105.2 million requests per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae1665",
   "metadata": {},
   "source": [
    "### Basics of Parallel Processing\n",
    "\n",
    "* Parallel Speedup\n",
    "    * Ahmdal‘s Law\n",
    "* Levels of Parallelism\n",
    "    * Instruction-level, data-level, task-level\n",
    "* Modes of Query Parallelism\n",
    "    * Inter-Query\n",
    "        * Multi-user ability\n",
    "    * Intra-Query\n",
    "        * Pipeline (Inter Operator) / Data (Intra Operator)\n",
    "\n",
    "\n",
    "### Parallel Speedup – Amdahl‘s law\n",
    "\n",
    "* Speedup: Sp = T1/Tp\n",
    "    * T1: Sequential runtime (1 processor)\n",
    "    * Tp: Parallel runtime (p processors)\n",
    "* Amdahl‘s law: Maximal speedup is determined by non-parallelizable part of program\n",
    "    * where f is parallelizable fraction of program\n",
    "    * Ideal speedup: S = p for f = 1 (linear speedup)\n",
    "    * Usually f < 1 => S is bound by constant\n",
    "        * For f = 0,9 and 10 / 20 servers: <br>\n",
    "  and \n",
    "        * For p ∞,  10\n",
    "    * Fixed problems can only be parallelized to a certain degree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb1bca",
   "metadata": {},
   "source": [
    "### Parallel Speedup\n",
    "\n",
    "<img src=\"pictures/Parallel-Speedup.png\" alt=\"Parallel-Speedup\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Levels of Parallelism on Hardware\n",
    "\n",
    "* Instruction-level Parallelism (Prozessoranweisungen)\n",
    "    * Single instructions are automatically processed in parallel\n",
    "    * Example: Modern CPUs with multiple pipelines and instruction units.\n",
    "\n",
    "* Data Parallelism (Daten)\n",
    "    * Different data can be processed independently\n",
    "    * Each processor executes the same operations on its share of the input data.\n",
    "    * Example: Distributing loop iterations over multiple processors\n",
    "    * Example: GPU processing\n",
    "\n",
    "* Task Parallelism (Aufgaben)\n",
    "    * Different tasks are distributed among the processors/nodes\n",
    "    * Each processor executes a different thread/process.\n",
    "    * Example: Threaded programs\n",
    "\n",
    "### Modes of Query Parallelism\n",
    "\n",
    "* Inter-Query Parallelism (multiple concurrent queries)\n",
    "    * Necessary for efficient resource utilization: While one query waits, e.g., for I/O, another one executes\n",
    "    * Requires concurrency control (locking mechanisms) to guarantee transactional properties (the \"I\" in ACID)\n",
    "    * Important for highly transactional scenarios (OLTP)\n",
    "\n",
    "* Intra-Query Parallelism (parallel processing of a single query)\n",
    "    * I/O Parallelism: Concurrent reading from multiple disks\n",
    "        * Hidden: Hardware RAID\n",
    "        * Transparent: Spanned tablespaces / partitioning\n",
    "    * Intra-Operator Parallelism: Multiple threads work on the same operator. Example: Parallel Sort\n",
    "    * Inter-Operator Parallelism: Multiple parts of the plan run in parallel (pipeline)\n",
    "    * Important for complex analytical tasks (OLAP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03fbf6",
   "metadata": {},
   "source": [
    "### Pipeline Parallelism\n",
    "\n",
    "<img src=\"pictures/Pipeline-Parallelism.png\" alt=\"Pipeline-Parallelism\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "\n",
    "* Pipeline parallelism\n",
    "    * aka inter-operator parallelism: parallelism is between the operators\n",
    "\n",
    "* In addition: Execute multiple pipelines simultaneously\n",
    "    * Limited in its applicability, only if multiple pipelines are present and not dependent on each other\n",
    "\n",
    "* Problem:\n",
    "    * High synchronization overhead\n",
    "    * Mostly limited to lower degree of parallelism (not too many pipelines per query)\n",
    "    * Only suited for shared-memory architectures\n",
    "\n",
    "### Data Parallelism\n",
    "\n",
    "* Pipeline parallelism is not always applicable \n",
    " data parallelism\n",
    "* Divide data into several sub-sets\n",
    "    * Most operations do not need a complete view of the data\n",
    "        * E.g., selection looks only at a single tuple at a time\n",
    "    * Subsets can be are processed independently and hence in parallel.\n",
    "* Maximum degree of parallelism as high as the number of possible subsets\n",
    "    * For selection: As high as the number of tuples\n",
    "\n",
    "* Some operations possibly need a view of larger portions of the data\n",
    "    * E.g., grouping/aggregation operation needs all tuples with the same grouping key\n",
    "    * Are they all in the same subset? Can we guarantee that?\n",
    "    * Different operators need different sets\n",
    " \n",
    "### Basics of Parallel Query Processing\n",
    "\n",
    "* Levels of Resource Sharing\n",
    "    * Shared-Memory, Shared-Disk, Shared-Nothing\n",
    "\n",
    "* Data Partitioning\n",
    "    * Round-robin, Hash, Range\n",
    "\n",
    "* Parallel Operators and Costs\n",
    "    * Tuple-at-a-time (e.g. Selection)\n",
    "    * Sorting\n",
    "    * Projection, Grouping, Aggregation\n",
    "    * Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e578380",
   "metadata": {},
   "source": [
    "### Parallel Architectures – Shared Memory\n",
    "\n",
    "* Several CPUs share a single memory and disk (array)\n",
    "* Communication over a single common bus\n",
    "* In practice: Some private memory per processor\n",
    "    * NUMA (non-uniform memory access)\n",
    "\n",
    "<img src=\"pictures/shared-memory.png\" alt=\"shared-memory\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f7b36",
   "metadata": {},
   "source": [
    "### Parallel Architectures – Shared Disk\n",
    "\n",
    "* Several nodes with multiple CPUs, each node has its private memory\n",
    "* Single attached disk (array): Often NAS, SAN, etc…\n",
    "\n",
    "\n",
    "<img src=\"pictures/shared-disk.png\" alt=\"shared-disk\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26d30a",
   "metadata": {},
   "source": [
    "### Parallel Architectures – Shared Nothing\n",
    "\n",
    "* Each node has it own set of CPUs, memory and disks attached\n",
    "* Most commonly use architecture for large-scale data management\n",
    "* Data needs to be partitioned over the nodes\n",
    "* Data is exchanged through direct node-to-node communication\n",
    "    * Messages with significant overhead\n",
    "\n",
    "\n",
    "<img src=\"pictures/shared-nothing.png\" alt=\"shared-nothing\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Data Partitioning\n",
    "\n",
    "* Partitioning the data means creating a set of possibly disjoint\n",
    "sub-sets\n",
    "    * Example: Sales data, every year gets its own partition\n",
    "* For shared-nothing, data must be partitioned across nodes\n",
    "    * If it were replicated, it would effectively become a shared-disk with the local disks acting like a cache (must be kept coherent during updates!)\n",
    "\n",
    "* Partitioning with certain characteristics is beneficial\n",
    "    * Some queries can be limited to operate on certain parts only\n",
    "        * If provable that all relevant data (passing the predicates) is in that partition\n",
    "    * Database administration: Partition can be simply dropped as a whole when it is no longer needed (e.g., discard old sales)\n",
    "\n",
    "### Data Partitioning Strategies\n",
    "\n",
    "* Round robin\n",
    "    * Each partition gets a tuple in a round\n",
    "    * All sets are guaranteed to consist of an equal amount of tuples\n",
    "    * No apparent relationship between tuples in one partition\n",
    "* Hash Partitioned\n",
    "    * Define a set of partitioning columns.\n",
    "    * Generate a hash value over those columns to decide the target set.\n",
    "    * All tuples with equal values in the partitioning columns are in the same partition.\n",
    "* Range Partitioned\n",
    "    * Define a set of partitioning columns\n",
    "    * Split the domain of those columns into ranges\n",
    "    * Range determines the target set. All tuples in one partition are in the same range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225daf3a",
   "metadata": {},
   "source": [
    "### Data Parallelism: Example\n",
    "\n",
    "* Client sends a SQL query to one of the cluster nodes\n",
    "    * Node becomes the\n",
    "\"coordinator\"\n",
    "\n",
    "* Coordinator compiles query\n",
    "    * Parsing, checking, optimization\n",
    "    * Parallelization\n",
    "\n",
    "* Sends partial plans to the other cluster nodes\n",
    "    * Coordinator also executes the partial plan on his part of the data\n",
    "* Coordinator collects partial results and finalizes them \n",
    "\n",
    "\n",
    "<img src=\"pictures/Data-Parallelism-example.png\" alt=\"Data-Parallelism-example\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3936e",
   "metadata": {},
   "source": [
    "### Data Parallelism – Example\n",
    "\n",
    "* For shared-nothing & shared-disk\n",
    "    * Multiple instances of a sub-plan are executed on different computers\n",
    "    * The instances operate on different splits or partitions of the data\n",
    "    * At some point, results from the sub-plans are collected\n",
    "    * For more complex queries, results are not collected but re-distributed, for further parallel processing\n",
    "\n",
    "\n",
    "<img src=\"pictures/Data-Parallelism-example_2.png\" alt=\"Data-Parallelism-example_2\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Parallel Operators\n",
    "\n",
    "* Ideally: Operate as much as possible on individual partitions of the data\n",
    "    * Ship operation to data\n",
    "* Easy for simple “per-tuple” operators\n",
    "    * Scan, index-scan, selection\n",
    "* Problem: Some operators need the whole picture (blocking operators)\n",
    "    * Sort and aggregations can only be preprocessed in parallel and need a final step on a single node.\n",
    "        * Unless they occur in a correlated subplan known to contain only tuples from one partition.\n",
    "    * E.g., joins need matching tuples. \n",
    "        * Either organize the inputs accordingly,\n",
    "        * or join at the coordinator after collection of partial results (not parallel any more!)\n",
    "\n",
    "### Notations and Assumptions\n",
    "\n",
    "* S\tRelation S\n",
    "* S[i,h]\tPartition i of relation S according to partitioning scheme h\n",
    "* B(S)\tNumber of blocks of relation S\n",
    "* p\tNumber of nodes\n",
    "\n",
    "* Assume a shared-nothing architecture\n",
    "    * Most commercial database vendors use shared-nothing approaches.\n",
    "* Network transfer is at least as expensive as disk access\n",
    "    * In some cost models network is still far more expensive.\n",
    "    * Today network bandwidth ≈ disk bandwidth\n",
    "    * But: Network is shared\n",
    "        * Switches and routers have a throughput limit\n",
    "* Assume partitioning schemes (hash/range) produce partitions of roughly equal size.\n",
    "* Assume S[i,h] > M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94549d-f60a-4119-9fc2-8bf22cb0d946",
   "metadata": {},
   "source": [
    "### Parallel Selection / Projection\n",
    "\n",
    "* Selection and projection can be parallelized very efficiently\n",
    "    * “Embarrassingly parallel” problem\n",
    "\n",
    "* Each node performs the selection on its existing local partition.\n",
    "    * Selection needs no context\n",
    "    * Data can be partitioned in an arbitrary way\n",
    "\n",
    "* Partial results are unioned afterwards.\n",
    "\n",
    "* Cost:\tB(S)/p\t+ transfer (depends on selectivity)\n",
    "\n",
    "### Parallel Grouping & Aggregation\n",
    "\n",
    "* Two phases\n",
    "    1. Local grouping & aggregation to each partition\n",
    "    2. Merge results\n",
    "\n",
    "* Cost: 3 B(S)/p local algorithm + transfer of (small) results + (fast) merge\n",
    "\n",
    "* Works only for associative aggregation functions\n",
    "    * MIN, MAX, SUM, COUNT\n",
    "    * AVG: Use SUM / COUNT\n",
    "\n",
    "* To avoid possibly expensive second phase:\n",
    "    * Use hashing function on group-columns to re-partition relation onto nodes\n",
    "    * Or: Parallelization of merge phase\n",
    " \n",
    "### Parallel Sorting\n",
    "\n",
    "* Range partitioned sort: partition by range, then sort\n",
    "    * Range-partition the relation according to the sort column(s)\n",
    "    * Sort the single partitions locally (e.g., by TPMMS)\n",
    "    * Cost: B(S) partitioning + B(S) transfer + 3 B(S)/p local sorting\n",
    "    * Problem: Find a uniform range partitioning scheme\n",
    "        * Partitions of same/similar size\n",
    "\n",
    "* Parallel external sort-merge: sort locally, then merge\n",
    "    * Reuse an existing data partitioning\n",
    "    * Partitions are sorted locally (e.g. by TPMMS)\n",
    "    * Sorted partitions need to be merged\n",
    "    * Pair-wise with cost: 3 B(S)/p local sorting + log2(p)*B(S)/2 transfer + log2(p)*B(S) local merge\n",
    "    * Or multi-way merge\n",
    " \n",
    "### Symmetric Fragment-and-Replicate Join\n",
    "\n",
    "* Joining two relations R and S requires looking at every tuple of the Cartesian product. \n",
    "* Parallel databases need to combine every partition of R with every partition of S.\n",
    "* Symmetric Fragment-and-Replicate (or Broadcast) Join:\n",
    "* Given  nodes\n",
    "* Fragment R into m and S into n partitions\n",
    "* Replicate the fragments onto the nodes\n",
    "* Each fragment of R is replicated n times\n",
    "* Each fragment of S is replicated m times\n",
    "* Each node locally joins exactly one fragment pair of R and S.\n",
    "* Cost: \t\tfragmentation cost <br>\n",
    "        \t\ttransfer cost <br>\n",
    "        \t???\t\tlocal join cost\n",
    "* Only parallel join type that works for all join predicates (Theta-Join)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d950b",
   "metadata": {},
   "source": [
    "### Symmetric Fragment-and-Replicate Join\n",
    "\n",
    "<img src=\"pictures/Symmetric-Fragment-and-Replicate-Join.png\" alt=\"Symmetric-Fragment-and-Replicate-Join\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f361c36",
   "metadata": {},
   "source": [
    "### Asymmetric Fragment-and-Replicate Join\n",
    "\n",
    "* We can do better, if relation S is much smaller than R.\n",
    "\n",
    "* Idea: Reuse the existing partitioning of R and replicate the whole relation S to each node.\n",
    "\n",
    "* Cost:\tp * B(S)\ttransport <br>\n",
    "\t???\t\tlocal join\n",
    "\n",
    "* Asymmetric Fragment-and-replicate join is a special case of the Symmetric Algorithm with m=p and n=1.\n",
    "\n",
    "<img src=\"pictures/Asymmetric-Fragment-and-Replicate-Join.png\" alt=\"Asymmetric-Fragment-and-Replicate-Join\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4210fa-9e8b-4733-bb8c-449c120ec03b",
   "metadata": {},
   "source": [
    "### Parallel Equi-Joins (I)\n",
    "\n",
    "* A special class of joins that are more suited for parallelization are natural- and equi-joins.\n",
    "\n",
    "* Idea: Partition relations R and S using the same partition scheme over the join key.\n",
    "    * All tuples of R and S with the same join key end up at the same node.\n",
    "    * No further broadcast is needed, all joins can be performed locally.\n",
    "\n",
    "* Actual implementation depends on how the relations are partitioned:\n",
    "    * Co-Located Join\n",
    "    * Directed Join\n",
    "    * Re-Partitioning Join\n",
    " \n",
    "### Parallel Equi-Joins: Three cases\n",
    "\n",
    "1. Both R and S are already partitioned over the join key with the same partitioning scheme\n",
    "    - „Co-Located Join“\n",
    "    - No re-partitioning is needed!\n",
    "    - Cost: \t???\t\tLocal join cost\n",
    "2. Only one relation is partitioned over the join key:\n",
    "    - „Directed Join“\n",
    "    - Re-Partition the other relation with same partitioning scheme.\n",
    "    - Cost (assuming R is already partitioned):\t\n",
    "    \t- B(S) \t\tpartitioning\n",
    "        - B(S)\t\ttransfer \n",
    "    \t- ???\t\tLocal join cost\n",
    "3. No relation is partitioned over the join key: \t\n",
    "    - „Repartition Join“\n",
    "    - Re-Partition both relations over the join key\n",
    "    - Cost:\tB(S)+B(R) \tpartitioning\n",
    "        - B(S)+B(R)\ttransfer\n",
    "        - ???\t\tLocal join cost\n",
    "     \n",
    "### Limits in Parallel Databases\n",
    "\n",
    "* Database clusters tend to scale until 64 or 128 nodes\n",
    "    * Afterwards the speedup curve flattens\n",
    "    * Communication overhead eats speedup\n",
    "    * Hard limit example: 1000 nodes for DB2 (2010)\n",
    "\n",
    "* Shared Disk: Does not scale infinitely; bus and synchronization become overhead\n",
    "    * For updates: Cache Coherency Problem\n",
    "    * For reads: I/O Bandwidth Limits\n",
    "\n",
    "* Shared Nothing: Cannot compensate loss of a node easily\n",
    "    * In large clusters, failures and outages are most common.\n",
    "    * Loss of a node means loss of data!\n",
    "    * Unless: Data is replicated. But: Replicated data must be kept consistent! Has a high overhead…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0081069-b230-4718-a87c-41442d215f42",
   "metadata": {},
   "source": [
    "### Where traditional databases are unsuitable\n",
    "\n",
    "* Analysis over raw (unstructured) data\n",
    "    * Text processing\n",
    "    * In general: If relation schema does not fit\n",
    "* Where cost-effective scalability is required\n",
    "    * Use commodity hardware\n",
    "    * Adaptive cluster size (horizontal scaling)\n",
    "    * Incremental growth: add computers without expensive reorganization that halts the system\n",
    "* In unreliable (= large) infrastructures\n",
    "    * Must be able to deal with failures – hardware, software, network\n",
    "        * Failure is expected rather than the exception\n",
    "    * Transparent to applications\n",
    "        * Too expensive to build reliability into each application\n",
    "     \n",
    "### Example Use Case: Web Index Creation\n",
    "\n",
    "* A Search Engine scenario:\n",
    "    * Have crawled the internet and stored the relevant documents\n",
    "    * Documents contain words \t(Doc-URL, [list of words])\n",
    "    * Documents contain links   \t(Doc-URL, [Target-URLs])\n",
    "* Need to build a search index\n",
    "    * Invert the files \t\t(word, [list of URLs])\n",
    "    * Compute a ranking that requires an inverted graph: \n",
    "\t\t\t\t(Doc-URL, [URLs-pointing-to-it])\n",
    "* Obvious reasons against relational databases\n",
    "    * Relational schema is unsuitable/”unnatural”\n",
    "    * Importing the documents, converting them to the storage format is expensive\n",
    "* A mismatch between what databases were designed for and what is really needed:\n",
    "    * Databases come originally from transactional processing. They give hard guarantees about absolute consistencies in the case of concurrent updates.\n",
    "    * Analytics are added on top of that\n",
    "    * The documents are never updated, they are read only.\n",
    "    * Perfect transactional consistency is not always necessary\n",
    " \n",
    "### An ongoing Re-Design…\n",
    "\n",
    "* Driven by companies like Google, Facebook, Yahoo, Apple, Microsoft\n",
    "* Use heavily distributed system\n",
    "    * Google used 450,000 low-cost commodity servers in 2006\n",
    "in cluster of 1000 – 5000 nodes\n",
    "* Redesign infrastructure and architectures completely with the key goal to be\n",
    "    * Highly scalable\n",
    "    * Tolerant of failures\n",
    "* Stay generic and schema free in the data model\n",
    "* Start with: Data Storage\n",
    "* Next Step: Distributed Analysis\n",
    "\n",
    "### Storage Requirements\n",
    "\n",
    "* Extremely large files: Terabytes to Petabytes\n",
    "* High availability: Data must be kept replicated\n",
    "* High throughput\n",
    "    * Read/write operations must not go through other servers\n",
    "* No single point of failure\n",
    "    * Any master must be kept redundantly\n",
    "* Many different distributed file systems exist.\n",
    "    * Different goals: transparency, updateability, archiving, etc…\n",
    "* Google Filesystem (GFS)\n",
    "    * Widely used reference architecture for high-throughput and high-availability DFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4035b",
   "metadata": {},
   "source": [
    "### The Storage Model – Distributed File System\n",
    "\n",
    "* The file system\n",
    "    * Distributed across many nodes (DataNodes)\n",
    "    * Provides a single namespace for the entire cluster\n",
    "    * Metadata is managed on a dedicated node (NameNode)\n",
    "    * Write-once-read-many access model\n",
    "* Files are split into blocks\n",
    "    * Typically 128 MB block size\n",
    "    * Each block is replicated on multiple data nodes\n",
    "* The client\n",
    "    * can determine the location of blocks\n",
    "    * can access data directly from the DataNode\n",
    "    * over the network\n",
    "* Problem: bandwidth to data\n",
    "    * Scanning the data from remote storage is expensive (50MB/s remote access vs. 150-200MB/s local access)\n",
    "    * Moving computation is more efficient than moving data\n",
    "    * Map/Reduce framework tries to perform computations close to the data\n",
    "    * Nodes have two purposes: data storage and computation\n",
    "\n",
    "<img src=\"pictures/The-Storage-Model.png\" alt=\"The-Storage-Model\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Retrieving and Analyzing Data\n",
    "\n",
    "* Data is stored as custom records in files\n",
    "    * Most generic data model that is possible\n",
    "    * Key/value model\n",
    "* Records are read and written with data model specific (de)serializers\n",
    "* Analysis or transformation tasks must be written directly as a program\n",
    "    * Not possible to generate it from a higher level statement\n",
    "    * Like a query-plan that is automatically generated from SQL\n",
    "* Programs must be parallel, highly scalable, fault tolerant\n",
    "    * Extremely hard to program\n",
    "    * Need a programming model and framework that takes care of that\n",
    "    * The map/reduce model has been suggested and successfully adapted on a broad scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62756e90",
   "metadata": {},
   "source": [
    "### Skalierungsmuster\n",
    "\n",
    "* Phase 0: Daten verteilen (split)\n",
    "* Phase 1: Berechnungen auf Teilmengen der Daten (map)\n",
    "* Phase 2: Zusammenführung der Teilmengen (reduce)\n",
    "    * Gemeinsame Betrachtung zusammengehöriger Daten\n",
    "\n",
    "* Beispiel: **Two-Phase-Multiway-Mergesort** (TPMMS)\n",
    "    * Phase 1: Sortierung von Teilen der Daten\n",
    "    * Phase 2: Sortierter Teillisten zusammenführen\n",
    "* Beispiel: **Datenanalyse**\n",
    "    * Phase 1: Gruppierung\n",
    "    * Phase 2: Aggregation\n",
    "* Beispiel: **Index bauen**\n",
    "    * Phase 1: Teilmengen indizieren\n",
    "    * Phase 2: Indizes zusammenführen\n",
    "\n",
    "<img src=\"pictures/Skalierungsmuster.png\" alt=\"Skalierungsmuster\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8c619-36ea-4038-8bbb-b4c3183d7b28",
   "metadata": {},
   "source": [
    "## Map Reduce & Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247aa192",
   "metadata": {},
   "source": [
    "### “MapReduce is a programming model and an associated implementation for processing and generating large data sets.”\n",
    "\n",
    "<img src=\"pictures/MapReduce-Simplified-data-processing-on-large-clusters.png\" alt=\"MapReduce-Simplified-data-processing-on-large-clusters\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### What is Map/Reduce?\n",
    "\n",
    "* Programming model\n",
    "    * Borrows concepts from functional programming\n",
    "    * Suited for parallel execution\n",
    "        * Automatic parallelization & distribution of data and computational logic\n",
    "    * Clean abstraction for programmers\n",
    "* Functional programming influences\n",
    "    * Treats computation as the evaluation of mathematical functions\n",
    "    * No changes of states (no side effects)\n",
    "    * Output value of a function depends only on its arguments\n",
    "* Map and Reduce are higher-order functions (2nd order)\n",
    "    * Take user-defined functions as argument\n",
    "    * Return a function as result\n",
    "    * User implements the two functions\n",
    "\n",
    "### Grundbausteine\n",
    "\n",
    "* Datenmodel\n",
    "    * Schlüssel/Wert-Paare  \n",
    "        * “key/value pairs”\n",
    "        * Z.B. (int, string), oder(string, [string]), …\n",
    "\n",
    "* MapReduce Programm\n",
    "    * Input: Liste an Schlüssel/Wert-Paare \n",
    "    * Output: Liste an Werten\n",
    "* Zwei Herausforderungen\n",
    "    * Entwurf der Funktionen\n",
    "    * Verteilte, fehlertolerante und effiziente Ausführung des Programms\n",
    " \n",
    "* Nutzer definieren zwei Funktionen\n",
    "    * Map:\t\n",
    "        * Oft nur ein Paar \n",
    "    * Reduce:\t\n",
    "        * Meist nur ein Wert \n",
    "        * Meist auch  im Output, dadurch Verkettung von MapReduce-Schritten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8244244",
   "metadata": {},
   "source": [
    "### MapReduce workflow\n",
    "\n",
    "<img src=\"pictures/MapReduce-workflow.png\" alt=\"MapReduce-workflow\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ab009",
   "metadata": {},
   "source": [
    "#### Aufgabe: Bestimme für jedes Wort dessen Häufigkeit im Korpus\n",
    "\n",
    "```\n",
    "map(filename, line){\n",
    "\t  for each (word in line)\n",
    "\t     emit(word, 1);\t \t \n",
    "}\n",
    "\n",
    "reduce(word, numbers){\n",
    "\t  int sum = 0;\n",
    "\t  for each (value in numbers){\n",
    "\t    sum += value;\n",
    "\t  }\n",
    "\t  emit(word, sum);\n",
    "}\n",
    "\n",
    "```\n",
    "<img src=\"pictures/Aufgabe-Häufigkeit-bestimmen.png\" alt=\"Aufgabe-Häufigkeit-bestimmen\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6a36f",
   "metadata": {},
   "source": [
    "### Map Reduce Illustrated (2)\n",
    "\n",
    "<img src=\"pictures/MapReduce-Illustrated-2.png\" alt=\"MapReduce-Illustrated-2\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d802f",
   "metadata": {},
   "source": [
    "#### Aufgabe: Bestimme Liste gemeinsamer Bekannte für jedes Personenpaar\n",
    "\n",
    "```\n",
    "map(person, friendlist){\n",
    "\t  for each (friend in friendlist)\n",
    "      if(friend < person)\n",
    "\t        emit(, friendlist);\n",
    "      else \n",
    "         emit(, friendlist);\n",
    "}\n",
    "\n",
    "reduce(, friendlists){\n",
    "   emit(, friendlist[1] ∩ friendlist[2]);\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "* 2016: 1,4 Milliarden Facebook-Nutzer, durchschnittlich 155 Freunde\n",
    "    * 979.999.999.300.000.000 Paare\n",
    " \n",
    "\n",
    "<img src=\"pictures/Aufgabe-gemeinsame-Bekannte.png\" alt=\"Aufgabe-gemeinsame-Bekannte\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "**Friends lists:** <br>\n",
    "A -> B C D <br>\n",
    "B -> A C D E <br>\n",
    "C -> A B D E <br>\n",
    "D -> A B C E <br>\n",
    "E -> B C D <br>\n",
    "\n",
    "**After mapping:**\n",
    "\n",
    "(A B) -> B C D <br>\n",
    "(A C) -> B C D <br>\n",
    "(A D) -> B C D <br>\n",
    " <br>\n",
    "(A B) -> A C D E <br>\n",
    "(B C) -> A C D E <br>\n",
    "(B D) -> A C D E <br>\n",
    "(B E) -> A C D E <br>\n",
    " <br>\n",
    "(A C) -> A B D E <br>\n",
    "(B C) -> A B D E <br>\n",
    "(C D) -> A B D E <br>\n",
    "(C E) -> A B D E <br>\n",
    " <br>\n",
    "(A D) -> A B C E <br>\n",
    "(B D) -> A B C E <br>\n",
    "(C D) -> A B C E <br>\n",
    "(D E) -> A B C E <br>\n",
    " <br>\n",
    "(B E) -> B C D <br>\n",
    "(C E) -> B C D <br>\n",
    "(D E) -> B C D <br>\n",
    "\n",
    "**After shuffling:** <br>\n",
    "(A B) -> (A C D E) (B C D) <br>\n",
    "(A C) -> (A B D E) (B C D) <br>\n",
    "(A D) -> (A B C E) (B C D) <br>\n",
    "(B C) -> (A B D E) (A C D E) <br>\n",
    "(B D) -> (A B C E) (A C D E) <br>\n",
    "(B E) -> (A C D E) (B C D) <br>\n",
    "(C D) -> (A B C E) (A B D E) <br>\n",
    "(C E) -> (A B D E) (B C D) <br>\n",
    "(D E) -> (A B C E) (B C D) <br>\n",
    "\n",
    "**After reducing:** <br>\n",
    "(A B) -> (C D) <br>\n",
    "(A C) -> (B D) <br>\n",
    "(A D) -> (B C) <br>\n",
    "(B C) -> (A D E) <br>\n",
    "(B D) -> (A C E) <br>\n",
    "(B E) -> (C D) <br>\n",
    "(C D) -> (A B E) <br>\n",
    "(C E) -> (B D) <br>\n",
    "(D E) -> (B C) <br>\n",
    "\n",
    "\n",
    "### Parallel DBMS vs. Map/Reduce\n",
    "\n",
    "<img src=\"pictures/Parallel_DBMS_vs_Map_Reduce.png\" alt=\"Parallel_DBMS_vs_Map_Reduce\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "\n",
    "### Relational Operators as Map/Reduce jobs\n",
    "\n",
    "* **SQL Query** \n",
    "\n",
    "```\n",
    "SELECT year, SUM(price)\n",
    "FROM   sales\n",
    "WHERE  area_code = “US”\n",
    "GROUP BY year\n",
    "```\n",
    "* **Map/Reduce job:**\n",
    "\n",
    "```\n",
    "map(key, tuple) {\n",
    "\t  int year = YEAR(tuple.date);\n",
    "\t  if (tuple.area_code = “US”)\n",
    "\t    emit(year, {‘price’ => tuple.price });\n",
    "\t}\n",
    "\n",
    "\treduce(key, tuples) {\n",
    "\t  double sum_price = 0;\n",
    "\t  foreach (tuple in tuples) {\n",
    "\t    sum_price += tuple.price;\n",
    "\t  }\n",
    "\t  emit(key, sum_price);\n",
    "\t}\n",
    "```\n",
    "\n",
    "* **Sorting with SQL Query:**\n",
    "\n",
    "```\n",
    "SELECT * \n",
    "FROM sales \n",
    "ORDER BY year\n",
    "```\n",
    "\n",
    "* **Map/Reduce job:**\n",
    "\n",
    "```\n",
    "map(key, tuple) {\n",
    "\t  emit(YEAR(tuple.date) div 10, tuple);\n",
    "\t}\n",
    "\n",
    "\treduce(key, tuples) {\n",
    "\t  emit(key, sort(tuples));\n",
    "\t}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6fc60",
   "metadata": {},
   "source": [
    "### Hadoop – A map/reduce Framework \n",
    "\n",
    "* Hadoop: Apache Top Level Project\n",
    "    * Open source\n",
    "    * Written in Java\n",
    "\n",
    "* Hadoop provides a stack of\n",
    "    * Distributed file system (HDFS) – modeled after the Google File System\n",
    "    * Map/Reduce engine\n",
    "    * Data processing languages (Pig Latin, Hive SQL)\n",
    "    * Plus very many packages\n",
    "\n",
    "* Runs on\n",
    "    * Linux, Mac OS/X, Windows, Solaris\n",
    "    * Commodity hardware\n",
    "\n",
    "<img src=\"pictures/hadoop.png\" alt=\"hadoop\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43d741",
   "metadata": {},
   "source": [
    "### Hadoop Distributed File System (HDFS)\n",
    "\n",
    "* Master-Slave Architecture\n",
    "    * Based on GFS architecture\n",
    "\n",
    "* HDFS Master “NameNode”\n",
    "    * Manages all file system metadata\n",
    "    * Transactions are logged, merged \n",
    "    * at startup\n",
    "    * Controls read/write access to files\n",
    "    * Manages block replication\n",
    "    * Can be replicated to avoid single-point-of-failure\n",
    "\n",
    "* HDFS Slave “DataNode”\n",
    "    * Communicates with the NameNode periodically via heartbeats\n",
    "    * Serves read/write requests from clients\n",
    "    * Performs replication tasks upon instruction by NameNode\n",
    "        * Default replication factor: 3\n",
    "\n",
    "<img src=\"pictures/HDFS.png\" alt=\"HDFS\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Hadoop Map/Reduce Engine\n",
    "\n",
    "Jobs are executed like a Unix pipeline: <br>\n",
    "* cat * | grep | sort | uniq -c | cat    > output <br>\n",
    "* Input | Map  | Sort & Shuffle | Reduce | Output\n",
    "\n",
    "Workflow\n",
    "1. Input phase: generates a number of FileSplits from input files (one per Map task)\n",
    "2. Map phase: executes a user function to transform input kv-pairs into a new set of kv-pairs\n",
    "3. Sort & shuffle phase: sort and distribute the kv-pairs to output nodes\n",
    "4. Reduce phase: combines all kv-pairs with the same key into new kv-pairs\n",
    "5. Output phase writes the resulting pairs to files\n",
    "\n",
    "All phases are distributed with many tasks doing the work\n",
    "* Framework handles scheduling of tasks on cluster\n",
    "* Framework handles recovery when a node fails\n",
    "\n",
    "\n",
    "* Master / Slave architecture\n",
    "\n",
    "* Map/Reduce Master: JobTracker\n",
    "    * Accepts jobs submitted by clients\n",
    "    * Assigns map and reduce tasks to TaskTrackers\n",
    "    * Monitors execution status, re-executes tasks upon failure\n",
    "\n",
    "* Map/Reduce Slave: TaskTracker\n",
    "    * Runs map / reduce tasks upon instruction from the task tracker\n",
    "    * Manage storage, sorting and transmission of intermediate output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38894c7c",
   "metadata": {},
   "source": [
    "### Hadoop Map/Reduce Engine\n",
    "\n",
    "<img src=\"pictures/Hadoop-Map_Reduce-engine.png\" alt=\"Hadoop-Map_Reduce-engine\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "### Fehlertoleranz\n",
    "\n",
    "* Viele Daten  …  in langen Prozessen  …  auf vielen Maschinen\n",
    "\n",
    "\n",
    "* Verteiltes Dateisystem (DFS / HDFS)\n",
    "    * Speichert verteilt und fehlertolerant durch Replikation\n",
    "    * Input ist redundant verfügbar\n",
    "* Speicherung von Zwischenergebnissen ins DFS\n",
    "    * Aufwändig, aber Fehlererholung im laufenden Prozess einfach und schnell\n",
    "* Abstürze\n",
    "    * Werden erkannt falls periodisches Signal ausfällt (heartbeat)\n",
    "    * Neustart des Mappers oder Reducers\n",
    "        * Auf anderer Maschine, mit Replikat des ursprünglichen Inputs\n",
    "     \n",
    "### When to use Hadoop?\n",
    "\n",
    "* Good fit for batch processing applications that need to touch all your data:\n",
    "    * Data mining\n",
    "    * Model tuning\n",
    "    * Text processing\n",
    "\n",
    "* Bad fit for applications that need to find/edit one particular record\n",
    "    * High overhead\n",
    "    * High latency\n",
    "\n",
    "* Bad fit for applications that need to communicate between processes\n",
    "    * Hadoop is oriented around independent units of work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa99f30",
   "metadata": {},
   "source": [
    "### In der Praxis: Komplexe (optimierte) MapReduce Workflows\n",
    "* Neue (relationale) Operatoren\n",
    "    * Join, Cross, Union, …\n",
    "* Planoptimierung & Re-optimierung\n",
    "* Scheduling & Lastbalancierung\n",
    "* Cross-Plattform Ausführung\n",
    "\n",
    "<img src=\"pictures/Komplexe-optimierte-MapReduce-workflows.png\" alt=\"Komplexe-optimierte-MapReduce-workflows\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74280512",
   "metadata": {},
   "source": [
    "### Hadoop vs. Parallel DBMS\n",
    "\n",
    "* 2012\n",
    "  \n",
    "<img src=\"pictures/Hadoop_vs_Parallel_DBMS.png\" alt=\"Hadoop_vs_Parallel_DBMS\" width=\"500\" style=\"background-color: white;\"/>\n",
    "  \n",
    "* 2014\n",
    "  \n",
    "<img src=\"pictures/Hadoop_vs_Parallel_DBMS_2.png\" alt=\"Hadoop_vs_Parallel_DBMS_2\" width=\"500\" style=\"background-color: white;\"/>\n",
    "\n",
    "\n",
    "<img src=\"pictures/Hadoop-vs-Parallel-DBMS.png\" alt=\"Hadoop-vs-Parallel-DBMS\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093d5f2",
   "metadata": {},
   "source": [
    "### In der Praxis: Viele Bibliotheken\n",
    "\n",
    "* Startpunkt: Hadoop\n",
    "    * Java, open-source\n",
    "    * Basis-Bibliotheken\t\t\t\tCommon, MapReduce\n",
    "    * Verteiltes Dateisystem\t\t\tHDFS\n",
    "    * Scheduling, Monitoring\t\t\tYarn\n",
    "\n",
    "* Erweiterungen\n",
    "    * Service- und Cluster-Verwaltung\t\tZooKeeper\n",
    "    * Datenspeicher\t\t\t\tHBase\n",
    "    * Datenbank und Anfragesprachen\t\tPig, Hive, Phoenix\n",
    "    * Bibliotheken für komplexe Verfahren\t\tMahout, Giraph, Solr\n",
    "    * Datenstromverarbeitung\t\t\tKafka, Flink, Spark\n",
    "    * …\n",
    "\n",
    "\n",
    "<img src=\"pictures/Viele-Bibliotheken.png\" alt=\"Viele-Bibliotheken\" width=\"500\" style=\"background-color: white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e7461-dd8b-4016-b578-78899686aeda",
   "metadata": {},
   "source": [
    "## Outlook: What about updates/transactions?\n",
    "\n",
    "* OLTP style applications that are beyond relational databases' capabilities exist as well.\n",
    "* Some applications still require fast and efficient lookup and retrieval of small amounts of data\n",
    "    * Web index access, mail accounts, warehouse updates for resellers \n",
    "    * Addressed by Key/Value pair based storage systems (e.g. Google BigTable and Megastore)\n",
    "    * Can access the data only through a key\n",
    "    * Can apply only an additional filter on columns and timestamps\n",
    "* Some applications still need updates and certain guarantees about them\n",
    "    * No hard transactions, especially no multi record transactions.\n",
    "    * Eventual consistency model\n",
    "* See next set of slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddc406-ec67-4812-9780-cfc72bdda3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
